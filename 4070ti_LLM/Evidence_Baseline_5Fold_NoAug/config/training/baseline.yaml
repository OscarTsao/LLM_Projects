num_epochs: 20
batch_size: 24
eval_batch_size: 24
learning_rate: 8.6e-06
weight_decay: 0.00039
warmup_ratio: 0.168
adam_eps: 7.8e-09
adam_beta1: 0.846
adam_beta2: 0.974
layerwise_lr_decay: 0.897
gradient_accumulation_steps: 3
max_grad_norm: 0.51
early_stopping_patience: 10
metric_for_best: f1
use_amp: true
auto_resume: true
resume_checkpoint: null
num_workers: null
prefetch_factor: 4
persistent_workers: true
lr_scheduler_type: cosine_restarts
lr_scheduler_num_cycles: 2
use_torch_compile: true
torch_compile_mode: reduce-overhead
torch_compile_fullgraph: false
torch_compile_dynamic: true
