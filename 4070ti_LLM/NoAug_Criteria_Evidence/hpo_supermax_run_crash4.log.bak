[0;34m======================================================================
  Running Super-Max HPO for ALL Architectures Sequentially
======================================================================[0m 

[0;33mSequence: Criteria â†’ Evidence â†’ Share â†’ Joint[0m 
[0;33mTotal trials: ~19,000 (5000+8000+3000+3000)[0m 
[0;33mEstimated time: ~80-120 hours with PAR=4[0m 

[0;32mStarting...[0m 

[0;34m[1/4] Running Criteria (5000 trials)...[0m 
make[1]: Entering directory '/media/user/SSD1/YuNing/NoAug_Criteria_Evidence'
[0;34mRunning SUPER-MAX HPO for Criteria...[0m 
Trials: 5000 | Parallel: 3 (reduced from 4 to prevent GPU OOM) | Epochs: 100 | Patience: 20
PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python HPO_EPOCHS=100 HPO_PATIENCE=20 poetry run python scripts/tune_max.py \
	--agent criteria --study noaug-criteria-supermax \
	--n-trials 5000 --parallel 3 \
	--outdir ./_runs
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py:969: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return PatientPruner(hb, patience=4)  # More patient (was 2)
[I 2025-10-30 05:15:34,870] Using an existing study with name 'noaug-criteria-supermax' instead of creating a new one.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
âœ“ Configuration validation passed
  Agent: criteria
  Epochs: 100 | Patience: 20
  Output: ./_runs
[HPO] agent=criteria epochs=100 storage=sqlite:////media/user/SSD1/YuNing/NoAug_Criteria_Evidence/_optuna/noaug.db
[HPO] Study 'noaug-criteria-supermax' is compatible. Resuming optimization.

================================================================================
TRIAL 2380 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 3.656042902916346e-05
  Dropout: 0.17807525037676236
================================================================================


================================================================================
TRIAL 2379 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.746428440962193e-06
  Dropout: 0.2834620396097389
================================================================================


================================================================================
TRIAL 2381 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.789456347463861e-06
  Dropout: 0.2967160462085566
================================================================================

[I 2025-10-30 05:28:32,080] Trial 2381 pruned. Pruned at step 10 with metric 0.5968
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2382 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 48
  Learning rate: 5.625964536759856e-06
  Dropout: 0.18843235861598842
================================================================================

[I 2025-10-30 05:28:37,859] Trial 2380 pruned. OOM: microsoft/deberta-v3-base bs=12 len=288
[I 2025-10-30 05:28:38,095] Trial 2382 pruned. OOM: microsoft/deberta-v3-base bs=48 len=192

[OOM] Trial 2382 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 48 (effective: 288 with grad_accum=6)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 69.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2380 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 49.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2384 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 4.3229121980299073e-05
  Dropout: 0.0232157020722085
================================================================================


================================================================================
TRIAL 2383 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 4.061398847143381e-05
  Dropout: 0.09125625762875696
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 05:28:42,812] Trial 2379 pruned. OOM: roberta-base bs=16 len=224
[I 2025-10-30 05:28:43,883] Trial 2384 pruned. OOM: roberta-large bs=16 len=256
[I 2025-10-30 05:28:44,793] Trial 2383 pruned. OOM: bert-base-uncased bs=12 len=128

[OOM] Trial 2379 exceeded GPU memory:
  Model: roberta-base
  Batch size: 16 (effective: 16 with grad_accum=1)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 51.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2384 exceeded GPU memory:
  Model: roberta-large
  Batch size: 16 (effective: 48 with grad_accum=3)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 91.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2385 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.2919370900828701e-05
  Dropout: 0.036689369092686115
================================================================================


[OOM] Trial 2383 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 51.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2386 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 0.0001400456750309457
  Dropout: 0.0815676714572088
================================================================================


================================================================================
TRIAL 2387 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.1971996241855754e-05
  Dropout: 0.4306494386147558
================================================================================

[I 2025-10-30 05:28:52,441] Trial 2387 pruned. OOM: bert-base-uncased bs=64 len=192
[I 2025-10-30 05:28:52,603] Trial 2385 pruned. OOM: bert-base-uncased bs=48 len=288
[I 2025-10-30 05:28:53,949] Trial 2386 pruned. OOM: bert-large-uncased bs=16 len=224
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2385 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 48 with grad_accum=1)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 33.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2387 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 33.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2388 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.429775868581281e-06
  Dropout: 0.43079960708032433
================================================================================


[OOM] Trial 2386 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 16 (effective: 16 with grad_accum=1)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 33.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2389 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 9.649173049304514e-06
  Dropout: 0.2652731076694736
================================================================================


================================================================================
TRIAL 2390 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.456217324609926e-05
  Dropout: 0.2670894757067873
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 05:29:00,030] Trial 2390 pruned. OOM: roberta-base bs=64 len=224
[I 2025-10-30 05:29:01,505] Trial 2389 pruned. OOM: bert-large-uncased bs=12 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 05:29:01,993] Trial 2392 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

[OOM] Trial 2390 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 87.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2391 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 3.0224590770991582e-05
  Dropout: 0.2159730055379106
================================================================================


[OOM] Trial 2389 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 87.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2393 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6669610515143498e-05
  Dropout: 0.015885060116821448
================================================================================

[I 2025-10-30 05:41:09,733] Trial 2393 pruned. Pruned at step 7 with metric 0.5941
[I 2025-10-30 05:41:10,201] Trial 2394 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2395 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3874130098953173e-05
  Dropout: 0.12519596371464903
================================================================================

[I 2025-10-30 05:41:30,402] Trial 2391 finished with value: 0.6870865981479597 and parameters: {'seed': 40542, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.0224590770991582e-05, 'optim.weight_decay': 0.005682387492889164, 'optim.beta1': 0.9497936926574762, 'optim.beta2': 0.9583041631019972, 'optim.eps': 1.863600788431132e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.09409661715858603, 'sched.poly_power': 0.6378509032847415, 'train.clip_grad': 1.449382533322438, 'model.dropout': 0.2159730055379106, 'model.attn_dropout': 0.16791215675373464, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.90482066551456, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.09454478411397438, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.6316297127264106, 'loss.cls.alpha': 0.40251526424670636, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-30 05:41:30,888] Trial 2396 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 2397 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.0059824309409713e-05
  Dropout: 0.3420722288881537
================================================================================

[I 2025-10-30 05:54:12,818] Trial 2388 pruned. Pruned at step 12 with metric 0.5371

================================================================================
TRIAL 2398 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.705359258764982e-05
  Dropout: 0.30231774564607866
================================================================================

[I 2025-10-30 06:00:41,812] Trial 2397 finished with value: 0.7540231507622812 and parameters: {'seed': 51291, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.0059824309409713e-05, 'optim.weight_decay': 3.679566804704771e-05, 'optim.beta1': 0.8530085702437027, 'optim.beta2': 0.9847429764144603, 'optim.eps': 5.2063488536300066e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.18646173655332327, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.08827715590712, 'model.dropout': 0.3420722288881537, 'model.attn_dropout': 0.07863999907842542, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9223735021054991, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.4986046553224039, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.066508665727586, 'loss.cls.alpha': 0.48358824136842204, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 46 (patience=20)

================================================================================
TRIAL 2399 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.6304951562361215e-05
  Dropout: 0.35955926318875436
================================================================================

[I 2025-10-30 06:09:29,208] Trial 2399 finished with value: 0.6272727272727273 and parameters: {'seed': 23281, 'model.name': 'bert-base-uncased', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.6304951562361215e-05, 'optim.weight_decay': 0.00031346960446203785, 'optim.beta1': 0.9001253407403665, 'optim.beta2': 0.9812479563895234, 'optim.eps': 1.4774673705711432e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.18993080260582124, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.7662594316938237, 'model.dropout': 0.35955926318875436, 'model.attn_dropout': 0.12136653453677107, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9359034208205322, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.3851248884329246, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.125589833522526, 'loss.cls.alpha': 0.48745134680895774, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 2400 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 6.135788438216691e-06
  Dropout: 0.19975069149404165
================================================================================

[I 2025-10-30 06:09:37,827] Trial 2395 pruned. OOM: bert-base-uncased bs=24 len=288
[I 2025-10-30 06:09:38,718] Trial 2400 pruned. OOM: microsoft/deberta-v3-large bs=12 len=256
[I 2025-10-30 06:09:39,329] Trial 2402 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

[OOM] Trial 2395 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 24 (effective: 48 with grad_accum=2)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 82.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2400 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 42.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2401 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.946666574934413e-05
  Dropout: 0.2848146684356206
================================================================================


================================================================================
TRIAL 2403 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.52411237812365e-05
  Dropout: 0.3930343852838593
================================================================================

[I 2025-10-30 06:12:49,163] Trial 2403 pruned. Pruned at step 8 with metric 0.5619

================================================================================
TRIAL 2404 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.8532352411179216e-05
  Dropout: 0.4167110265232581
================================================================================

[I 2025-10-30 06:17:01,018] Trial 2404 pruned. Pruned at step 11 with metric 0.6077
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2405 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.1033634455422017e-05
  Dropout: 0.4058187816867182
================================================================================

[I 2025-10-30 06:17:06,910] Trial 2398 pruned. Pruned at step 8 with metric 0.5657
[I 2025-10-30 06:17:07,398] Trial 2406 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2407 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 64
  Learning rate: 1.9195834201255065e-05
  Dropout: 0.3430109771017349
================================================================================

[I 2025-10-30 06:17:12,208] Trial 2407 pruned. OOM: microsoft/deberta-v3-base bs=64 len=256
[I 2025-10-30 06:17:12,737] Trial 2408 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)

[OOM] Trial 2407 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 64 (effective: 384 with grad_accum=6)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 204.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2409 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 3.392204506656118e-05
  Dropout: 0.034732462876914685
================================================================================

[I 2025-10-30 06:25:58,500] Trial 2409 pruned. Pruned at step 8 with metric 0.5823
[W 2025-10-30 06:25:59,004] The parameter `tok.doc_stride` in Trial#2410 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2410 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.389250173823015e-05
  Dropout: 0.20529661955873113
================================================================================

[I 2025-10-30 06:33:12,592] Trial 2405 pruned. Pruned at step 9 with metric 0.6364
[I 2025-10-30 06:33:13,142] Trial 2411 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2412 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.1244240566300856e-05
  Dropout: 0.26958452335666927
================================================================================

[I 2025-10-30 06:34:38,784] Trial 2410 pruned. Pruned at step 17 with metric 0.6009

================================================================================
TRIAL 2413 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.330615828477145e-06
  Dropout: 0.3010539153791193
================================================================================

[I 2025-10-30 06:38:18,893] Trial 2412 pruned. Pruned at step 7 with metric 0.6285
[I 2025-10-30 06:38:19,382] Trial 2414 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2415 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.7442385644616552e-05
  Dropout: 0.19112535294035776
================================================================================

[I 2025-10-30 06:44:56,401] Trial 2415 pruned. Pruned at step 20 with metric 0.6038
[I 2025-10-30 06:44:56,841] Trial 2416 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-10-30 06:44:57,225] The parameter `tok.doc_stride` in Trial#2417 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 06:44:57,274] Trial 2417 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-10-30 06:44:57,674] The parameter `tok.doc_stride` in Trial#2418 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 06:44:57,723] Trial 2418 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2419 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.416754173404743e-05
  Dropout: 0.0486497099219165
================================================================================

[I 2025-10-30 06:50:03,380] Trial 2419 pruned. Pruned at step 6 with metric 0.5805

================================================================================
TRIAL 2420 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.458574337300994e-05
  Dropout: 0.30850162283347055
================================================================================

[I 2025-10-30 06:51:51,782] Trial 2420 pruned. Pruned at step 9 with metric 0.5486

================================================================================
TRIAL 2421 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.0232947819239574e-05
  Dropout: 0.26114499320120566
================================================================================

[I 2025-10-30 06:52:29,296] Trial 2413 pruned. Pruned at step 10 with metric 0.5427

================================================================================
TRIAL 2422 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.953381448094779e-05
  Dropout: 0.2801037208827554
================================================================================

[I 2025-10-30 06:59:45,860] Trial 2421 pruned. Pruned at step 15 with metric 0.6337
[W 2025-10-30 06:59:46,276] The parameter `tok.doc_stride` in Trial#2423 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 06:59:46,324] Trial 2423 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2424 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 6.880946141907464e-06
  Dropout: 0.38355747873661417
================================================================================

[I 2025-10-30 07:00:49,718] Trial 2422 pruned. Pruned at step 9 with metric 0.5409
[I 2025-10-30 07:00:50,172] Trial 2425 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2426 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.5752135498455854e-05
  Dropout: 0.3454794081757216
================================================================================

[I 2025-10-30 07:06:21,967] Trial 2426 pruned. Pruned at step 7 with metric 0.6042
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2427 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.3684305866395318e-05
  Dropout: 0.034552353597160315
================================================================================

[I 2025-10-30 07:07:51,046] Trial 2424 pruned. Pruned at step 13 with metric 0.6134
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2428 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 6.480814023641317e-06
  Dropout: 0.2859512038389319
================================================================================

[I 2025-10-30 07:12:13,648] Trial 2401 pruned. Pruned at step 25 with metric 0.5864

================================================================================
TRIAL 2429 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.7876812577524866e-05
  Dropout: 0.017331011936233133
================================================================================

[I 2025-10-30 07:18:59,110] Trial 2427 finished with value: 0.6916871752802844 and parameters: {'seed': 65335, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.3684305866395318e-05, 'optim.weight_decay': 0.035560225901942015, 'optim.beta1': 0.8631775257825425, 'optim.beta2': 0.9594726201448835, 'optim.eps': 3.663211451341325e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.16757835925332332, 'sched.poly_power': 0.5032614187010664, 'train.clip_grad': 1.4281941591528953, 'model.dropout': 0.034552353597160315, 'model.attn_dropout': 0.25344732118669977, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8912575190371328, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.2548418251600476, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.07030334632958166, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 2430 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 9.701278617522263e-06
  Dropout: 0.16968719863210424
================================================================================

[I 2025-10-30 07:28:45,760] Trial 2430 pruned. Pruned at step 10 with metric 0.6372
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2431 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 6.350782666805477e-05
  Dropout: 0.11803053296051674
================================================================================

[I 2025-10-30 07:53:17,064] Trial 2428 finished with value: 0.6738636363636363 and parameters: {'seed': 55584, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 6.480814023641317e-06, 'optim.weight_decay': 1.477463839667674e-06, 'optim.beta1': 0.8226715005758816, 'optim.beta2': 0.9935525927960527, 'optim.eps': 3.135781424151259e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1947193601204982, 'train.clip_grad': 0.3625918541409132, 'model.dropout': 0.2859512038389319, 'model.attn_dropout': 0.09758557488536278, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.903132981995809, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.4059702145389457, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.29597131962123, 'loss.cls.alpha': 0.3903320300426245, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 60 (patience=20)

================================================================================
TRIAL 2432 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.5664141285558432e-05
  Dropout: 0.4617542526133954
================================================================================

[I 2025-10-30 07:53:24,993] Trial 2431 pruned. Pruned at step 27 with metric 0.6496
[I 2025-10-30 07:53:25,552] Trial 2433 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-10-30 07:53:25,986] Trial 2434 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2435 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 5.571532214498939e-06
  Dropout: 0.003602579292129135
================================================================================

[I 2025-10-30 07:58:51,913] Trial 2432 pruned. Pruned at step 7 with metric 0.5988
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2436 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 3.854132013809519e-05
  Dropout: 0.04500536955248713
================================================================================

[I 2025-10-30 07:59:05,251] Trial 2435 pruned. OOM: bert-base-uncased bs=32 len=256
[I 2025-10-30 07:59:06,891] Trial 2436 pruned. OOM: roberta-large bs=16 len=320
[I 2025-10-30 07:59:07,048] Trial 2429 pruned. OOM: xlm-roberta-base bs=8 len=160

[OOM] Trial 2435 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 32 (effective: 192 with grad_accum=6)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 64.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2437 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.936107845341243e-06
  Dropout: 0.13909726463505634
================================================================================


[OOM] Trial 2429 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 326.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2436 exceeded GPU memory:
  Model: roberta-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 64.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2439 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.3609320051799167e-05
  Dropout: 0.34427194993949733
================================================================================


================================================================================
TRIAL 2438 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.1381657774844498e-05
  Dropout: 0.3620334469625423
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 08:04:19,300] Trial 2437 pruned. Pruned at step 8 with metric 0.6107
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2440 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 5.678539214891971e-05
  Dropout: 0.49668457147131106
================================================================================

[I 2025-10-30 08:07:42,325] Trial 2440 pruned. Pruned at step 10 with metric 0.5567

================================================================================
TRIAL 2441 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.9608277684163796e-05
  Dropout: 0.35869313170602535
================================================================================

[I 2025-10-30 08:10:00,936] Trial 2439 pruned. Pruned at step 8 with metric 0.6315

================================================================================
TRIAL 2442 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 3.10625339539025e-05
  Dropout: 0.47860829705662566
================================================================================

[I 2025-10-30 08:13:50,227] Trial 2441 pruned. Pruned at step 11 with metric 0.5633
[W 2025-10-30 08:13:50,891] The parameter `tok.doc_stride` in Trial#2443 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2443 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.4507759814878521e-05
  Dropout: 0.04172547098347645
================================================================================

[I 2025-10-30 08:17:38,372] Trial 2442 pruned. Pruned at step 9 with metric 0.5629
[I 2025-10-30 08:17:38,830] Trial 2444 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2445 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.0542122491177812e-05
  Dropout: 0.006911141166220901
================================================================================

[I 2025-10-30 08:32:52,102] Trial 2438 pruned. Pruned at step 10 with metric 0.6111

================================================================================
TRIAL 2446 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2535024664541508e-05
  Dropout: 0.050896020354128144
================================================================================

[I 2025-10-30 08:57:42,374] Trial 2445 finished with value: 0.7082634957781222 and parameters: {'seed': 20389, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.0542122491177812e-05, 'optim.weight_decay': 0.15951539017704247, 'optim.beta1': 0.8683345321087004, 'optim.beta2': 0.9667884138586359, 'optim.eps': 3.6677218763449e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.19583287814539685, 'sched.poly_power': 0.9517159352152935, 'train.clip_grad': 1.4847308991075852, 'model.dropout': 0.006911141166220901, 'model.attn_dropout': 0.17679321791334962, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8165832195747637, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.24890060757092194, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.0828868241374342, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-30 08:57:42,837] Trial 2447 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 2448 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.577310601113142e-05
  Dropout: 0.34160375510251395
================================================================================

[I 2025-10-30 09:03:38,150] Trial 2448 pruned. Pruned at step 12 with metric 0.5479
[I 2025-10-30 09:03:38,639] Trial 2449 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2450 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0302101693257184e-05
  Dropout: 0.3508757926512523
================================================================================

[I 2025-10-30 09:07:15,948] Trial 2446 pruned. Pruned at step 8 with metric 0.5801
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2451 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 8.732552722305684e-06
  Dropout: 0.04631128949892978
================================================================================

[I 2025-10-30 09:10:36,938] Trial 2450 pruned. Pruned at step 10 with metric 0.5878

================================================================================
TRIAL 2452 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.23401237939234e-06
  Dropout: 0.29207283388850036
================================================================================

[I 2025-10-30 09:18:19,756] Trial 2452 pruned. Pruned at step 9 with metric 0.6231

================================================================================
TRIAL 2453 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 3.0503744212369057e-05
  Dropout: 0.40904646949301826
================================================================================

[I 2025-10-30 09:24:33,165] Trial 2443 finished with value: 0.6648777027235433 and parameters: {'seed': 3834, 'model.name': 'bert-large-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.4507759814878521e-05, 'optim.weight_decay': 0.007688248695966395, 'optim.beta1': 0.8901400884007284, 'optim.beta2': 0.9709178761432192, 'optim.eps': 4.869545613813357e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.19505375148955229, 'train.clip_grad': 1.1079654941408092, 'model.dropout': 0.04172547098347645, 'model.attn_dropout': 0.2637710923300359, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8218991568088616, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.22475928662616632, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.686321109221015, 'loss.cls.alpha': 0.4176716907756479, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 2454 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.994828420118578e-06
  Dropout: 0.21634248385303623
================================================================================

[I 2025-10-30 09:35:22,864] Trial 2454 pruned. Pruned at step 16 with metric 0.6299
[I 2025-10-30 09:35:23,324] Trial 2455 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2456 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.041279454305403e-05
  Dropout: 0.09558226669964685
================================================================================

[I 2025-10-30 09:35:29,398] Trial 2451 pruned. OOM: roberta-base bs=32 len=224
[I 2025-10-30 09:35:29,892] Trial 2457 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-10-30 09:35:30,291] The parameter `tok.doc_stride` in Trial#2458 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 09:35:30,344] Trial 2458 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2451 exceeded GPU memory:
  Model: roberta-base
  Batch size: 32 (effective: 128 with grad_accum=4)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 60.44 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2459 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 7.346835796285029e-06
  Dropout: 0.3488612062852646
================================================================================

[I 2025-10-30 09:42:50,972] Trial 2456 pruned. Pruned at step 12 with metric 0.6231

================================================================================
TRIAL 2460 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.736853726854652e-05
  Dropout: 0.3118418343417945
================================================================================

[I 2025-10-30 09:47:24,769] Trial 2460 pruned. Pruned at step 9 with metric 0.5275
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2461 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 1.3446668413510618e-05
  Dropout: 0.28521142198790567
================================================================================

[I 2025-10-30 09:54:47,283] Trial 2461 pruned. Pruned at step 7 with metric 0.5936
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2462 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 3.40967930012656e-05
  Dropout: 0.1947059227006831
================================================================================

[I 2025-10-30 09:59:56,562] Trial 2453 finished with value: 0.45187165775401067 and parameters: {'seed': 4592, 'model.name': 'bert-large-uncased', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.0503744212369057e-05, 'optim.weight_decay': 1.615269470702897e-05, 'optim.beta1': 0.818672499426212, 'optim.beta2': 0.9929969606679832, 'optim.eps': 1.1631398193164752e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.13585968849880717, 'sched.poly_power': 0.6159087097766657, 'train.clip_grad': 0.8017280828365083, 'model.dropout': 0.40904646949301826, 'model.attn_dropout': 0.07014574662276418, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9533859850023936, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.47721872671808907, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.473598477889437, 'loss.cls.alpha': 0.501555405924429, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2463 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1839745683215592e-05
  Dropout: 0.08552647048508627
================================================================================

[I 2025-10-30 10:03:52,371] Trial 2459 pruned. Pruned at step 14 with metric 0.5842
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2464 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 7.0460861060509435e-06
  Dropout: 0.4705310919967366
================================================================================

[I 2025-10-30 10:09:37,851] Trial 2462 pruned. Pruned at step 9 with metric 0.5923
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2465 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.42189882205077e-05
  Dropout: 0.13198344160807096
================================================================================

[I 2025-10-30 10:17:31,627] Trial 2463 pruned. Pruned at step 32 with metric 0.6359

================================================================================
TRIAL 2466 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.540436064708564e-06
  Dropout: 0.4631032601283002
================================================================================

[I 2025-10-30 10:24:02,345] Trial 2465 pruned. Pruned at step 27 with metric 0.6372
[I 2025-10-30 10:24:02,832] Trial 2467 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-10-30 10:24:03,273] Trial 2468 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[W 2025-10-30 10:24:03,689] The parameter `tok.doc_stride` in Trial#2469 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2469 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1180852226606269e-05
  Dropout: 0.06203663515095595
================================================================================

[I 2025-10-30 10:27:43,736] Trial 2466 pruned. Pruned at step 10 with metric 0.6134
[I 2025-10-30 10:27:44,221] Trial 2470 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2471 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.068643425846747e-06
  Dropout: 0.3433517743813467
================================================================================

[I 2025-10-30 10:34:41,206] Trial 2471 pruned. Pruned at step 13 with metric 0.6125
[I 2025-10-30 10:34:41,648] Trial 2472 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2473 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 6.662843235693317e-06
  Dropout: 0.3474228030199446
================================================================================

[I 2025-10-30 10:34:48,636] Trial 2469 pruned. OOM: bert-base-uncased bs=24 len=160
[I 2025-10-30 10:34:48,792] Trial 2473 pruned. OOM: microsoft/deberta-v3-base bs=24 len=224
[I 2025-10-30 10:34:49,277] Trial 2475 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-10-30 10:34:49,582] Trial 2474 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-10-30 10:34:50,272] Trial 2464 pruned. OOM: roberta-large bs=12 len=352
[I 2025-10-30 10:34:51,383] Trial 2476 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-10-30 10:34:51,818] Trial 2479 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

[OOM] Trial 2473 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 24 (effective: 48 with grad_accum=2)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 54.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2469 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 24 (effective: 96 with grad_accum=4)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 76.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2464 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 54.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2477 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.6736670938393267e-05
  Dropout: 0.18713474077254205
================================================================================


================================================================================
TRIAL 2478 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.183848824990186e-05
  Dropout: 0.18501559937515216
================================================================================


================================================================================
TRIAL 2480 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.1987544977510496e-05
  Dropout: 0.38064737293126977
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 10:41:43,310] Trial 2477 pruned. Pruned at step 20 with metric 0.6215
[W 2025-10-30 10:41:43,745] The parameter `tok.doc_stride` in Trial#2481 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2481 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.2937107894077053e-05
  Dropout: 0.011591797756446864
================================================================================

[I 2025-10-30 10:45:07,613] Trial 2481 pruned. Pruned at step 17 with metric 0.6299
[I 2025-10-30 10:45:08,069] Trial 2482 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2483 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 6.4762360545226766e-06
  Dropout: 0.20374105453608898
================================================================================

[I 2025-10-30 11:06:06,327] Trial 2480 finished with value: 0.6991412958626073 and parameters: {'seed': 38658, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.1987544977510496e-05, 'optim.weight_decay': 0.12181505152989887, 'optim.beta1': 0.8520742548446489, 'optim.beta2': 0.9667461879460008, 'optim.eps': 5.038781457567188e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.10213163589909251, 'sched.poly_power': 0.5095523368136526, 'train.clip_grad': 1.4459763754502089, 'model.dropout': 0.38064737293126977, 'model.attn_dropout': 0.2157794319510507, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9114902913899295, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.4767064628528433, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.7120670616100435, 'loss.cls.alpha': 0.6528192705666698, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 2484 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 4.848982763409822e-05
  Dropout: 0.31218988756950333
================================================================================

[I 2025-10-30 11:10:08,374] Trial 2483 pruned. Pruned at step 10 with metric 0.6583
[W 2025-10-30 11:10:08,899] The parameter `tok.doc_stride` in Trial#2485 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2485 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 8.36903869943148e-06
  Dropout: 0.3244233051624076
================================================================================

[I 2025-10-30 11:18:35,479] Trial 2485 pruned. Pruned at step 13 with metric 0.5801

================================================================================
TRIAL 2486 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 8.59500489201976e-06
  Dropout: 0.34330673618918783
================================================================================

[I 2025-10-30 11:22:34,745] Trial 2486 pruned. Pruned at step 17 with metric 0.5962
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2487 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 48
  Learning rate: 3.0102286643471716e-05
  Dropout: 0.045223304844566624
================================================================================

[I 2025-10-30 11:22:40,053] Trial 2487 pruned. OOM: microsoft/deberta-v3-base bs=48 len=256
[I 2025-10-30 11:22:41,605] Trial 2484 pruned. OOM: xlm-roberta-base bs=8 len=192
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2487 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 48 (effective: 384 with grad_accum=8)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 103.94 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2488 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.7901982244448633e-05
  Dropout: 0.019097385352378288
================================================================================


[OOM] Trial 2484 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 24 with grad_accum=3)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 463.94 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2489 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.5856958550020794e-05
  Dropout: 0.39401761965692156
================================================================================

[I 2025-10-30 11:29:52,556] Trial 2489 pruned. Pruned at step 12 with metric 0.6208
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2490 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.5274464124404304e-05
  Dropout: 0.12549533104971708
================================================================================

[I 2025-10-30 11:30:50,772] Trial 2488 pruned. Pruned at step 11 with metric 0.6208
[W 2025-10-30 11:30:51,207] The parameter `tok.doc_stride` in Trial#2491 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2491 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.130295497432505e-06
  Dropout: 0.4434723004116607
================================================================================

[I 2025-10-30 11:35:34,014] Trial 2491 pruned. Pruned at step 9 with metric 0.5962
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2492 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 64
  Learning rate: 9.278050827845083e-06
  Dropout: 0.14374861268550354
================================================================================

[I 2025-10-30 11:35:38,920] Trial 2492 pruned. OOM: microsoft/deberta-v3-base bs=64 len=288
[W 2025-10-30 11:35:39,429] The parameter `tok.doc_stride` in Trial#2493 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 2492 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 64 (effective: 512 with grad_accum=8)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 139.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2493 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.401712463147078e-05
  Dropout: 0.3659803188191913
================================================================================

[I 2025-10-30 11:39:21,142] Trial 2493 pruned. Pruned at step 15 with metric 0.6053
[I 2025-10-30 11:39:21,587] Trial 2494 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-10-30 11:39:22,006] The parameter `tok.doc_stride` in Trial#2495 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2495 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.1625118447025574e-05
  Dropout: 0.16039356880758107
================================================================================

[I 2025-10-30 11:41:45,303] Trial 2478 pruned. Pruned at step 22 with metric 0.6205
[I 2025-10-30 11:41:45,771] Trial 2496 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2497 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.5099041231440762e-05
  Dropout: 0.10292045455685495
================================================================================

[I 2025-10-30 11:49:56,869] Trial 2495 pruned. Pruned at step 19 with metric 0.5839

================================================================================
TRIAL 2498 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 3.254943030219083e-05
  Dropout: 0.05597203531730441
================================================================================

[I 2025-10-30 12:24:17,699] Trial 2498 finished with value: 0.44594594594594594 and parameters: {'seed': 3417, 'model.name': 'bert-large-uncased', 'tok.max_length': 320, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.254943030219083e-05, 'optim.weight_decay': 0.0015531944216239158, 'optim.beta1': 0.8657432343761813, 'optim.beta2': 0.982729776202159, 'optim.eps': 9.760805867570134e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1603503131086973, 'sched.poly_power': 0.6248718278055634, 'train.clip_grad': 0.5868909291338464, 'model.dropout': 0.05597203531730441, 'model.attn_dropout': 0.20111345291602484, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8898824691229176, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.34556075734083536, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.4233828951681247, 'loss.cls.alpha': 0.6955645580048524, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-10-30 12:24:18,119] The parameter `tok.doc_stride` in Trial#2499 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 12:24:18,168] Trial 2499 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2500 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 3.13648495020904e-05
  Dropout: 0.3890564799411646
================================================================================

[I 2025-10-30 12:27:35,034] Trial 2497 finished with value: 0.6798980933596318 and parameters: {'seed': 53809, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.5099041231440762e-05, 'optim.weight_decay': 0.00024118323013840726, 'optim.beta1': 0.8332953605884815, 'optim.beta2': 0.981407759980172, 'optim.eps': 1.515867771797301e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.06056905707131252, 'train.clip_grad': 1.398365742623932, 'model.dropout': 0.10292045455685495, 'model.attn_dropout': 0.1689488821682512, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8122034515149487, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.11435570215163457, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.824473804057985, 'loss.cls.alpha': 0.5074522471819854, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 2501 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6112356689499742e-05
  Dropout: 0.10692496242035548
================================================================================

[I 2025-10-30 12:33:19,204] Trial 2500 pruned. Pruned at step 9 with metric 0.5769
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2502 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 2.6537669575560755e-05
  Dropout: 0.4680021667287205
================================================================================

[I 2025-10-30 12:33:25,573] Trial 2502 pruned. OOM: microsoft/deberta-v3-base bs=24 len=320
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2502 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 24 (effective: 144 with grad_accum=6)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 48.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2503 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.663984131806932e-05
  Dropout: 0.19849185984444723
================================================================================

[I 2025-10-30 12:43:13,373] Trial 2501 pruned. Pruned at step 9 with metric 0.6053
[I 2025-10-30 12:43:13,826] Trial 2504 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2505 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.07678741635137e-06
  Dropout: 0.4532539683166392
================================================================================

[I 2025-10-30 12:44:03,893] Trial 2503 finished with value: 0.44594594594594594 and parameters: {'seed': 54448, 'model.name': 'roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 8.663984131806932e-05, 'optim.weight_decay': 0.00023542503733415748, 'optim.beta1': 0.9097220788872159, 'optim.beta2': 0.9696428473178191, 'optim.eps': 2.578504274918319e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.07006867373942256, 'sched.poly_power': 0.6536302226866268, 'train.clip_grad': 1.187048077676331, 'model.dropout': 0.19849185984444723, 'model.attn_dropout': 0.2889956079368773, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8508894051738103, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.39176385476291237, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.06726772798820539, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2506 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.741336766070364e-06
  Dropout: 0.0025616883989045547
================================================================================

[I 2025-10-30 12:45:21,454] Trial 2490 pruned. Pruned at step 17 with metric 0.6038
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2507 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 48
  Learning rate: 7.436519176030749e-06
  Dropout: 0.27002825351574566
================================================================================

[I 2025-10-30 12:45:26,642] Trial 2507 pruned. OOM: microsoft/deberta-v3-base bs=48 len=224
[I 2025-10-30 12:45:27,186] Trial 2508 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2507 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 48 (effective: 192 with grad_accum=4)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 88.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2509 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.42597940041651e-05
  Dropout: 0.1718445219019093
================================================================================

[I 2025-10-30 12:52:54,341] Trial 2506 pruned. Pruned at step 12 with metric 0.6508
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2510 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.422396451545198e-06
  Dropout: 0.0785253004447646
================================================================================

[I 2025-10-30 13:03:50,998] Trial 2509 pruned. Pruned at step 27 with metric 0.6027
[I 2025-10-30 13:03:51,471] Trial 2511 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-10-30 13:03:51,904] Trial 2512 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2513 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.154362314682374e-05
  Dropout: 0.02616871541468435
================================================================================

[I 2025-10-30 13:27:09,072] Trial 2510 finished with value: 0.6906199210587416 and parameters: {'seed': 43125, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.422396451545198e-06, 'optim.weight_decay': 5.710008550185033e-06, 'optim.beta1': 0.8549995220278332, 'optim.beta2': 0.9960367493993849, 'optim.eps': 3.520466280851915e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.13830301895388414, 'sched.poly_power': 0.815892482131316, 'train.clip_grad': 0.3849501088055418, 'model.dropout': 0.0785253004447646, 'model.attn_dropout': 0.17193272355297282, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8151968186722778, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.030288995315261816, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.442813316644315, 'loss.cls.alpha': 0.5425642020412611, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 2514 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.5957984145467885e-05
  Dropout: 0.005192204620585832
================================================================================

[I 2025-10-30 13:27:12,022] Trial 2514 pruned. OOM: roberta-base bs=64 len=256

[OOM] Trial 2514 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 148.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2515 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 3.172966512357153e-05
  Dropout: 0.37277608235969945
================================================================================

[I 2025-10-30 13:27:15,217] Trial 2513 pruned. OOM: microsoft/deberta-v3-large bs=8 len=128
[I 2025-10-30 13:27:15,798] Trial 2516 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-10-30 13:27:16,239] Trial 2517 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

[OOM] Trial 2513 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 440.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2518 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.301712521989089e-06
  Dropout: 0.062267508292932996
================================================================================

[I 2025-10-30 13:27:19,367] Trial 2518 pruned. OOM: bert-base-uncased bs=64 len=352
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2518 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 80.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2519 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 3.171259056126324e-05
  Dropout: 0.46358448768787774
================================================================================

[I 2025-10-30 13:39:25,614] Trial 2515 pruned. Pruned at step 16 with metric 0.6164
[I 2025-10-30 13:39:26,088] Trial 2520 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2521 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.4574241047781109e-05
  Dropout: 0.45799273610491914
================================================================================

[I 2025-10-30 13:39:29,350] Trial 2521 pruned. OOM: bert-base-uncased bs=48 len=320
[I 2025-10-30 13:39:29,815] Trial 2522 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)

[OOM] Trial 2521 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 384 with grad_accum=8)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 100.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2523 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.9606635623848e-06
  Dropout: 0.06220470671892819
================================================================================

[I 2025-10-30 13:39:36,161] Trial 2523 pruned. OOM: xlm-roberta-base bs=8 len=160
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

[OOM] Trial 2523 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 738.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2524 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 48
  Learning rate: 3.00353519143543e-05
  Dropout: 0.0010333383946297472
================================================================================

[I 2025-10-30 13:39:41,450] Trial 2524 pruned. OOM: microsoft/deberta-v3-base bs=48 len=160
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2524 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 48 (effective: 288 with grad_accum=6)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 128.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2525 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.3415092073759493e-05
  Dropout: 0.32569493812708594
================================================================================

[I 2025-10-30 13:59:44,795] Trial 2505 pruned. Pruned at step 10 with metric 0.5474
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2526 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 2.2831680630303795e-05
  Dropout: 0.3245030322889586
================================================================================

[I 2025-10-30 14:18:43,396] Trial 2519 finished with value: 0.4444444444444444 and parameters: {'seed': 48558, 'model.name': 'roberta-large', 'tok.max_length': 352, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.171259056126324e-05, 'optim.weight_decay': 0.0577176285457645, 'optim.beta1': 0.8913680348686135, 'optim.beta2': 0.9618783692253944, 'optim.eps': 1.3660552168029193e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.12306164525761039, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.26850563921004, 'model.dropout': 0.46358448768787774, 'model.attn_dropout': 0.27949061201724346, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8710169293563816, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.3273532839904028, 'loss.cls.type': 'focal', 'loss.cls.gamma': 2.075842868615085, 'loss.cls.alpha': 0.494089876374557, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2527 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.0783399672871047e-05
  Dropout: 0.14886598399714515
================================================================================

[I 2025-10-30 14:21:19,661] Trial 2525 finished with value: 0.6825343388095879 and parameters: {'seed': 53976, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.3415092073759493e-05, 'optim.weight_decay': 4.148802711894131e-05, 'optim.beta1': 0.8433643742447795, 'optim.beta2': 0.9925408675710368, 'optim.eps': 1.3258695710921592e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.19947919318254376, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.1954497302446234, 'model.dropout': 0.32569493812708594, 'model.attn_dropout': 0.06025696897527973, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.980660228048478, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.48277525983154407, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.11755280094107, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 52 (patience=20)

================================================================================
TRIAL 2528 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.3895025514770974e-05
  Dropout: 0.02519366152349442
================================================================================

[I 2025-10-30 14:29:24,407] Trial 2528 pruned. Pruned at step 14 with metric 0.5807
[I 2025-10-30 14:29:24,866] Trial 2529 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2530 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.769592269502219e-05
  Dropout: 0.045597645446611657
================================================================================

[I 2025-10-30 14:56:27,667] Trial 2527 finished with value: 0.6863387978142077 and parameters: {'seed': 25176, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.0783399672871047e-05, 'optim.weight_decay': 0.0029409708571379867, 'optim.beta1': 0.9409542608724666, 'optim.beta2': 0.9886554340662685, 'optim.eps': 1.7125802884397172e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.01475460100482854, 'train.clip_grad': 0.3596401644902609, 'model.dropout': 0.14886598399714515, 'model.attn_dropout': 0.07264510672016507, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9100299593585238, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.038832941772384034, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.10895403039352325, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-30 14:56:28,135] Trial 2531 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 2532 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 9.575794995987674e-05
  Dropout: 0.49252327282527936
================================================================================

[I 2025-10-30 15:04:57,037] Trial 2530 finished with value: 0.6650004806305874 and parameters: {'seed': 34127, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.769592269502219e-05, 'optim.weight_decay': 0.05278939489864341, 'optim.beta1': 0.8456792110469328, 'optim.beta2': 0.9755509701346705, 'optim.eps': 1.9056655052387485e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.10885073685094465, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.055253557295131, 'model.dropout': 0.045597645446611657, 'model.attn_dropout': 0.29072885955909455, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8992637008580413, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.06805196835158141, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.496709918131728, 'loss.cls.alpha': 0.5580538999264625, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 2533 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 2.0395451973732414e-05
  Dropout: 0.3959891778856163
================================================================================

[I 2025-10-30 15:08:51,657] Trial 2533 pruned. Pruned at step 11 with metric 0.6027
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2534 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.0206801470697263e-05
  Dropout: 0.0028192344967218758
================================================================================

[I 2025-10-30 15:15:32,479] Trial 2534 pruned. Pruned at step 13 with metric 0.5917
[W 2025-10-30 15:15:32,927] The parameter `tok.doc_stride` in Trial#2535 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2535 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.180181582159253e-06
  Dropout: 0.0497521527470458
================================================================================

[I 2025-10-30 15:19:51,041] Trial 2535 pruned. Pruned at step 13 with metric 0.6016
[I 2025-10-30 15:19:51,492] Trial 2536 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2537 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.552799151374709e-05
  Dropout: 0.3413426185918782
================================================================================

[I 2025-10-30 15:22:45,283] Trial 2532 pruned. OOM: roberta-base bs=8 len=384
[W 2025-10-30 15:22:45,740] The parameter `tok.doc_stride` in Trial#2538 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2532 exceeded GPU memory:
  Model: roberta-base
  Batch size: 8 (effective: 64 with grad_accum=8)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 166.75 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2538 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.311796274125028e-06
  Dropout: 0.09238628425508022
================================================================================

[I 2025-10-30 15:24:08,238] Trial 2537 pruned. OOM: bert-base-uncased bs=48 len=320
[I 2025-10-30 15:24:08,737] Trial 2539 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)

[OOM] Trial 2537 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 192 with grad_accum=4)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 165.62 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2540 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.27001506284825e-05
  Dropout: 0.07117183030978858
================================================================================

[I 2025-10-30 15:27:55,289] Trial 2540 pruned. Pruned at step 9 with metric 0.5656
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2541 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.2842960635819843e-05
  Dropout: 0.029183892483843434
================================================================================

[I 2025-10-30 15:30:40,980] Trial 2541 pruned. Pruned at step 10 with metric 0.5451
[I 2025-10-30 15:30:41,447] Trial 2542 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2543 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.0456786435783278e-05
  Dropout: 0.3266467597233517
================================================================================

[I 2025-10-30 15:40:24,600] Trial 2543 pruned. Pruned at step 27 with metric 0.6242
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2544 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 64
  Learning rate: 7.4787489126947886e-06
  Dropout: 0.33005685755600306
================================================================================

[I 2025-10-30 15:40:29,734] Trial 2544 pruned. OOM: microsoft/deberta-v3-base bs=64 len=352

[OOM] Trial 2544 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 64 (effective: 64 with grad_accum=1)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 528.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 547.75 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2545 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 1.7982314675414457e-05
  Dropout: 0.46322739716893674
================================================================================

[I 2025-10-30 15:40:36,105] Trial 2545 pruned. OOM: microsoft/deberta-v3-base bs=24 len=288
[I 2025-10-30 15:40:37,127] Trial 2526 pruned. OOM: microsoft/deberta-v3-base bs=12 len=320
[I 2025-10-30 15:40:38,220] Trial 2538 pruned. OOM: roberta-base bs=32 len=128
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2545 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 24 (effective: 72 with grad_accum=3)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 103.75 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2526 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 103.75 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2538 exceeded GPU memory:
  Model: roberta-base
  Batch size: 32 (effective: 128 with grad_accum=4)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 43.75 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2546 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.164650634043494e-05
  Dropout: 0.34716749649727646
================================================================================


================================================================================
TRIAL 2547 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 2.3484116098374533e-05
  Dropout: 0.06247210500010032
================================================================================


================================================================================
TRIAL 2548 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.0149445997644263e-05
  Dropout: 0.005553032247063362
================================================================================

[I 2025-10-30 16:01:31,294] Trial 2546 pruned. Pruned at step 27 with metric 0.6492
[W 2025-10-30 16:01:31,754] The parameter `tok.doc_stride` in Trial#2549 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2549 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 64
  Learning rate: 4.0829242019288255e-05
  Dropout: 0.009898933499966053
================================================================================

[I 2025-10-30 16:01:37,093] Trial 2548 pruned. OOM: bert-base-uncased bs=48 len=192
[I 2025-10-30 16:01:37,293] Trial 2549 pruned. OOM: microsoft/deberta-v3-base bs=64 len=160
[W 2025-10-30 16:01:38,152] The parameter `tok.doc_stride` in Trial#2551 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2549 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 64 (effective: 384 with grad_accum=6)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 120.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 48.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2548 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 288 with grad_accum=6)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 48.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2550 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 7.212099014311552e-06
  Dropout: 0.3694505702233341
================================================================================


================================================================================
TRIAL 2551 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.1831533385130259e-05
  Dropout: 0.2831458453332806
================================================================================

[I 2025-10-30 16:05:03,900] Trial 2551 pruned. Pruned at step 11 with metric 0.5769
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2552 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.6187811603385646e-05
  Dropout: 0.26682833452185406
================================================================================

[I 2025-10-30 16:05:10,791] Trial 2552 pruned. OOM: roberta-base bs=64 len=224
[I 2025-10-30 16:05:11,252] Trial 2553 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-10-30 16:05:11,662] The parameter `tok.doc_stride` in Trial#2554 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 16:05:11,714] Trial 2554 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

[OOM] Trial 2552 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 149.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2555 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 1.6827599200542116e-05
  Dropout: 0.4941918510209312
================================================================================

[I 2025-10-30 16:05:18,163] Trial 2555 pruned. OOM: microsoft/deberta-v3-large bs=16 len=224
[I 2025-10-30 16:05:18,724] Trial 2556 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2555 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 48 with grad_accum=3)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 89.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2557 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 8.95468951686473e-06
  Dropout: 0.46967046214633634
================================================================================

[I 2025-10-30 16:05:24,263] Trial 2557 pruned. OOM: roberta-base bs=64 len=224
[I 2025-10-30 16:05:24,441] Trial 2550 pruned. OOM: roberta-base bs=24 len=320
[I 2025-10-30 16:05:24,970] Trial 2559 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2557 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 59.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2550 exceeded GPU memory:
  Model: roberta-base
  Batch size: 24 (effective: 192 with grad_accum=8)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 59.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2558 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.7682032452788176e-05
  Dropout: 0.0019691310801210893
================================================================================


================================================================================
TRIAL 2560 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.0725006622253075e-05
  Dropout: 0.2067072991983407
================================================================================

[I 2025-10-30 16:05:33,475] Trial 2558 pruned. OOM: roberta-base bs=48 len=224

[OOM] Trial 2558 exceeded GPU memory:
  Model: roberta-base
  Batch size: 48 (effective: 96 with grad_accum=2)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 63.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2561 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.263503678039488e-05
  Dropout: 0.2188974603421227
================================================================================

[I 2025-10-30 16:05:36,855] Trial 2561 pruned. OOM: bert-base-uncased bs=64 len=320
[I 2025-10-30 16:05:38,146] Trial 2560 pruned. OOM: bert-base-uncased bs=48 len=256
[I 2025-10-30 16:05:38,652] Trial 2563 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)

[OOM] Trial 2561 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 233.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2562 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 6.541884776932463e-06
  Dropout: 0.3456460490710478
================================================================================


[OOM] Trial 2560 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 48 with grad_accum=1)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 137.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2564 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.300189370991868e-06
  Dropout: 0.49360323100046793
================================================================================

[I 2025-10-30 16:05:43,874] Trial 2564 pruned. OOM: roberta-base bs=24 len=384
[W 2025-10-30 16:05:44,322] The parameter `tok.doc_stride` in Trial#2565 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 16:05:44,371] Trial 2565 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-10-30 16:05:44,804] The parameter `tok.doc_stride` in Trial#2566 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 16:05:45,270] Trial 2562 pruned. OOM: bert-large-uncased bs=12 len=384

[OOM] Trial 2564 exceeded GPU memory:
  Model: roberta-base
  Batch size: 24 (effective: 192 with grad_accum=8)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 55.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2562 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 55.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2566 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.5330194510074206e-05
  Dropout: 0.3813011124105575
================================================================================


================================================================================
TRIAL 2567 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.8803205059359716e-05
  Dropout: 0.12801217481793048
================================================================================

[I 2025-10-30 16:17:52,683] Trial 2566 finished with value: 0.4257703081232493 and parameters: {'seed': 28570, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 5.5330194510074206e-05, 'optim.weight_decay': 0.08739794244553556, 'optim.beta1': 0.8736562775317067, 'optim.beta2': 0.9582844885750367, 'optim.eps': 1.8912685650485888e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.05265037908725686, 'train.clip_grad': 1.1593544864510694, 'model.dropout': 0.3813011124105575, 'model.attn_dropout': 0.2823171401201136, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8971953309142701, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.44720641630683156, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.2201431394782236, 'loss.cls.alpha': 0.6268115373185782, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2568 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 7.984255043671399e-06
  Dropout: 0.008505227536245212
================================================================================

[I 2025-10-30 16:17:58,563] Trial 2568 pruned. OOM: roberta-large bs=12 len=288
[I 2025-10-30 16:17:58,743] Trial 2567 pruned. OOM: xlm-roberta-base bs=8 len=160
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

[OOM] Trial 2568 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 65.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2567 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 245.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2570 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 3.447881934480349e-05
  Dropout: 0.08126234784257204
================================================================================


================================================================================
TRIAL 2569 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.920516691411961e-05
  Dropout: 0.034783486486161044
================================================================================

[I 2025-10-30 16:18:05,780] Trial 2569 pruned. OOM: bert-base-uncased bs=64 len=192
[I 2025-10-30 16:18:05,933] Trial 2570 pruned. OOM: microsoft/deberta-v3-base bs=24 len=192
[W 2025-10-30 16:18:06,365] The parameter `tok.doc_stride` in Trial#2571 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 16:18:07,760] Trial 2547 pruned. OOM: microsoft/deberta-v3-large bs=8 len=128

[OOM] Trial 2570 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 24 (effective: 192 with grad_accum=8)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 120.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2569 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 162.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2571 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.1416319229250737e-05
  Dropout: 0.04984356255789811
================================================================================


================================================================================
TRIAL 2572 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 5.274623355285125e-06
  Dropout: 0.43932529633977935
================================================================================


[OOM] Trial 2547 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 52.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2573 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.0679448183930598e-05
  Dropout: 0.13870861203194618
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 16:18:14,968] Trial 2571 pruned. OOM: bert-base-uncased bs=12 len=128
[I 2025-10-30 16:18:15,129] Trial 2572 pruned. OOM: microsoft/deberta-v3-large bs=16 len=320
[I 2025-10-30 16:18:16,368] Trial 2573 pruned. OOM: roberta-large bs=12 len=128
[W 2025-10-30 16:18:18,391] The parameter `tok.doc_stride` in Trial#2576 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 2571 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 92.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2572 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 132.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2575 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 32
  Learning rate: 5.107976783562633e-05
  Dropout: 0.3511162981488528
================================================================================


[OOM] Trial 2573 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 52.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2574 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 4.312531143511197e-05
  Dropout: 0.0719321725160558
================================================================================


================================================================================
TRIAL 2576 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.192105466892777e-06
  Dropout: 0.35132177674527737
================================================================================

[I 2025-10-30 16:18:23,979] Trial 2575 pruned. OOM: microsoft/deberta-v3-base bs=32 len=288
[I 2025-10-30 16:18:24,228] Trial 2576 pruned. OOM: bert-base-uncased bs=24 len=160
[W 2025-10-30 16:18:25,010] The parameter `tok.doc_stride` in Trial#2578 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 16:18:25,744] Trial 2574 pruned. OOM: bert-large-uncased bs=16 len=256
[W 2025-10-30 16:18:26,371] The parameter `tok.doc_stride` in Trial#2579 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 16:18:26,420] Trial 2579 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2576 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 24 (effective: 144 with grad_accum=6)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2575 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 32 (effective: 256 with grad_accum=8)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2577 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.804588586900626e-05
  Dropout: 0.010751378651585028
================================================================================


================================================================================
TRIAL 2578 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.807612914745922e-06
  Dropout: 0.2895667512038192
================================================================================


[OOM] Trial 2574 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.12 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2580 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.7677218363907726e-05
  Dropout: 0.16123628547827115
================================================================================

[I 2025-10-30 16:35:27,198] Trial 2580 pruned. Pruned at step 9 with metric 0.5371
[I 2025-10-30 16:35:27,677] Trial 2581 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[W 2025-10-30 16:35:28,079] The parameter `tok.doc_stride` in Trial#2582 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2582 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.454681912084133e-05
  Dropout: 0.1474100401860146
================================================================================

[I 2025-10-30 16:42:53,010] Trial 2582 finished with value: 0.613358955197324 and parameters: {'seed': 55167, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.454681912084133e-05, 'optim.weight_decay': 0.024914168182101848, 'optim.beta1': 0.898131431669499, 'optim.beta2': 0.9771665317796143, 'optim.eps': 2.090351983196961e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13160100815158696, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.3851094210460361, 'model.dropout': 0.1474100401860146, 'model.attn_dropout': 0.16965023345325375, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9038866678078289, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.02264577015760552, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.2612659143246114, 'loss.cls.alpha': 0.462762578249524, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 2583 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.555339253882839e-05
  Dropout: 0.18285360803419662
================================================================================

[I 2025-10-30 17:09:26,835] Trial 2577 finished with value: 0.6770975532920298 and parameters: {'seed': 64188, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 3.804588586900626e-05, 'optim.weight_decay': 4.581387017003067e-06, 'optim.beta1': 0.9230772364715333, 'optim.beta2': 0.9780110090523375, 'optim.eps': 2.1053463539588538e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.11021808946229827, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.9093428079751981, 'model.dropout': 0.010751378651585028, 'model.attn_dropout': 0.29501356064831474, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8300282245451529, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3006753236541514, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.2404662763831276, 'loss.cls.alpha': 0.2942874036467024, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 2584 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.0689368232617291e-05
  Dropout: 0.10745323669164987
================================================================================

[I 2025-10-30 17:12:02,546] Trial 2578 finished with value: 0.7101400484783194 and parameters: {'seed': 40148, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.807612914745922e-06, 'optim.weight_decay': 0.0003841359450683691, 'optim.beta1': 0.8882182354998294, 'optim.beta2': 0.9954096006918165, 'optim.eps': 1.8129291703618503e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.11418177270560712, 'train.clip_grad': 0.8554132593272408, 'model.dropout': 0.2895667512038192, 'model.attn_dropout': 0.07629208743504622, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9383355225325055, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.08268258405148501, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.09110106407696558, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 70 (patience=20)

================================================================================
TRIAL 2585 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.0772053580120373e-05
  Dropout: 0.46929672862236255
================================================================================

[I 2025-10-30 17:12:15,329] Trial 2584 pruned. OOM: roberta-base bs=12 len=352
[I 2025-10-30 17:12:16,499] Trial 2583 pruned. OOM: roberta-large bs=12 len=288
[W 2025-10-30 17:12:16,942] The parameter `tok.doc_stride` in Trial#2587 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 2584 exceeded GPU memory:
  Model: roberta-base
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 111.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2586 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.1653703027200676e-06
  Dropout: 0.44287848302328464
================================================================================


[OOM] Trial 2583 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 109.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2587 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.265081993614966e-06
  Dropout: 0.40377505999909313
================================================================================

[I 2025-10-30 17:22:40,463] Trial 2587 pruned. Pruned at step 14 with metric 0.6252

================================================================================
TRIAL 2588 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 8.109645851528184e-06
  Dropout: 0.068493075204481
================================================================================

[I 2025-10-30 17:27:35,819] Trial 2586 pruned. Pruned at step 15 with metric 0.6200
[I 2025-10-30 17:27:36,298] Trial 2589 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-10-30 17:27:36,743] Trial 2590 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2591 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.3706881465815398e-05
  Dropout: 0.3741100476573623
================================================================================

[I 2025-10-30 17:28:20,569] Trial 2588 pruned. Pruned at step 9 with metric 0.5658
[I 2025-10-30 17:28:21,034] Trial 2592 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2593 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 1.9226995737798196e-05
  Dropout: 0.285459888033819
================================================================================

[I 2025-10-30 17:40:40,074] Trial 2593 pruned. Pruned at step 15 with metric 0.6238
[I 2025-10-30 17:40:40,623] Trial 2594 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2595 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.7637096041334157e-05
  Dropout: 0.20718491701339783
================================================================================

[I 2025-10-30 17:50:26,338] Trial 2595 pruned. Pruned at step 20 with metric 0.5905
[I 2025-10-30 17:50:26,819] Trial 2596 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2597 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.0161803780393943e-05
  Dropout: 0.0005779150686994611
================================================================================

[I 2025-10-30 17:52:33,956] Trial 2597 pruned. Pruned at step 8 with metric 0.5124
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2598 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 48
  Learning rate: 2.0311961759864954e-05
  Dropout: 0.48478225003739617
================================================================================

[I 2025-10-30 17:52:38,876] Trial 2598 pruned. OOM: microsoft/deberta-v3-base bs=48 len=352
[W 2025-10-30 17:52:39,428] The parameter `tok.doc_stride` in Trial#2599 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 17:52:41,069] Trial 2585 pruned. OOM: microsoft/deberta-v3-large bs=8 len=128

[OOM] Trial 2598 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 48 (effective: 384 with grad_accum=8)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 274.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 44.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2599 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.348829364004014e-05
  Dropout: 0.07157297511784333
================================================================================


[OOM] Trial 2585 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 70.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

[I 2025-10-30 17:52:41,965] Trial 2600 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2601 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.311979301561243e-06
  Dropout: 0.20470660909979194
================================================================================

[I 2025-10-30 17:58:28,192] Trial 2591 pruned. Pruned at step 9 with metric 0.5796
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2602 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 3.8917315509485976e-05
  Dropout: 0.28890206278717684
================================================================================

[I 2025-10-30 18:05:02,175] Trial 2601 pruned. Pruned at step 13 with metric 0.5917
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2603 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.526167474722114e-05
  Dropout: 0.20349657576722038
================================================================================

[I 2025-10-30 18:06:13,973] Trial 2599 pruned. Pruned at step 27 with metric 0.5733
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2604 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 7.636099927346032e-05
  Dropout: 0.1209853503617615
================================================================================

[I 2025-10-30 18:07:23,596] Trial 2602 pruned. Pruned at step 11 with metric 0.6338
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2605 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 1.5609332398683854e-05
  Dropout: 0.38955351300464613
================================================================================

[I 2025-10-30 18:22:43,410] Trial 2603 finished with value: 0.6693652952152052 and parameters: {'seed': 38190, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.526167474722114e-05, 'optim.weight_decay': 0.00048832425902246, 'optim.beta1': 0.8337028157255099, 'optim.beta2': 0.9917620581804694, 'optim.eps': 8.89593283164342e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.09010004550626223, 'train.clip_grad': 1.2300960584040594, 'model.dropout': 0.20349657576722038, 'model.attn_dropout': 0.21207177620388665, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8561694496372122, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.0005497926537493258, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.10326068689195916, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 2606 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 48
  Learning rate: 9.266046958899559e-05
  Dropout: 0.4168932301790239
================================================================================

[I 2025-10-30 18:22:49,401] Trial 2605 pruned. OOM: microsoft/deberta-v3-base bs=24 len=256
[I 2025-10-30 18:22:49,629] Trial 2606 pruned. OOM: microsoft/deberta-v3-base bs=48 len=384
[I 2025-10-30 18:22:50,221] Trial 2607 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-10-30 18:22:50,761] Trial 2604 pruned. OOM: roberta-base bs=12 len=192
[I 2025-10-30 18:22:51,347] Trial 2609 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)

[OOM] Trial 2606 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 48 (effective: 144 with grad_accum=3)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 94.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2605 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 24 (effective: 48 with grad_accum=2)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 94.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2604 exceeded GPU memory:
  Model: roberta-base
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 74.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2608 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 8.778581029141603e-06
  Dropout: 0.2866974625004563
================================================================================


================================================================================
TRIAL 2610 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 9.422666151858739e-06
  Dropout: 0.04080692916524547
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2611 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.713433199594863e-05
  Dropout: 0.10712132705249777
================================================================================

[I 2025-10-30 18:41:38,333] Trial 2611 pruned. Pruned at step 27 with metric 0.5917
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2612 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.1969316501794564e-05
  Dropout: 0.03754404806386399
================================================================================

[I 2025-10-30 19:00:15,644] Trial 2612 pruned. Pruned at step 11 with metric 0.6148
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2613 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.254767698351883e-06
  Dropout: 0.027332339107048374
================================================================================

[I 2025-10-30 19:06:47,177] Trial 2613 pruned. Pruned at step 10 with metric 0.6273

================================================================================
TRIAL 2614 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.7361910862317342e-05
  Dropout: 0.10297769835114987
================================================================================

[I 2025-10-30 19:06:51,053] Trial 2614 pruned. OOM: bert-base-uncased bs=64 len=256
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2614 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 80.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2615 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 6.47185589552106e-06
  Dropout: 0.013120710617144486
================================================================================

[I 2025-10-30 19:19:32,631] Trial 2615 pruned. Pruned at step 10 with metric 0.6242
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2616 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.0578898131956555e-05
  Dropout: 0.01455976515629168
================================================================================

[I 2025-10-30 19:19:46,972] Trial 2608 pruned. Pruned at step 9 with metric 0.5788
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2617 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 7.562373643656818e-06
  Dropout: 0.41325736103241073
================================================================================

[I 2025-10-30 19:23:37,552] Trial 2610 finished with value: 0.6899159663865546 and parameters: {'seed': 9974, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 9.422666151858739e-06, 'optim.weight_decay': 0.006822885982951047, 'optim.beta1': 0.9358432981550066, 'optim.beta2': 0.9678737338018091, 'optim.eps': 1.494132466054101e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.12349662688735398, 'sched.poly_power': 0.6350165093891373, 'train.clip_grad': 1.0634035447001557, 'model.dropout': 0.04080692916524547, 'model.attn_dropout': 0.1518964026665774, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.859197860597909, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4154676754185921, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.641897138957326, 'loss.cls.alpha': 0.3927356353478895, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 31 (patience=20)

================================================================================
TRIAL 2618 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 9.358429676172032e-06
  Dropout: 0.07917080444759014
================================================================================

[I 2025-10-30 19:29:42,615] Trial 2618 finished with value: 0.7092656791679799 and parameters: {'seed': 24572, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.358429676172032e-06, 'optim.weight_decay': 0.006090058938072371, 'optim.beta1': 0.9310115351808329, 'optim.beta2': 0.9694048280515305, 'optim.eps': 1.5005197222850707e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18128477992714853, 'sched.poly_power': 0.7754972086005947, 'train.clip_grad': 1.4784731112100802, 'model.dropout': 0.07917080444759014, 'model.attn_dropout': 0.24080171863034466, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8364851313002476, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.2865594516038874, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.9598137878685984, 'loss.cls.alpha': 0.7130694120891568, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-10-30 19:29:43,042] The parameter `tok.doc_stride` in Trial#2619 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 2619 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.7365662620731427e-05
  Dropout: 0.27706464297448974
================================================================================

[I 2025-10-30 19:42:42,339] Trial 2619 finished with value: 0.6991412958626073 and parameters: {'seed': 17418, 'model.name': 'xlm-roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.7365662620731427e-05, 'optim.weight_decay': 0.09586746280024322, 'optim.beta1': 0.8367647106164082, 'optim.beta2': 0.9873776740651355, 'optim.eps': 4.6569080209914846e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.06051734902750429, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.5552935324566868, 'model.dropout': 0.27706464297448974, 'model.attn_dropout': 0.13448187365086148, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8858699709371498, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.09550190977172554, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.031168513935375, 'loss.cls.alpha': 0.4947260315428917, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 2620 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 4.668428056600515e-05
  Dropout: 0.32962980662137853
================================================================================

[I 2025-10-30 19:43:08,459] Trial 2616 pruned. Pruned at step 9 with metric 0.6417

================================================================================
TRIAL 2621 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.2808243597294356e-05
  Dropout: 0.357015948991936
================================================================================

[I 2025-10-30 19:45:28,333] Trial 2620 pruned. Pruned at step 10 with metric 0.5731
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2622 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.7684454634585948e-05
  Dropout: 0.0857341968918104
================================================================================

[I 2025-10-30 19:48:04,795] Trial 2621 pruned. Pruned at step 9 with metric 0.5176
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2623 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 32
  Learning rate: 3.3696939384298414e-05
  Dropout: 0.43295530818707517
================================================================================

[I 2025-10-30 19:48:09,819] Trial 2623 pruned. OOM: microsoft/deberta-v3-base bs=32 len=320

[OOM] Trial 2623 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 32 (effective: 32 with grad_accum=1)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 60.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2624 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.2830475111534075e-05
  Dropout: 0.03541886591936875
================================================================================

[I 2025-10-30 19:52:49,231] Trial 2624 finished with value: 0.6667268786127167 and parameters: {'seed': 25654, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.2830475111534075e-05, 'optim.weight_decay': 0.056292221534730055, 'optim.beta1': 0.9422192175589673, 'optim.beta2': 0.9565689432777931, 'optim.eps': 5.5813799222618125e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.16240524668023146, 'sched.poly_power': 0.7798464276606388, 'train.clip_grad': 0.8882636720192587, 'model.dropout': 0.03541886591936875, 'model.attn_dropout': 0.1744931302278842, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8196121390435035, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.4887537620003377, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7930545486713863, 'loss.cls.alpha': 0.6097586321655397, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-30 19:52:49,721] Trial 2625 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-10-30 19:52:50,163] The parameter `tok.doc_stride` in Trial#2626 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 2626 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3169245711807477e-05
  Dropout: 0.016001705139909894
================================================================================

[I 2025-10-30 19:58:10,213] Trial 2622 pruned. Pruned at step 13 with metric 0.6009
[I 2025-10-30 19:58:10,693] Trial 2627 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-10-30 19:58:11,134] Trial 2628 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-10-30 19:58:11,585] Trial 2629 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2630 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.6934728425684853e-05
  Dropout: 0.24203901190010285
================================================================================

[I 2025-10-30 20:03:44,063] Trial 2626 pruned. Pruned at step 27 with metric 0.6200
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2631 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.3544985458812002e-05
  Dropout: 0.10229902289080234
================================================================================

[I 2025-10-30 20:13:14,800] Trial 2630 finished with value: 0.7103658536585367 and parameters: {'seed': 41578, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.6934728425684853e-05, 'optim.weight_decay': 0.06356884873986987, 'optim.beta1': 0.91835037475841, 'optim.beta2': 0.9833272937527177, 'optim.eps': 8.27613804612535e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06276654744340517, 'sched.poly_power': 0.9835614848588364, 'train.clip_grad': 0.9579693722651508, 'model.dropout': 0.24203901190010285, 'model.attn_dropout': 0.1891345281562135, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8932688529122954, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.31224441908008516, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.949345984725056, 'loss.cls.alpha': 0.1780869250129361, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-30 20:13:15,276] Trial 2632 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 2633 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.148097038017296e-06
  Dropout: 0.3625897987225208
================================================================================

[I 2025-10-30 20:49:42,722] Trial 2631 finished with value: 0.43370165745856354 and parameters: {'seed': 61769, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.3544985458812002e-05, 'optim.weight_decay': 0.08810612643878309, 'optim.beta1': 0.8765867956645578, 'optim.beta2': 0.9704964178816409, 'optim.eps': 1.0763587460227006e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.17769890665598187, 'train.clip_grad': 0.7569545452956494, 'model.dropout': 0.10229902289080234, 'model.attn_dropout': 0.26745736032636386, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8297651594071533, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.033001295590943454, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.998866717534498, 'loss.cls.alpha': 0.3997004676358875, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-30 20:49:43,208] Trial 2634 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-10-30 20:49:43,638] The parameter `tok.doc_stride` in Trial#2635 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2635 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 7.199556117311198e-05
  Dropout: 0.4806751163205996
================================================================================

[I 2025-10-30 20:51:47,125] Trial 2635 pruned. Pruned at step 9 with metric 0.5827
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2636 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.471288283642458e-05
  Dropout: 0.08988945365955757
================================================================================

[I 2025-10-30 21:01:37,084] Trial 2633 finished with value: 0.6914487731457174 and parameters: {'seed': 45193, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 6.148097038017296e-06, 'optim.weight_decay': 0.001572478456220991, 'optim.beta1': 0.8607693936758236, 'optim.beta2': 0.9674358044723561, 'optim.eps': 2.4391410675738523e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.12317645665060153, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.7340200512838669, 'model.dropout': 0.3625897987225208, 'model.attn_dropout': 0.10109167353452028, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8464525877054628, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.4202260593761681, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7767551589677373, 'loss.cls.alpha': 0.47111713191808413, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-30 21:01:37,580] Trial 2637 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 2638 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.839379691282124e-06
  Dropout: 0.10937368242232037
================================================================================

[I 2025-10-30 21:04:09,952] Trial 2638 pruned. Pruned at step 9 with metric 0.5927
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2639 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 7.833008776123046e-05
  Dropout: 0.11492717906993566
================================================================================

[I 2025-10-30 21:19:02,515] Trial 2636 finished with value: 0.4444444444444444 and parameters: {'seed': 45090, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.471288283642458e-05, 'optim.weight_decay': 0.18836609334109256, 'optim.beta1': 0.9174736542382682, 'optim.beta2': 0.9503435690577722, 'optim.eps': 3.76712996862856e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.12134839229018898, 'sched.poly_power': 0.5124388556896808, 'train.clip_grad': 1.2807552278145065, 'model.dropout': 0.08988945365955757, 'model.attn_dropout': 0.2659306940852549, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8262844403894033, 'head.pooling': 'mean', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.344014889145319, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.30815014058588, 'loss.cls.alpha': 0.4308083882271787, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2640 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.104828816007833e-06
  Dropout: 0.4888817865496086
================================================================================

[I 2025-10-30 21:39:00,455] Trial 2639 finished with value: 0.4444444444444444 and parameters: {'seed': 47026, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 7.833008776123046e-05, 'optim.weight_decay': 0.03658756952436386, 'optim.beta1': 0.8794474229771797, 'optim.beta2': 0.9731861829519642, 'optim.eps': 3.8547398874918465e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.15803151283969646, 'sched.poly_power': 0.9008668829152309, 'train.clip_grad': 1.407128118492885, 'model.dropout': 0.11492717906993566, 'model.attn_dropout': 0.21047023703436613, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8094411175774805, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.2869035814158602, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1350173246273117, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2641 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 2.456407961283263e-05
  Dropout: 0.43657192035654674
================================================================================

[I 2025-10-30 21:39:06,923] Trial 2641 pruned. OOM: microsoft/deberta-v3-large bs=12 len=384

[OOM] Trial 2641 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 78.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2642 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 24
  Learning rate: 2.716125535250308e-05
  Dropout: 0.4628088271206988
================================================================================

[I 2025-10-30 21:39:13,645] Trial 2642 pruned. OOM: microsoft/deberta-v3-base bs=24 len=288
[I 2025-10-30 21:39:14,204] Trial 2643 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-10-30 21:39:15,403] Trial 2640 pruned. OOM: roberta-large bs=8 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2642 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 24 (effective: 192 with grad_accum=8)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 141.62 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2644 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 4.387217159551599e-05
  Dropout: 0.03409665794887293
================================================================================


[OOM] Trial 2640 exceeded GPU memory:
  Model: roberta-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 41.62 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2645 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.8963804256882276e-05
  Dropout: 0.1479277046432973
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 21:42:26,442] Trial 2644 pruned. Pruned at step 9 with metric 0.5477
[I 2025-10-30 21:42:26,918] Trial 2646 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2647 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 2.729948286886555e-05
  Dropout: 0.3543626401638379
================================================================================

[I 2025-10-30 21:51:18,089] Trial 2645 pruned. Pruned at step 10 with metric 0.5119
[I 2025-10-30 21:51:18,929] Trial 2648 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-10-30 21:51:19,401] Trial 2649 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2650 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.818769827183812e-06
  Dropout: 0.056163510877094786
================================================================================

[I 2025-10-30 21:58:29,551] Trial 2617 pruned. Pruned at step 36 with metric 0.5927

================================================================================
TRIAL 2651 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.4081061222888723e-05
  Dropout: 0.06738606949020214
================================================================================

[I 2025-10-30 22:03:10,930] Trial 2650 pruned. Pruned at step 12 with metric 0.5666
[I 2025-10-30 22:03:11,407] Trial 2652 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2653 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 64
  Learning rate: 1.7343062039342884e-05
  Dropout: 0.3629072794031007
================================================================================

[I 2025-10-30 22:03:17,539] Trial 2651 pruned. OOM: bert-base-uncased bs=32 len=288
[I 2025-10-30 22:03:17,708] Trial 2653 pruned. OOM: microsoft/deberta-v3-base bs=64 len=192
[W 2025-10-30 22:03:18,379] The parameter `tok.doc_stride` in Trial#2654 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-30 22:03:19,237] Trial 2647 pruned. OOM: bert-large-uncased bs=16 len=256
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2653 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 64 (effective: 64 with grad_accum=1)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 118.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2651 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 32 (effective: 64 with grad_accum=2)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 118.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2654 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 4.965134860363683e-05
  Dropout: 0.014325123058115244
================================================================================


[OOM] Trial 2647 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 16 (effective: 16 with grad_accum=1)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 78.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2655 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.1881115244179454e-05
  Dropout: 0.07804610397132722
================================================================================


================================================================================
TRIAL 2656 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 6.69910264677179e-05
  Dropout: 0.10020753204847649
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 22:03:24,714] Trial 2656 pruned. OOM: roberta-base bs=64 len=384
[I 2025-10-30 22:03:26,019] Trial 2655 pruned. OOM: bert-base-uncased bs=48 len=160
[I 2025-10-30 22:03:26,486] Trial 2658 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-10-30 22:03:27,017] The parameter `tok.doc_stride` in Trial#2659 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2656 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 512 with grad_accum=8)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 199.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2657 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.8073993416279242e-05
  Dropout: 0.022211033737112952
================================================================================


[OOM] Trial 2655 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 192 with grad_accum=4)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 59.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2659 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 8.781676348033275e-06
  Dropout: 0.08869027674374544
================================================================================

[I 2025-10-30 22:09:11,297] Trial 2657 pruned. Pruned at step 14 with metric 0.5807
[I 2025-10-30 22:09:11,814] Trial 2660 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 2661 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0189168359411355e-05
  Dropout: 0.03228967356327753
================================================================================

[I 2025-10-30 22:12:34,367] Trial 2661 pruned. Pruned at step 10 with metric 0.6273
[I 2025-10-30 22:12:34,835] Trial 2662 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-10-30 22:12:35,289] Trial 2663 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-10-30 22:12:35,772] Trial 2664 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 2665 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 64
  Learning rate: 3.853198348716176e-05
  Dropout: 0.24723236747702643
================================================================================

[I 2025-10-30 22:12:40,641] Trial 2665 pruned. OOM: microsoft/deberta-v3-base bs=64 len=160
[I 2025-10-30 22:12:42,486] Trial 2659 pruned. OOM: xlm-roberta-base bs=8 len=128
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-10-30 22:12:43,346] Trial 2667 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-10-30 22:12:43,813] Trial 2668 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

[OOM] Trial 2665 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 59.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2666 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 7.006271902898893e-06
  Dropout: 0.32007429856984226
================================================================================


[OOM] Trial 2659 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 441.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.

[W 2025-10-30 22:12:44,539] The parameter `tok.doc_stride` in Trial#2669 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 2669 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 5.17322788392728e-06
  Dropout: 0.4092672101740011
================================================================================

[I 2025-10-30 22:59:57,497] Trial 2666 pruned. Pruned at step 30 with metric 0.6254
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 2670 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.822865160615665e-05
  Dropout: 0.34147294438059783
================================================================================

[I 2025-10-30 23:20:00,053] Trial 2669 finished with value: 0.7204545454545455 and parameters: {'seed': 53543, 'model.name': 'bert-large-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.17322788392728e-06, 'optim.weight_decay': 0.0001304035785260969, 'optim.beta1': 0.8990599690195357, 'optim.beta2': 0.9809440763452582, 'optim.eps': 2.170681402749193e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1538809604280063, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.2590125991131073, 'model.dropout': 0.4092672101740011, 'model.attn_dropout': 0.06356610109636694, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.895163469983517, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.49510721824880194, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.501280402374353, 'loss.cls.alpha': 0.2899987329493219, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 2671 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 6.798663316878442e-05
  Dropout: 0.1377539745212933
================================================================================

[I 2025-10-30 23:25:52,642] Trial 2670 finished with value: 0.4444444444444444 and parameters: {'seed': 53194, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.822865160615665e-05, 'optim.weight_decay': 0.009972435161439419, 'optim.beta1': 0.9293946118625448, 'optim.beta2': 0.984418461006135, 'optim.eps': 4.2182870415802776e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06477721724814031, 'sched.poly_power': 1.069560764853879, 'train.clip_grad': 0.9618048494099091, 'model.dropout': 0.34147294438059783, 'model.attn_dropout': 0.25114910340166813, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9515040642080168, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.37443092161486446, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.549765729275141, 'loss.cls.alpha': 0.46594266945774, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 2672 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.2123570519766586e-06
  Dropout: 0.4832401593117503
================================================================================

[I 2025-10-30 23:45:32,061] Trial 2672 finished with value: 0.6082446082446082 and parameters: {'seed': 41207, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 7.2123570519766586e-06, 'optim.weight_decay': 0.0009180510138341612, 'optim.beta1': 0.8564853640460937, 'optim.beta2': 0.9895240205932178, 'optim.eps': 4.415808122886122e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.17310537008149235, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.6360473987437134, 'model.dropout': 0.4832401593117503, 'model.attn_dropout': 0.14590625574757016, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.911767764928106, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.14328637092670315, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.06880403135349386, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-30 23:45:32,532] Trial 2673 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-10-30 23:45:32,981] Trial 2674 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 50 (patience=20)

================================================================================
TRIAL 2675 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.865862223972365e-06
  Dropout: 0.37655391417454037
================================================================================

[I 2025-10-30 23:49:42,697] Trial 2671 finished with value: 0.6041859746679024 and parameters: {'seed': 34271, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 6.798663316878442e-05, 'optim.weight_decay': 0.008704225619309652, 'optim.beta1': 0.8425077191824052, 'optim.beta2': 0.9797593261597541, 'optim.eps': 1.2189247583878406e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18473572860399587, 'train.clip_grad': 0.8745226263112721, 'model.dropout': 0.1377539745212933, 'model.attn_dropout': 0.29319799925290746, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8072359726122262, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.09708903009861108, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.0290137333589686, 'loss.cls.alpha': 0.3526773120108814, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 2676 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.4441218872402715e-05
  Dropout: 0.0782978438656616
================================================================================

[I 2025-10-31 00:04:03,819] Trial 2675 finished with value: 0.6750132108450876 and parameters: {'seed': 41130, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.865862223972365e-06, 'optim.weight_decay': 0.0004559541986993461, 'optim.beta1': 0.8788638680290015, 'optim.beta2': 0.9837888033156674, 'optim.eps': 7.202115179444064e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.12841411211918996, 'train.clip_grad': 1.115802446240701, 'model.dropout': 0.37655391417454037, 'model.attn_dropout': 0.09205349966562366, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8570042429045551, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.36610258066948675, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.641702504989182, 'loss.cls.alpha': 0.8177393245066928, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-10-31 00:04:04,308] Trial 2677 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 2678 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 6.189254498119326e-06
  Dropout: 0.20816592641754855
================================================================================

[I 2025-10-31 00:04:09,707] Trial 2678 pruned. OOM: bert-large-uncased bs=16 len=384
[I 2025-10-31 00:04:09,868] Trial 2676 pruned. OOM: roberta-large bs=12 len=320
[I 2025-10-31 00:04:10,393] Trial 2679 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-10-31 00:04:10,691] Trial 2680 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-10-31 00:04:11,493] The parameter `tok.doc_stride` in Trial#2682 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-10-31 00:04:11,577] Trial 2682 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 2678 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 62.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 2676 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 62.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 2681 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.928226024814193e-06
  Dropout: 0.04268237937161781
================================================================================


================================================================================
TRIAL 2683 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.3967602459448194e-05
  Dropout: 0.3820329887755124
================================================================================

/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [0,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [1,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [2,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [3,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [4,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [5,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [6,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [7,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [8,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [9,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [10,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [11,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [12,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [13,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [14,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [15,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [16,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [17,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [18,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [19,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [20,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [21,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [22,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [23,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [24,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [25,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [26,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [27,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [28,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [29,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [30,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [31,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [128,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [129,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [130,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [131,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [132,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [133,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [134,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [135,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [136,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [137,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [138,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [139,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [140,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [141,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [142,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [143,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [144,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [145,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [146,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [147,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [148,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [149,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [150,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [151,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [152,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [153,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [154,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [155,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [156,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [157,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [158,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [104,0,0], thread: [159,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/linear.py:134: UserWarning: gemm_and_bias error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 768 n 2048 k 768 mat1_ld 768 mat2_ld 768 result_ld 768 abType 0 cType 0 computeType 68 scaleType 0. Will attempt to recover by calling unfused cublas path. (Triggered internally at /pytorch/aten/src/ATen/cuda/CUDABlas.cpp:1707.)
  return F.linear(input, self.weight, self.bias)
[W 2025-10-31 00:04:14,263] Trial 2654 failed with parameters: {'seed': 26850, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 4.965134860363683e-05, 'optim.weight_decay': 0.004574598664532099, 'optim.beta1': 0.9283752878445029, 'optim.beta2': 0.987033973381669, 'optim.eps': 1.3616406158621552e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06779927915511043, 'sched.poly_power': 0.7676957858017246, 'train.clip_grad': 1.09473381200416, 'model.dropout': 0.014325123058115244, 'model.attn_dropout': 0.257155221174515, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8630169112349756, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.2132064816704864, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.10865874951924803, 'loss.cls.balance': 'none'} because of the following error: RuntimeError('CUDA context corrupted after 3 consecutive failures. Process must restart.').
Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1075, in _obj
    res = run_training_eval(cfg, {"on_epoch": _cb}, trial_number=trial.number)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 853, in run_training_eval
    torch.nn.utils.clip_grad_norm_(
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 43, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 231, in clip_grad_norm_
    total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 43, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 96, in _get_total_norm
    norms.extend(torch._foreach_norm(device_tensors, norm_type))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1163, in _obj
    raise RuntimeError(
RuntimeError: CUDA context corrupted after 3 consecutive failures. Process must restart.
[W 2025-10-31 00:04:14,265] Trial 2654 failed with value None.
[W 2025-10-31 00:04:14,520] Trial 2681 failed with parameters: {'seed': 45607, 'model.name': 'roberta-large', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.928226024814193e-06, 'optim.weight_decay': 0.17831309648984522, 'optim.beta1': 0.9436814550000687, 'optim.beta2': 0.9737973041531439, 'optim.eps': 4.2758300983573005e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.10758646897599719, 'sched.poly_power': 0.5040994889966279, 'train.clip_grad': 0.6701276994029258, 'model.dropout': 0.04268237937161781, 'model.attn_dropout': 0.22646430276038376, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8489970902660443, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.11053816080695728, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.816538532760011, 'loss.cls.alpha': 0.5028784953510329, 'loss.cls.balance': 'none'} because of the following error: RuntimeError('CUDA context corrupted after 3 consecutive failures. Process must restart.').
Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1075, in _obj
    res = run_training_eval(cfg, {"on_epoch": _cb}, trial_number=trial.number)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 809, in run_training_eval
    logits = model(input_ids, attention_mask)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/src/Project/Criteria/models/model.py", line 119, in forward
    outputs = self.encoder(
              ^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 798, in forward
    embedding_output = self.embeddings(
                       ^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 111, in forward
    token_type_embeddings = self.token_type_embeddings(token_type_ids)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 192, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/functional.py", line 2542, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1163, in _obj
    raise RuntimeError(
RuntimeError: CUDA context corrupted after 3 consecutive failures. Process must restart.
[W 2025-10-31 00:04:14,522] Trial 2681 failed with value None.
[W 2025-10-31 00:04:14,687] Trial 2683 failed with parameters: {'seed': 34563, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.3967602459448194e-05, 'optim.weight_decay': 0.0003259337892106637, 'optim.beta1': 0.8957758552963138, 'optim.beta2': 0.9800792473278183, 'optim.eps': 1.724372297812084e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1923005352016803, 'train.clip_grad': 1.1281155036041528, 'model.dropout': 0.3820329887755124, 'model.attn_dropout': 0.14475041926106136, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9007093171846747, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.4899574399402628, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.562944579406767, 'loss.cls.alpha': 0.30618572950022604, 'loss.cls.balance': 'effective_num'} because of the following error: RuntimeError('CUDA context corrupted after 3 consecutive failures. Process must restart.').
Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1075, in _obj
    res = run_training_eval(cfg, {"on_epoch": _cb}, trial_number=trial.number)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 809, in run_training_eval
    logits = model(input_ids, attention_mask)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/src/Project/Criteria/models/model.py", line 119, in forward
    outputs = self.encoder(
              ^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1000, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 650, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 558, in forward
    self_attention_outputs = self.attention(
                             ^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 497, in forward
    attention_output = self.output(self_outputs[0], hidden_states)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 436, in forward
    hidden_states = self.dense(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1163, in _obj
    raise RuntimeError(
RuntimeError: CUDA context corrupted after 3 consecutive failures. Process must restart.
[W 2025-10-31 00:04:14,689] Trial 2683 failed with value None.
Warning: Error during model cleanup: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Error during CUDA cleanup: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Error during model cleanup: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Error during CUDA cleanup: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Error during model cleanup: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Error during CUDA cleanup: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


[CUDA ERROR] Trial 2654 encountered CUDA error (consecutive failures: 3):
  Error Type: AcceleratorError
  Model: roberta-large
  Batch size: 8
  Learning rate: 4.965134860363683e-05
  Dropout: 0.014325123058115244
  Error: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.



================================================================================
FATAL: 3 consecutive CUDA failures detected!
This indicates CUDA context corruption that cannot be recovered.
Raising fatal error to trigger process restart...
================================================================================


[CUDA ERROR] Trial 2681 encountered CUDA error (consecutive failures: 3):
  Error Type: AcceleratorError
  Model: roberta-large
  Batch size: 8
  Learning rate: 5.928226024814193e-06
  Dropout: 0.04268237937161781
  Error: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.



================================================================================
FATAL: 3 consecutive CUDA failures detected!
This indicates CUDA context corruption that cannot be recovered.
Raising fatal error to trigger process restart...
================================================================================


[CUDA ERROR] Trial 2683 encountered CUDA error (consecutive failures: 3):
  Error Type: AcceleratorError
  Model: bert-base-uncased
  Batch size: 8
  Learning rate: 1.3967602459448194e-05
  Dropout: 0.3820329887755124
  Error: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.



================================================================================
FATAL: 3 consecutive CUDA failures detected!
This indicates CUDA context corruption that cannot be recovered.
Raising fatal error to trigger process restart...
================================================================================

Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1075, in _obj
    res = run_training_eval(cfg, {"on_epoch": _cb}, trial_number=trial.number)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 853, in run_training_eval
    torch.nn.utils.clip_grad_norm_(
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 43, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 231, in clip_grad_norm_
    total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 43, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 96, in _get_total_norm
    norms.extend(torch._foreach_norm(device_tensors, norm_type))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1618, in <module>
    main()
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1567, in main
    study.optimize(
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 100, in _optimize
    f.result()
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/concurrent/futures/thread.py", line 59, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 258, in _run_trial
    raise func_err
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1163, in _obj
    raise RuntimeError(
RuntimeError: CUDA context corrupted after 3 consecutive failures. Process must restart.
[W1031 00:05:35.152844456 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
make[1]: *** [Makefile:368: tune-criteria-supermax] Error 1
make[1]: Leaving directory '/media/user/SSD1/YuNing/NoAug_Criteria_Evidence'
make: *** [Makefile:413: tune-all-supermax] Error 2
