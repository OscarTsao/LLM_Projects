num_epochs: 100
batch_size: 24
eval_batch_size: 24
learning_rate: 7.654383756459686e-05
weight_decay: 7.081987993117511e-05
warmup_ratio: 0.056158147782353465
adam_eps: 1.558941813991955e-09
adam_beta1: 0.8723995037336485
adam_beta2: 0.9945170267237899
gradient_accumulation_steps: 4
max_grad_norm: 0.5018938581122676
layerwise_lr_decay: 0.9505792981669728
freeze_encoder_layers: 1
early_stopping_patience: 10
metric_for_best: f1
use_amp: true
auto_resume: true
resume_checkpoint: null
num_workers: 10
prefetch_factor: 4
persistent_workers: true
