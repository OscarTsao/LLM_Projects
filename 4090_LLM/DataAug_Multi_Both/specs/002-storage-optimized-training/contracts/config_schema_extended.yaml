# Extended HPO Configuration Schema (with PEFT, Extended Optimizers, DAPT/TAPT)
# This extends config_schema.yaml with all additional hyperparameters from research_addendum.md

title: TrialConfigExtended
description: Complete hyperparameter and configuration schema including PEFT and advanced optimizations
type: object

required:
  - trial_id
  - model_id
  - input_format
  - optimizer
  - scheduler
  - warmup_strategy
  - learning_rate
  - train_batch_size
  - epochs
  - peft_method
  - freeze_strategy
  - seed

properties:
  # ===========================
  # Core Identifiers
  # ===========================
  trial_id:
    type: string
    format: uuid

  timestamp:
    type: string
    format: date-time

  # ===========================
  # ðŸ§  Backbone / Tokenizer
  # ===========================
  model_id:
    type: string
    enum:
      # Include all 30 models from original schema
      # Subset shown for brevity
      - google-bert/bert-base-uncased
      - microsoft/deberta-v3-base
      - FacebookAI/roberta-large
      - SpanBERT/spanbert-large-cased
      - allenai/longformer-base-4096
      # ... (full list in original config_schema.yaml)

  max_length:
    type: integer
    enum: [256, 384, 512, 768, 1024]
    description: Maximum sequence length for tokenization

  doc_stride:
    type: integer
    enum: [64, 96, 128]
    description: Stride for sliding window (for span tasks on long documents)

  padding_side:
    type: string
    enum: [left, right]
    description: Padding direction

  # ===========================
  # âš™ï¸ Batch / Precision
  # ===========================
  train_batch_size:
    type: integer
    enum: [4, 8, 16, 32, 64, 128, 256]  # Upper bound determined dynamically per model
    description: Training batch size (before gradient accumulation)

  eval_batch_size:
    type: integer
    enum: [8, 16, 32, 64, 128, 256]
    description: Evaluation batch size (can be larger than train since no gradients)

  grad_accum:
    type: integer
    enum: [1, 2, 4, 8]
    description: Gradient accumulation steps (effective_batch_size = train_batch_size * grad_accum)

  fp_precision:
    type: string
    enum: [fp16, bf16, none]
    description: Mixed precision training mode

  grad_clip_norm:
    type: number
    enum: [0.0, 1.0, 2.0, 5.0]
    description: Gradient clipping max norm (0.0 = no clipping)

  gradient_checkpointing:
    type: boolean
    description: Enable gradient checkpointing to reduce memory at cost of compute

  # ===========================
  # ðŸ§© Optimizer
  # ===========================
  optimizer:
    type: string
    enum: [adamw, adam, adafactor, radam, lamb, sgd, sgd_momentum, novograd, adabelief, lion]
    description: Optimizer choice

  encoder_lr:
    type: number
    minimum: 0.000001  # 1e-6
    maximum: 0.00005   # 5e-5
    description: Learning rate for encoder (log scale)

  head_lr:
    type: number
    minimum: 0.000005  # 5e-6
    maximum: 0.0002    # 2e-4
    description: Learning rate for task-specific heads

  peft_lr:
    type: number
    minimum: 0.0001    # 1e-4
    maximum: 0.001     # 1e-3
    description: Learning rate for PEFT parameters (if PEFT enabled)

  weight_decay:
    type: number
    minimum: 0.0
    maximum: 0.1
    description: Weight decay (L2 regularization)

  betas:
    type: array
    items:
      type: number
    minItems: 2
    maxItems: 2
    examples: [[0.9, 0.999], [0.9, 0.98], [0.95, 0.999]]
    description: Adam/AdamW beta parameters (Î²1, Î²2)

  eps:
    type: number
    enum: [1e-8, 1e-7, 1e-6]
    description: Epsilon for numerical stability

  # Optimizer-specific params
  sgd_momentum:
    type: number
    minimum: 0.0
    maximum: 0.99
    description: Momentum for SGD (if optimizer=sgd_momentum)

  sgd_nesterov:
    type: boolean
    description: Use Nesterov momentum (if optimizer=sgd_momentum)

  adafactor_scale_parameter:
    type: boolean
    description: Scale learning rate by root mean square (Adafactor-specific)

  adafactor_relative_step:
    type: boolean
    description: Use relative step sizes (Adafactor-specific)

  # ===========================
  # ðŸ“‰ Scheduler / Warmup
  # ===========================
  scheduler:
    type: string
    enum: [none, linear, cosine, cosine_restart, polynomial, constant, constant_warmup, exponential, one_cycle, reduce_plateau]
    description: Learning rate scheduler

  warmup_strategy:
    type: string
    enum: [ratio, steps, none]
    description: Warmup strategy (ratio=percentage of total steps, steps=fixed number, none=no warmup)

  warmup_ratio:
    type: number
    minimum: 0.0
    maximum: 0.2
    description: Warmup as fraction of total steps (if warmup_strategy=ratio)

  warmup_steps:
    type: integer
    enum: [0, 500, 1000, 2000]
    description: Fixed number of warmup steps (if warmup_strategy=steps)

  # Scheduler-specific params
  cosine_num_cycles:
    type: integer
    enum: [1, 2, 4]
    description: Number of cycles for cosine_restart scheduler

  cosine_T_mult:
    type: integer
    enum: [1, 2]
    description: Multiplier for cycle length in cosine_restart

  polynomial_power:
    type: number
    minimum: 1.0
    maximum: 3.0
    description: Power for polynomial decay scheduler

  exponential_gamma:
    type: number
    minimum: 0.95
    maximum: 0.99
    description: Decay factor for exponential scheduler

  one_cycle_pct_start:
    type: number
    minimum: 0.1
    maximum: 0.3
    description: Percentage of cycle spent increasing LR (OneCycleLR)

  one_cycle_div_factor:
    type: number
    minimum: 10.0
    maximum: 100.0
    description: Initial LR = max_lr / div_factor (OneCycleLR)

  one_cycle_final_div_factor:
    type: number
    minimum: 100.0
    maximum: 10000.0
    description: Final LR = max_lr / final_div_factor (OneCycleLR)

  reduce_plateau_patience:
    type: integer
    enum: [2, 5, 10]
    description: Epochs with no improvement before LR reduction (ReduceLROnPlateau)

  reduce_plateau_factor:
    type: number
    minimum: 0.5
    maximum: 0.9
    description: Factor by which to reduce LR (ReduceLROnPlateau)

  reduce_plateau_threshold:
    type: number
    minimum: 0.0001
    maximum: 0.01
    description: Threshold for measuring improvement (ReduceLROnPlateau)

  # ===========================
  # ðŸ”’ Regularization
  # ===========================
  encoder_dropout:
    type: number
    minimum: 0.0
    maximum: 0.2
    description: Dropout rate within encoder layers

  attn_dropout:
    type: number
    minimum: 0.0
    maximum: 0.2
    description: Dropout on attention weights

  head_dropout:
    type: number
    minimum: 0.1
    maximum: 0.5
    description: Dropout in task-specific heads

  token_dropout:
    type: number
    minimum: 0.0
    maximum: 0.2
    description: Token-level dropout (randomly mask tokens during training)

  label_smoothing:
    type: number
    minimum: 0.0
    maximum: 0.2
    description: Label smoothing factor

  layerwise_lr_decay:
    type: number
    minimum: 0.75
    maximum: 0.95
    description: Layer-wise learning rate decay factor (lower layers get lower LR)

  # ===========================
  # ðŸŽ¯ Criteria Matching Head
  # ===========================
  pooling:
    type: string
    enum: [cls, mean, attn1, attn4, scalar_mix_cls]
    description: Pooling strategy over encoder hidden states

  head_type:
    type: string
    enum: [linear, mlp1, mlp2_res, glu, ms_dropout]
    description: Architecture type for criteria matching head

  hidden_size:
    type: integer
    enum: [256, 384, 512, 768, 1024]
    description: Hidden dimension for MLP heads

  activation:
    type: string
    enum: [gelu, silu, relu, leakyrelu, mish, tanh]
    description: Activation function

  ms_dropout_passes:
    type: integer
    enum: [1, 4, 8]
    description: Number of dropout passes for multi-sample dropout (if head_type=ms_dropout)

  loss_type:
    type: string
    enum: [ce, focal, bce, weighted_bce, adaptive_focal, hybrid]
    description: Loss function for criteria matching

  focal_gamma:
    type: number
    minimum: 1.0
    maximum: 3.0
    description: Focusing parameter for focal loss

  class_weights_enabled:
    type: boolean
    description: Whether to apply class weighting to loss

  decision_threshold_tau:
    type: number
    minimum: 0.3
    maximum: 0.8
    description: Classification threshold (for binary predictions from sigmoid)

  # ===========================
  # ðŸ” Evidence Binding Head
  # ===========================
  span_head:
    type: string
    enum: [linear2, mlp2, biaffine, bio_crf]
    description: Architecture for evidence span prediction

  max_spans_per_doc:
    type: integer
    enum: [3, 5, 10]
    description: Maximum number of evidence spans to extract per document

  max_span_len_chars:
    type: integer
    enum: [128, 256]
    description: Maximum character length for a valid evidence span

  use_sentence_reranker:
    type: boolean
    description: Use sentence-level reranker for evidence extraction

  reranker_topk_sent:
    type: integer
    enum: [3, 5, 10]
    description: Top-K sentences to consider (if reranker enabled)

  span_nms_iou:
    type: number
    minimum: 0.4
    maximum: 0.7
    description: IoU threshold for non-maximum suppression on spans

  char_level_metric:
    type: boolean
    description: Compute character-level F1 for evidence spans (in addition to token-level)

  # ===========================
  # ðŸ”— Joint Training
  # ===========================
  input_format:
    type: string
    enum: [binary_pairs, multi_label]
    description: Data formatting (binary post-criterion pairs vs multi-label)

  lambda_class:
    type: number
    minimum: 0.5
    maximum: 2.0
    description: Loss weight for criteria matching task

  lambda_span:
    type: number
    minimum: 0.5
    maximum: 2.0
    description: Loss weight for evidence binding task

  use_span2cls_concat:
    type: boolean
    description: Concatenate span embeddings to criteria matching input (task coupling)

  span_pooling:
    type: string
    enum: [mean, max, attn1]
    description: Pooling method for span embeddings (if use_span2cls_concat=true)

  # ===========================
  # ðŸ§± PEFT: LoRA / Adapters / IAÂ³
  # ===========================
  peft_method:
    type: string
    enum: [none, lora, lora_plus, adalora, pfeiffer, houlsby, compacter, ia3]
    description: Parameter-Efficient Fine-Tuning method

  peft_target_modules:
    type: array
    items:
      type: string
    examples: [["query", "value"], ["query", "key", "value", "output"], ["query", "value", "ffn.up", "ffn.down"]]
    description: Attention/FFN modules to apply PEFT to

  peft_r:
    type: integer
    enum: [4, 8, 16, 32]
    description: LoRA rank (if peft_method=lora/lora_plus/adalora)

  peft_alpha:
    type: integer
    enum: [8, 16, 32, 64]
    description: LoRA alpha scaling parameter

  peft_dropout:
    type: number
    minimum: 0.0
    maximum: 0.2
    description: Dropout for PEFT layers

  peft_bias:
    type: string
    enum: [none, lora_only, all]
    description: Which biases to train (if peft_method=lora)

  adapter_bottleneck:
    type: integer
    enum: [32, 64, 128, 192]
    description: Bottleneck dimension for adapter layers (if peft_method=pfeiffer/houlsby)

  adapter_dropout:
    type: number
    minimum: 0.1
    maximum: 0.3
    description: Dropout for adapter layers

  adapter_layers:
    type: string
    enum: [all, top4, top8]
    description: Which transformer layers get adapters

  ia3_enable_ffn:
    type: boolean
    description: Apply IAÂ³ to FFN in addition to attention (if peft_method=ia3)

  prefix_len:
    type: integer
    enum: [10, 20, 50]
    description: Prefix tuning length (if using prefix tuning variant)

  prompt_len:
    type: integer
    enum: [10, 20, 50]
    description: Prompt tuning length (if using prompt tuning variant)

  # ===========================
  # ðŸ”’ Encoder Freezing Strategy
  # ===========================
  freeze_strategy:
    type: string
    enum: [none, freeze_layers_n, freeze_encoder, adapter]
    description: Encoder freezing strategy

  freeze_layers_n:
    type: integer
    enum: [0, 2, 6]
    description: Number of bottom encoder layers to freeze (if freeze_strategy=freeze_layers_n)

  unfreeze_layernorm:
    type: boolean
    description: Unfreeze LayerNorm layers even if encoder frozen

  unfreeze_pooler:
    type: boolean
    description: Unfreeze pooler layer even if encoder frozen

  # ===========================
  # ðŸ§¨ Adversarial Training
  # ===========================
  adv_training:
    type: string
    enum: [none, fgm, pgd]
    description: Adversarial training method

  adv_eps:
    type: number
    minimum: 0.000001  # 1e-6
    maximum: 0.005     # 5e-3
    description: Adversarial perturbation magnitude (log scale)

  adv_steps:
    type: integer
    enum: [1, 2, 3]
    description: Number of adversarial attack steps (for PGD)

  # ===========================
  # ðŸ§¬ DAPT / TAPT
  # ===========================
  use_dapt:
    type: boolean
    description: Use Domain Adaptive Pretraining before task fine-tuning

  dapt_epochs:
    type: integer
    enum: [0, 1, 2, 3]
    description: Number of DAPT epochs (0 = skip DAPT)

  dapt_mlm_prob:
    type: number
    minimum: 0.15
    maximum: 0.3
    description: Masking probability for DAPT masked language modeling

  use_tapt:
    type: boolean
    description: Use Task Adaptive Pretraining before task fine-tuning

  tapt_epochs:
    type: integer
    enum: [0, 1, 2]
    description: Number of TAPT epochs (0 = skip TAPT)

  tapt_mlm_prob:
    type: number
    minimum: 0.15
    maximum: 0.3
    description: Masking probability for TAPT masked language modeling

  masking_style:
    type: string
    enum: [token, whole_word, span]
    description: Masking strategy for MLM pretraining

  # ===========================
  # ðŸ§  Data Augmentation
  # ===========================
  aug_library:
    type: string
    enum: [none, nlpaug, textattack]
    description: Data augmentation library to use

  aug_ops:
    type: array
    items:
      type: string
      enum: [synonym, backtranslation, delete, contextual]
    examples: [["synonym"], ["synonym", "delete"], ["contextual"]]
    description: Augmentation operations to apply

  aug_protect_list_enabled:
    type: boolean
    description: Protect specific tokens (e.g., domain terms) from augmentation

  aug_prob:
    type: number
    minimum: 0.0
    maximum: 0.3
    description: Per-example probability of applying augmentation

  # ===========================
  # ðŸ§© Semi-supervised & Mining
  # ===========================
  use_pseudo_labels:
    type: boolean
    description: Use pseudo-labeling on unlabeled data

  pl_conf_thresh:
    type: number
    minimum: 0.7
    maximum: 0.95
    description: Confidence threshold for pseudo-labels

  pl_weight:
    type: number
    minimum: 0.2
    maximum: 1.0
    description: Loss weight for pseudo-labeled examples

  hard_negative_mining:
    type: boolean
    description: Use hard negative mining to focus on difficult examples

  hnm_ratio:
    type: number
    minimum: 0.1
    maximum: 0.5
    description: Ratio of hard negatives to include in training

  # ===========================
  # ðŸ‹ï¸ Training Schedule
  # ===========================
  epochs:
    type: integer
    minimum: 1
    maximum: 100
    description: Maximum number of training epochs

  early_stopping_patience:
    type: integer
    default: 20
    description: Early stopping patience (epochs with no improvement)

  early_stopping_min_delta:
    type: number
    default: 0.001
    description: Minimum improvement to qualify as progress (early stopping)

  # ===========================
  # ðŸ“Š Optimization Metric
  # ===========================
  optimization_metric:
    type: string
    examples: [val_f1_macro, val_f1_micro, val_accuracy]
    description: Metric to maximize during HPO

  # ===========================
  # ðŸ’¾ Checkpoint Retention
  # ===========================
  keep_last_n:
    type: integer
    minimum: 1
    default: 3

  keep_best_k:
    type: integer
    minimum: 1
    default: 5

  max_checkpoint_size_gb:
    type: number
    minimum: 0.1
    default: 50.0

  # ===========================
  # ðŸŽ² Reproducibility
  # ===========================
  seed:
    type: integer
    minimum: 0
    description: Random seed for reproducibility

  # ===========================
  # ðŸ–¥ï¸ Hardware Configuration
  # ===========================
  num_workers:
    type: integer
    default: 12
    description: Number of DataLoader workers (match CPU cores)

  pin_memory:
    type: boolean
    default: true
    description: Pin memory for faster CPU->GPU transfer

  persistent_workers:
    type: boolean
    default: true
    description: Keep DataLoader workers alive between epochs

# ============================================
# Conditional Dependencies
# ============================================
# IF peft_method != "none" THEN peft_* params MUST be specified
# IF freeze_strategy == "freeze_layers_n" THEN freeze_layers_n MUST be specified
# IF warmup_strategy == "ratio" THEN warmup_ratio MUST be specified
# IF warmup_strategy == "steps" THEN warmup_steps MUST be specified
# IF scheduler == "cosine_restart" THEN cosine_num_cycles, cosine_T_mult MUST be specified
# IF scheduler == "one_cycle" THEN one_cycle_* params MUST be specified
# IF scheduler == "reduce_plateau" THEN reduce_plateau_* params MUST be specified
# IF use_dapt == true THEN dapt_epochs, dapt_mlm_prob MUST be specified
# IF use_tapt == true THEN tapt_epochs, tapt_mlm_prob MUST be specified
# IF adv_training != "none" THEN adv_eps, adv_steps MUST be specified
# IF loss_type contains "focal" THEN focal_gamma MUST be specified
