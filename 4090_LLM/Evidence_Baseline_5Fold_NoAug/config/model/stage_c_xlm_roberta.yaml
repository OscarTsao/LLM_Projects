pretrained_model_name: xlm-roberta-base
max_seq_length: 160
classifier_hidden_sizes:
  - 384
dropout: 0.12608192254733397
encoder_dropout: 0.24752388097258377
attention_dropout: 0.1068400619645917
loss_type: adaptive_focal
alpha: 0.25
gamma: 2.0
delta: 1.0
use_gradient_checkpointing: false
freeze_encoder_layers: 1
