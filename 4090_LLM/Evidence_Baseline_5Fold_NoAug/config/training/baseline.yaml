num_epochs: 20
batch_size: 8
eval_batch_size: 8
learning_rate: 5.715491938156607e-06
weight_decay: 0.00964492121875502
warmup_ratio: 0.16648852816008436
adam_eps: 1.0e-08
gradient_accumulation_steps: 2
max_grad_norm: 1.0
early_stopping_patience: 10
metric_for_best: f1
use_amp: true
auto_resume: true
resume_checkpoint: null
num_workers: 8
prefetch_factor: 4
persistent_workers: true
