seed: 42

encoder:
  model_name: "microsoft/deberta-v3-base"
  revision: "main"
  tokenizer_name: "microsoft/deberta-v3-base"
  gradient_checkpointing: false

tokenizer:
  max_length: 384
  padding: true
  truncation: true

train:
  num_epochs: 100
  per_device_batch_size: 16
  grad_accum_steps: 1
  max_length: 384
  amp: "bf16"
  grad_clip_norm: 1.0
  torch_compile: false
  logging_steps: 25
  eval_frequency: 1
  save_frequency: 1
  # DataLoader optimization settings
  num_workers: 8  # Parallel data loading workers (increased from 2)
  eval_num_workers: 8  # Workers for evaluation (can be higher since eval is faster)
  eval_batch_size_multiplier: 2.0  # Eval can use 2x batch size (no gradients)
  deterministic: true
  early_stopping:
    patience: 20
    min_delta: 1.0e-6

optim:
  optimizer: "adamw"
  lr_encoder: 1.0e-5
  lr_head: 5.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

sched:
  type: "linear"
  warmup_ratio: 0.1
  num_cycles: 0
  min_lr_ratio: 0.0

loss:
  label_smoothing: 0.0
  class_weighting: "none"

mtl:
  task_weight_evidence: 0.5

heads:
  evidence:
    num_classes: 3
    pooler_type: "cls"
    type: "linear"
    activation: "gelu"
    dropout: 0.1
    norm: "none"
  criteria:
    num_classes: 5
    pooler_type: "cls"
    type: "linear"
    activation: "gelu"
    dropout: 0.1
    norm: "none"

objective:
  primary_metric: "val_ev_macro_f1"
  composite:
    enabled: false
    weights:
      val_ev_macro_f1: 0.5
      val_cr_macro_f1: 0.5

checkpoint:
  dir: "./experiments"
  filename: "checkpoint.pt"
  save_best_only: true
