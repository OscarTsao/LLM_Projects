# Gemma Encoder Configuration for ReDSM5 Criteria Matching

# Model settings
model:
  name: "google/gemma-2-2b"  # Options: google/gemma-2-2b, google/gemma-2-9b, google/gemma-2-27b
  pooling_strategy: "mean"  # Options: mean, first_k, last_k, attention_kv, attention_query
  freeze_encoder: false

# Classification head
classifier:
  hidden_size: 768  # Set to null for simple linear layer
  dropout: 0.1

# Training
training:
  batch_size: 16
  num_epochs: 10
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0

# Data
data:
  max_length: 512
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  data_dir: "/media/cvrlab308/cvrlab308_4090/YuNing/LLM_Criteria_Gemma/data/redsm5"

# Hyperparameter optimization
hpo:
  n_trials: 50
  study_name: "gemma_redsm5_criteria"

  # Search space
  pooling_strategies: ["mean", "attention_kv", "attention_query"]
  dropout_rates: [0.05, 0.1, 0.15, 0.2, 0.3]
  hidden_sizes: [384, 512, 768, 1024]
  learning_rates: [1e-5, 2e-5, 3e-5, 5e-5]
  batch_sizes: [8, 16, 32]

# Evaluation
evaluation:
  metrics: ["accuracy", "precision", "recall", "f1", "confusion_matrix"]
  save_predictions: true

# Outputs
output:
  dir: "./outputs"
  save_best_only: true
  checkpoint_frequency: 1  # epochs
