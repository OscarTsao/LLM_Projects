[0;34m======================================================================
  Running Super-Max HPO for ALL Architectures Sequentially
======================================================================[0m 

[0;33mSequence: Criteria â†’ Evidence â†’ Share â†’ Joint[0m 
[0;33mTotal trials: ~19,000 (5000+8000+3000+3000)[0m 
[0;33mEstimated time: ~120-180 hours with PAR=2 (reduced for GPU stability)[0m 

[0;32mStarting...[0m 

[0;34m[1/4] Running Criteria (5000 trials)...[0m 
make[1]: Entering directory '/media/user/SSD1/YuNing/NoAug_Criteria_Evidence'
[0;34mRunning SUPER-MAX HPO for Criteria...[0m 
Trials: 5000 | Parallel: 2 (reduced 4â†’3â†’2 for stability) | Epochs: 100 | Patience: 20
PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python HPO_EPOCHS=100 HPO_PATIENCE=20 poetry run python scripts/tune_max.py \
	--agent criteria --study noaug-criteria-supermax \
	--n-trials 5000 --parallel 2 \
	--outdir ./_runs
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py:975: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return PatientPruner(hb, patience=4)  # More patient (was 2)
[I 2025-11-01 23:07:02,469] Using an existing study with name 'noaug-criteria-supermax' instead of creating a new one.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
âœ“ Configuration validation passed
  Agent: criteria
  Epochs: 100 | Patience: 20
  Output: ./_runs
[HPO] agent=criteria epochs=100 storage=sqlite:////media/user/SSD1/YuNing/NoAug_Criteria_Evidence/_optuna/noaug.db
[HPO] Study 'noaug-criteria-supermax' is compatible. Resuming optimization.

================================================================================
TRIAL 3384 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 3.0290528206542202e-05
  Dropout: 0.002047505203241505
================================================================================


================================================================================
TRIAL 3385 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.0459762246252453e-05
  Dropout: 0.393367659040812
================================================================================

[I 2025-11-01 23:09:33,699] Trial 3384 pruned. Pruned at step 15 with metric 0.5240
[I 2025-11-01 23:09:34,233] Trial 3386 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3387 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.346652180089276e-06
  Dropout: 0.00857053595488779
================================================================================

[I 2025-11-01 23:13:26,032] Trial 3387 pruned. Pruned at step 8 with metric 0.6070
[I 2025-11-01 23:13:26,551] Trial 3388 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-01 23:13:26,989] The parameter `tok.doc_stride` in Trial#3389 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3389 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.37429947433536e-06
  Dropout: 0.054501898581176225
================================================================================

[I 2025-11-01 23:18:50,579] Trial 3389 pruned. Pruned at step 27 with metric 0.6580
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3390 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.4014082451006714e-05
  Dropout: 0.1615046475553624
================================================================================

[I 2025-11-01 23:23:57,800] Trial 3390 pruned. Pruned at step 14 with metric 0.5705

================================================================================
TRIAL 3391 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1072002708654014e-05
  Dropout: 0.2855118411834692
================================================================================

[I 2025-11-01 23:30:27,826] Trial 3391 pruned. Pruned at step 15 with metric 0.5819

================================================================================
TRIAL 3392 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.476101722283481e-05
  Dropout: 0.43262269785878593
================================================================================

[I 2025-11-01 23:35:56,315] Trial 3392 pruned. Pruned at step 17 with metric 0.6569
[I 2025-11-01 23:35:56,979] Trial 3393 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-01 23:35:57,449] Trial 3394 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-01 23:35:57,922] Trial 3395 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-01 23:35:58,356] The parameter `tok.doc_stride` in Trial#3396 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3396 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.0108140831647886e-05
  Dropout: 0.08812815991394209
================================================================================

[I 2025-11-01 23:37:45,319] Trial 3396 pruned. Pruned at step 9 with metric 0.6208

================================================================================
TRIAL 3397 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.161581369410509e-05
  Dropout: 0.42360547281502553
================================================================================

[I 2025-11-01 23:41:20,922] Trial 3385 pruned. Pruned at step 12 with metric 0.6124
[I 2025-11-01 23:41:21,502] Trial 3398 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3399 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.1815888502326844e-05
  Dropout: 0.008516887926042369
================================================================================

[I 2025-11-01 23:53:18,296] Trial 3397 pruned. Pruned at step 27 with metric 0.5705
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3400 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.029506877162541e-05
  Dropout: 0.24050618426382026
================================================================================

[I 2025-11-01 23:53:26,292] Trial 3400 pruned. OOM: microsoft/deberta-v3-large bs=12 len=256
[I 2025-11-01 23:53:26,913] Trial 3401 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)

[OOM] Trial 3400 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 106.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3402 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 1.0684048761242383e-05
  Dropout: 0.08509253568348907
================================================================================

[I 2025-11-02 00:02:43,413] Trial 3402 pruned. Pruned at step 7 with metric 0.5586
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3403 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 8.904725088929628e-06
  Dropout: 0.37049103325382127
================================================================================

[I 2025-11-02 00:03:59,648] Trial 3399 finished with value: 0.6981094527363184 and parameters: {'seed': 4791, 'model.name': 'xlm-roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.1815888502326844e-05, 'optim.weight_decay': 5.2229663735669126e-05, 'optim.beta1': 0.8476236416031632, 'optim.beta2': 0.9700703316304244, 'optim.eps': 5.9493644602318097e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1452154066397108, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.9571344349343349, 'model.dropout': 0.008516887926042369, 'model.attn_dropout': 0.1566018218758176, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8694881112093789, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.24182990144344702, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.09862704443965443, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-02 00:04:00,090] The parameter `tok.doc_stride` in Trial#3404 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3404 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 4.313086195523716e-05
  Dropout: 0.3515099265007027
================================================================================

[I 2025-11-02 00:33:24,759] Trial 3403 pruned. Pruned at step 11 with metric 0.5371
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3405 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 9.95792581518646e-06
  Dropout: 0.17907857227008284
================================================================================

[I 2025-11-02 00:42:07,331] Trial 3404 finished with value: 0.43370165745856354 and parameters: {'seed': 50849, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 4.313086195523716e-05, 'optim.weight_decay': 0.004309608187252893, 'optim.beta1': 0.9260970323488804, 'optim.beta2': 0.963413505776819, 'optim.eps': 7.801223951803937e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.10650637536851411, 'sched.poly_power': 0.8716333551794412, 'train.clip_grad': 1.2919698044015973, 'model.dropout': 0.3515099265007027, 'model.attn_dropout': 0.19546634963388665, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.9039141938192171, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.45989725784237057, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5523707323424825, 'loss.cls.alpha': 0.5972773222575007, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 00:42:07,837] Trial 3406 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3407 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 7.034842998525271e-06
  Dropout: 0.3419267358835514
================================================================================

[I 2025-11-02 00:56:17,329] Trial 3405 pruned. Pruned at step 27 with metric 0.5656

================================================================================
TRIAL 3408 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.871552272170212e-06
  Dropout: 0.0811086049448044
================================================================================

[I 2025-11-02 00:59:25,919] Trial 3408 pruned. Pruned at step 10 with metric 0.6038
[I 2025-11-02 00:59:26,412] Trial 3409 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3410 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 2.3857180781829674e-05
  Dropout: 0.4816803602661143
================================================================================

[I 2025-11-02 01:15:19,174] Trial 3407 pruned. Pruned at step 22 with metric 0.5316

================================================================================
TRIAL 3411 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 5.384456446065755e-06
  Dropout: 0.40693663175144035
================================================================================

[I 2025-11-02 01:15:53,024] Trial 3410 pruned. Pruned at step 8 with metric 0.6215
[I 2025-11-02 01:15:53,606] Trial 3412 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3413 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 9.195735833040517e-06
  Dropout: 0.4527969555850859
================================================================================

[I 2025-11-02 01:21:46,803] Trial 3413 pruned. Pruned at step 10 with metric 0.6135
[I 2025-11-02 01:21:47,285] Trial 3414 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3415 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.380180646683906e-06
  Dropout: 0.003581972966834962
================================================================================

[I 2025-11-02 01:24:33,015] Trial 3411 pruned. Pruned at step 24 with metric 0.5743
[W 2025-11-02 01:24:33,478] The parameter `tok.doc_stride` in Trial#3416 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 01:24:33,529] Trial 3416 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-02 01:24:33,989] Trial 3417 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3418 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.021477358587214e-05
  Dropout: 0.338944661527107
================================================================================

[I 2025-11-02 01:28:15,483] Trial 3415 pruned. Pruned at step 8 with metric 0.6299
[W 2025-11-02 01:28:15,948] The parameter `tok.doc_stride` in Trial#3419 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3419 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.5962744013129565e-05
  Dropout: 0.34028573264812795
================================================================================

[I 2025-11-02 01:29:56,360] Trial 3419 pruned. Pruned at step 9 with metric 0.6618
[I 2025-11-02 01:29:56,847] Trial 3420 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3421 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.1185032670064114e-05
  Dropout: 0.29447481174127244
================================================================================

[I 2025-11-02 01:33:02,750] Trial 3418 pruned. Pruned at step 14 with metric 0.5496

================================================================================
TRIAL 3422 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.3652077042137519e-05
  Dropout: 0.27496736626266544
================================================================================

[I 2025-11-02 01:34:19,100] Trial 3421 pruned. Pruned at step 7 with metric 0.6254

================================================================================
TRIAL 3423 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 4.552643302811121e-05
  Dropout: 0.05191218819617347
================================================================================

[I 2025-11-02 01:40:43,584] Trial 3423 finished with value: 0.67752382714907 and parameters: {'seed': 51741, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 4.552643302811121e-05, 'optim.weight_decay': 0.167365311473597, 'optim.beta1': 0.8784441511564544, 'optim.beta2': 0.9712700414654097, 'optim.eps': 1.4509999510260898e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1854464107105558, 'train.clip_grad': 0.708665676524171, 'model.dropout': 0.05191218819617347, 'model.attn_dropout': 0.25798341833339544, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8248828158612868, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.2471634354711563, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.0947921608117228, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 01:40:44,062] Trial 3424 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 3425 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.210556304167119e-05
  Dropout: 0.25581400274610155
================================================================================

[I 2025-11-02 01:45:37,456] Trial 3425 pruned. Pruned at step 9 with metric 0.5839
[I 2025-11-02 01:45:37,933] Trial 3426 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-02 01:45:38,402] Trial 3427 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-02 01:45:38,858] The parameter `tok.doc_stride` in Trial#3428 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 01:45:38,908] Trial 3428 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-02 01:45:39,336] The parameter `tok.doc_stride` in Trial#3429 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3429 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.1929977185457168e-05
  Dropout: 0.42773910609230437
================================================================================

[I 2025-11-02 01:49:09,561] Trial 3422 pruned. Pruned at step 14 with metric 0.5801
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3430 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 4.7545164275049746e-05
  Dropout: 0.09077085563687748
================================================================================

[I 2025-11-02 01:58:05,576] Trial 3430 finished with value: 0.6973394799481756 and parameters: {'seed': 13091, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 4.7545164275049746e-05, 'optim.weight_decay': 0.08507595068395143, 'optim.beta1': 0.8923731745333137, 'optim.beta2': 0.9743477546882449, 'optim.eps': 1.1103235374593571e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.10534671861916502, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.807058758539639, 'model.dropout': 0.09077085563687748, 'model.attn_dropout': 0.15850773171845767, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9035324485499245, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.06618786187965141, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.737822174942548, 'loss.cls.alpha': 0.6476367131812372, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-02 01:58:06,032] The parameter `tok.doc_stride` in Trial#3431 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 01:58:06,083] Trial 3431 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-02 01:58:06,550] Trial 3432 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 3433 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.8595872576060242e-05
  Dropout: 0.20215417950121756
================================================================================

[I 2025-11-02 02:19:28,657] Trial 3429 finished with value: 0.657129639514608 and parameters: {'seed': 63947, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.1929977185457168e-05, 'optim.weight_decay': 0.14874398501041053, 'optim.beta1': 0.8323355158515288, 'optim.beta2': 0.976820908550476, 'optim.eps': 6.937133320063598e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.08678592218659499, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.6190433910201363, 'model.dropout': 0.42773910609230437, 'model.attn_dropout': 0.13074557670241055, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8185420525365412, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.09300557288454966, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.04743713719770645, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 02:19:29,150] Trial 3434 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 3435 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 3.386049808741252e-05
  Dropout: 0.0668223681704429
================================================================================

[I 2025-11-02 02:36:05,605] Trial 3435 finished with value: 0.4383561643835616 and parameters: {'seed': 26147, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.386049808741252e-05, 'optim.weight_decay': 0.0005684468656395636, 'optim.beta1': 0.8334735015189267, 'optim.beta2': 0.9719507464035914, 'optim.eps': 1.737633334985008e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.14451500265085576, 'train.clip_grad': 1.1111438645676674, 'model.dropout': 0.0668223681704429, 'model.attn_dropout': 0.1342508876134883, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.834431681201852, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.20499588005292763, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.016827987843295228, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 02:36:06,090] Trial 3436 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-02 02:36:06,573] Trial 3437 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3438 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.597643507563686e-05
  Dropout: 0.19130392501574406
================================================================================

[I 2025-11-02 02:46:14,643] Trial 3433 finished with value: 0.6232247284878863 and parameters: {'seed': 60981, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.8595872576060242e-05, 'optim.weight_decay': 0.17995916807510187, 'optim.beta1': 0.9270721282128157, 'optim.beta2': 0.9551158766849899, 'optim.eps': 6.920918765829182e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18519493428716444, 'sched.poly_power': 0.7641487566070599, 'train.clip_grad': 1.1422344827917057, 'model.dropout': 0.20215417950121756, 'model.attn_dropout': 0.18996217108703267, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8368975557829103, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.3863861257206531, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.3997706014431137, 'loss.cls.alpha': 0.4786713987067363, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 3439 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.644626598335907e-05
  Dropout: 0.2934486333308543
================================================================================

[I 2025-11-02 02:46:45,293] Trial 3438 pruned. Pruned at step 15 with metric 0.5676
[W 2025-11-02 02:46:45,756] The parameter `tok.doc_stride` in Trial#3440 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 02:46:45,813] Trial 3440 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3441 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.8362415669270313e-05
  Dropout: 0.22417881021630506
================================================================================

[I 2025-11-02 02:57:57,201] Trial 3441 finished with value: 0.6750132108450876 and parameters: {'seed': 53453, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.8362415669270313e-05, 'optim.weight_decay': 0.0007485120487562533, 'optim.beta1': 0.8423643609266447, 'optim.beta2': 0.9748983251002012, 'optim.eps': 7.384505805163711e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.15905458409766232, 'train.clip_grad': 0.9486602819920135, 'model.dropout': 0.22417881021630506, 'model.attn_dropout': 0.11997575382456216, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.917257924559933, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.21188947525632768, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.4310744311524415, 'loss.cls.alpha': 0.4611229384917525, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 02:57:57,703] Trial 3442 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-02 02:57:58,139] The parameter `tok.doc_stride` in Trial#3443 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 02:57:58,191] Trial 3443 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3444 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.580086850432432e-05
  Dropout: 0.23437818413533545
================================================================================

[I 2025-11-02 02:59:16,695] Trial 3439 pruned. Pruned at step 9 with metric 0.6095

================================================================================
TRIAL 3445 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.6342083619974847e-05
  Dropout: 0.09280954248239165
================================================================================

[I 2025-11-02 03:06:14,428] Trial 3444 pruned. Pruned at step 13 with metric 0.6273

================================================================================
TRIAL 3446 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.43611827999732e-05
  Dropout: 0.06624993688214065
================================================================================

[I 2025-11-02 03:07:39,433] Trial 3445 pruned. Pruned at step 10 with metric 0.6038
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3447 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 7.25219301975965e-06
  Dropout: 0.10314694910579375
================================================================================

[I 2025-11-02 03:20:26,923] Trial 3446 finished with value: 0.4368131868131868 and parameters: {'seed': 29112, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 9.43611827999732e-05, 'optim.weight_decay': 0.0007681592872878829, 'optim.beta1': 0.9435816907521433, 'optim.beta2': 0.9923753673785913, 'optim.eps': 5.724753299851368e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0982338297052911, 'sched.poly_power': 0.8605250209698087, 'train.clip_grad': 1.1879192510651686, 'model.dropout': 0.06624993688214065, 'model.attn_dropout': 0.23722881776083288, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8601774867249857, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.3429687759258734, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.5313054435769615, 'loss.cls.alpha': 0.5494606257672824, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 03:20:27,455] Trial 3448 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3449 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.682743421929585e-06
  Dropout: 0.31725808373068304
================================================================================

[I 2025-11-02 03:29:28,170] Trial 3449 pruned. Pruned at step 16 with metric 0.6095
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3450 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 3.765256271624218e-05
  Dropout: 0.2872769277335221
================================================================================

[I 2025-11-02 03:38:23,999] Trial 3447 finished with value: 0.6893939393939394 and parameters: {'seed': 53347, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 7.25219301975965e-06, 'optim.weight_decay': 0.0023411197081229323, 'optim.beta1': 0.8518636948760752, 'optim.beta2': 0.9793622355382335, 'optim.eps': 4.712849348442532e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.06051376886063273, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.48129655460581205, 'model.dropout': 0.10314694910579375, 'model.attn_dropout': 0.2015841059173235, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8392818638002247, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.09252362929713848, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.12190972512332145, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-02 03:38:24,453] The parameter `tok.doc_stride` in Trial#3451 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3451 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4780482548832616e-05
  Dropout: 0.49927943328622365
================================================================================

[I 2025-11-02 03:41:26,867] Trial 3451 pruned. Pruned at step 9 with metric 0.6254
[W 2025-11-02 03:41:27,319] The parameter `tok.doc_stride` in Trial#3452 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 03:41:27,377] Trial 3452 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-02 03:41:27,832] The parameter `tok.doc_stride` in Trial#3453 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3453 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.610133511554645e-05
  Dropout: 0.33844493314480006
================================================================================

[I 2025-11-02 03:45:30,314] Trial 3453 pruned. Pruned at step 11 with metric 0.5997
[I 2025-11-02 03:45:30,820] Trial 3454 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-02 03:45:31,286] Trial 3455 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[W 2025-11-02 03:45:31,735] The parameter `tok.doc_stride` in Trial#3456 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 03:45:31,786] Trial 3456 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-02 03:45:32,283] Trial 3457 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-02 03:45:32,753] Trial 3458 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3459 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 8.859225976891175e-06
  Dropout: 0.24281310268031916
================================================================================

[I 2025-11-02 03:48:42,269] Trial 3459 pruned. Pruned at step 12 with metric 0.6285

================================================================================
TRIAL 3460 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4524354463182917e-05
  Dropout: 0.346700481216529
================================================================================

[I 2025-11-02 04:00:55,437] Trial 3460 finished with value: 0.6428030303030303 and parameters: {'seed': 54502, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.4524354463182917e-05, 'optim.weight_decay': 0.00019942749822556938, 'optim.beta1': 0.8598652771668291, 'optim.beta2': 0.9630354360723784, 'optim.eps': 8.438347209400705e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1647836540636383, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.5405615405133787, 'model.dropout': 0.346700481216529, 'model.attn_dropout': 0.03123853509212257, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8570073727536617, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.46132582143236023, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8854313453803373, 'loss.cls.alpha': 0.49315794371102123, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 31 (patience=20)

================================================================================
TRIAL 3461 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.0121055426495705e-05
  Dropout: 0.01524057888629933
================================================================================

[I 2025-11-02 04:01:51,634] Trial 3450 pruned. Pruned at step 27 with metric 0.6692
[I 2025-11-02 04:01:52,220] Trial 3462 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-02 04:01:52,685] Trial 3463 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3464 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.278726425120527e-05
  Dropout: 0.0856403333130935
================================================================================

[I 2025-11-02 04:16:03,812] Trial 3464 finished with value: 0.6722046722046722 and parameters: {'seed': 56952, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.278726425120527e-05, 'optim.weight_decay': 0.14085742332923312, 'optim.beta1': 0.9141957496118628, 'optim.beta2': 0.9839439107347415, 'optim.eps': 3.962207083991674e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.17952077594933233, 'sched.poly_power': 0.7628942610987298, 'train.clip_grad': 0.5434264414049683, 'model.dropout': 0.0856403333130935, 'model.attn_dropout': 0.10117687676421595, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8179556742756317, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.3796992047451586, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.0682555070359525, 'loss.cls.alpha': 0.787702261591626, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 3465 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 7.013111542711886e-05
  Dropout: 0.44887122077742736
================================================================================

[I 2025-11-02 04:16:11,263] Trial 3465 pruned. OOM: microsoft/deberta-v3-large bs=12 len=352
[I 2025-11-02 04:16:11,437] Trial 3461 pruned. OOM: roberta-large bs=8 len=192
[W 2025-11-02 04:16:12,255] The parameter `tok.doc_stride` in Trial#3467 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 3465 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 132.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 3461 exceeded GPU memory:
  Model: roberta-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3466 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 7.827123053833266e-05
  Dropout: 0.0364889115812837
================================================================================


================================================================================
TRIAL 3467 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 4.223333067524055e-05
  Dropout: 0.01285649993742842
================================================================================

[I 2025-11-02 04:29:42,622] Trial 3467 pruned. Pruned at step 12 with metric 0.6165
[I 2025-11-02 04:29:43,126] Trial 3468 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-02 04:29:43,560] The parameter `tok.doc_stride` in Trial#3469 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 04:29:43,613] Trial 3469 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3470 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.0346119475140523e-05
  Dropout: 0.23896789634236615
================================================================================

[I 2025-11-02 04:30:24,667] Trial 3466 finished with value: 0.42896935933147634 and parameters: {'seed': 49983, 'model.name': 'xlm-roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.827123053833266e-05, 'optim.weight_decay': 0.022976258654282062, 'optim.beta1': 0.8494138009604284, 'optim.beta2': 0.9780276980663727, 'optim.eps': 4.738845937618676e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1910787404706526, 'train.clip_grad': 0.7185184028941014, 'model.dropout': 0.0364889115812837, 'model.attn_dropout': 0.2261283215777703, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8999901152006666, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.3719220674216685, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.641160143985237, 'loss.cls.alpha': 0.43271565648722427, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-02 04:30:25,121] The parameter `tok.doc_stride` in Trial#3471 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3471 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.6978214215033426e-05
  Dropout: 0.3984078472775206
================================================================================

[I 2025-11-02 04:40:58,684] Trial 3470 pruned. Pruned at step 13 with metric 0.6535
[I 2025-11-02 04:40:59,181] Trial 3472 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3473 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 2.0743866919215245e-05
  Dropout: 0.3352881735582744
================================================================================

[I 2025-11-02 04:44:59,272] Trial 3471 pruned. Pruned at step 11 with metric 0.5847
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3474 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.166487671027991e-05
  Dropout: 0.025008304683515947
================================================================================

[I 2025-11-02 04:54:02,324] Trial 3474 pruned. Pruned at step 21 with metric 0.5283

================================================================================
TRIAL 3475 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.5982323716371505e-05
  Dropout: 0.058718532562238725
================================================================================

[I 2025-11-02 04:57:49,730] Trial 3475 pruned. Pruned at step 9 with metric 0.5439
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3476 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.65106492237782e-05
  Dropout: 0.17419905032969005
================================================================================

[I 2025-11-02 05:02:53,918] Trial 3476 pruned. Pruned at step 11 with metric 0.4444

================================================================================
TRIAL 3477 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.668049484854037e-05
  Dropout: 0.491310358925412
================================================================================

[I 2025-11-02 05:04:27,207] Trial 3473 pruned. Pruned at step 16 with metric 0.5656

================================================================================
TRIAL 3478 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.758373023272843e-05
  Dropout: 0.19433026252035188
================================================================================

[I 2025-11-02 05:08:52,241] Trial 3478 pruned. Pruned at step 7 with metric 0.6069

================================================================================
TRIAL 3479 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.2395703757469866e-05
  Dropout: 0.08248841348260606
================================================================================

[I 2025-11-02 05:14:17,677] Trial 3479 pruned. Pruned at step 9 with metric 0.5742

================================================================================
TRIAL 3480 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.557281996248717e-05
  Dropout: 0.35187576554945627
================================================================================

[I 2025-11-02 05:22:19,290] Trial 3477 pruned. Pruned at step 11 with metric 0.6079
[I 2025-11-02 05:22:20,045] Trial 3481 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-02 05:22:20,523] Trial 3482 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-02 05:22:21,025] Trial 3483 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3484 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.928434667701636e-05
  Dropout: 0.48545902182633915
================================================================================

[I 2025-11-02 05:26:20,374] Trial 3480 pruned. Pruned at step 9 with metric 0.6069
[I 2025-11-02 05:26:20,888] Trial 3485 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3486 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 5.7080239886510953e-05
  Dropout: 0.010551844096076984
================================================================================

[I 2025-11-02 05:30:01,760] Trial 3484 pruned. Pruned at step 9 with metric 0.5859
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3487 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.0294157615438425e-05
  Dropout: 0.4189262516713064
================================================================================

[I 2025-11-02 05:34:59,163] Trial 3486 finished with value: 0.43989071038251365 and parameters: {'seed': 33298, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.7080239886510953e-05, 'optim.weight_decay': 0.0006524524508375918, 'optim.beta1': 0.8718839548713735, 'optim.beta2': 0.99577858604339, 'optim.eps': 1.1434544349215082e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.08365408808352488, 'train.clip_grad': 1.4893359815913862, 'model.dropout': 0.010551844096076984, 'model.attn_dropout': 0.09528749889393297, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8000039415480464, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.09538434818852341, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.362892219260474, 'loss.cls.alpha': 0.6613891901493827, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 05:34:59,654] Trial 3488 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3489 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 4.0649148457480904e-05
  Dropout: 0.11025540451767277
================================================================================

[I 2025-11-02 05:38:26,838] Trial 3489 pruned. Pruned at step 12 with metric 0.5984
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3490 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.6302802696574788e-05
  Dropout: 0.32155372448231984
================================================================================

[I 2025-11-02 05:41:51,509] Trial 3490 pruned. Pruned at step 8 with metric 0.6107
[I 2025-11-02 05:41:51,996] Trial 3491 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3492 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.471852324822306e-05
  Dropout: 0.31835866056007983
================================================================================

[I 2025-11-02 05:52:07,258] Trial 3492 finished with value: 0.6961462450592886 and parameters: {'seed': 45584, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.471852324822306e-05, 'optim.weight_decay': 0.061629336445891254, 'optim.beta1': 0.9283240535864223, 'optim.beta2': 0.9869376831554972, 'optim.eps': 9.165219804484928e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.17937293967933227, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4820856571488423, 'model.dropout': 0.31835866056007983, 'model.attn_dropout': 0.2379211965235299, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8647437839777643, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.47615031311010436, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.631362774760721, 'loss.cls.alpha': 0.4413915938136089, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-02 05:52:07,710] The parameter `tok.doc_stride` in Trial#3493 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3493 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.463503548718738e-06
  Dropout: 0.29203777624125915
================================================================================

[I 2025-11-02 05:58:34,454] Trial 3493 finished with value: 0.6356302980585179 and parameters: {'seed': 54848, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.463503548718738e-06, 'optim.weight_decay': 0.012512564543449095, 'optim.beta1': 0.9445788591134606, 'optim.beta2': 0.9698668789954219, 'optim.eps': 1.599115556203139e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.1257209178967608, 'train.clip_grad': 1.1295122455283104, 'model.dropout': 0.29203777624125915, 'model.attn_dropout': 0.21048061593215828, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.874372972053509, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.4969835825878038, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.996039572506729, 'loss.cls.alpha': 0.39199879820390016, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 05:58:34,943] Trial 3494 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-02 05:58:35,414] Trial 3495 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3496 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 4.0092269105964856e-05
  Dropout: 0.0189477338506834
================================================================================

[I 2025-11-02 05:59:41,842] Trial 3487 pruned. Pruned at step 9 with metric 0.6139
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3497 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.842645973315512e-05
  Dropout: 0.22402360044327777
================================================================================

[I 2025-11-02 06:11:17,019] Trial 3496 finished with value: 0.43370165745856354 and parameters: {'seed': 49150, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 4.0092269105964856e-05, 'optim.weight_decay': 0.00011813379142266909, 'optim.beta1': 0.8506693481552506, 'optim.beta2': 0.9851950349307673, 'optim.eps': 1.0825280826757994e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13614422165141735, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.8570917156120709, 'model.dropout': 0.0189477338506834, 'model.attn_dropout': 0.18749039990821995, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8464408237124624, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.24619664067310018, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.029055338332844884, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3498 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.3668897355114957e-05
  Dropout: 0.0008929129671221905
================================================================================

[I 2025-11-02 06:27:13,769] Trial 3498 pruned. Pruned at step 15 with metric 0.6472
[W 2025-11-02 06:27:14,224] The parameter `tok.doc_stride` in Trial#3499 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3499 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.596446320266912e-05
  Dropout: 0.4726879023721726
================================================================================

[I 2025-11-02 06:40:01,247] Trial 3499 pruned. Pruned at step 9 with metric 0.4384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3500 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.79222320954914e-05
  Dropout: 0.3893623995952531
================================================================================

[I 2025-11-02 06:54:47,107] Trial 3500 pruned. Pruned at step 18 with metric 0.5124
[I 2025-11-02 06:54:47,603] Trial 3501 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3502 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 3.840370371326921e-05
  Dropout: 0.2689276921687267
================================================================================

[I 2025-11-02 06:54:55,470] Trial 3502 pruned. OOM: microsoft/deberta-v3-large bs=16 len=192
[I 2025-11-02 06:54:57,332] Trial 3497 pruned. OOM: roberta-large bs=8 len=192
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3502 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 48 with grad_accum=3)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 76.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3503 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.5417875006501768e-05
  Dropout: 0.4033479311442333
================================================================================


[OOM] Trial 3497 exceeded GPU memory:
  Model: roberta-large
  Batch size: 8 (effective: 24 with grad_accum=3)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 76.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3504 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 6.437469835247153e-05
  Dropout: 0.12004805706938085
================================================================================

[I 2025-11-02 07:01:48,244] Trial 3503 pruned. Pruned at step 16 with metric 0.5827

================================================================================
TRIAL 3505 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.3294518201952764e-05
  Dropout: 0.3850636412124517
================================================================================

[I 2025-11-02 07:08:32,684] Trial 3505 pruned. Pruned at step 9 with metric 0.5693
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3506 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 4.785237227993675e-05
  Dropout: 0.4225800761583381
================================================================================

[I 2025-11-02 07:22:29,440] Trial 3506 pruned. Pruned at step 31 with metric 0.6555
[I 2025-11-02 07:22:29,935] Trial 3507 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3508 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.5127410056831146e-05
  Dropout: 0.2696673649464987
================================================================================

[I 2025-11-02 07:29:02,704] Trial 3508 pruned. Pruned at step 7 with metric 0.5451
[I 2025-11-02 07:29:03,221] Trial 3509 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3510 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 4.1483148500789555e-05
  Dropout: 0.37144916279506957
================================================================================

[I 2025-11-02 07:42:25,555] Trial 3510 pruned. Pruned at step 16 with metric 0.6112
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3511 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 8.497054467332271e-06
  Dropout: 0.048923144049655885
================================================================================

[I 2025-11-02 07:45:45,101] Trial 3504 finished with value: 0.44594594594594594 and parameters: {'seed': 31264, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.437469835247153e-05, 'optim.weight_decay': 0.0016128700380552226, 'optim.beta1': 0.8978454405517406, 'optim.beta2': 0.9865965271377604, 'optim.eps': 4.0687431952350045e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0787046280676615, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2737420902011347, 'model.dropout': 0.12004805706938085, 'model.attn_dropout': 0.2999006713907517, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.9260098411576104, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.25081930397049046, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.582291519668802, 'loss.cls.alpha': 0.7420625247171192, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 07:45:45,603] Trial 3512 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3513 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.0904300567618379e-05
  Dropout: 0.18434902579070317
================================================================================

[I 2025-11-02 08:08:38,480] Trial 3511 finished with value: 0.7090322580645161 and parameters: {'seed': 44887, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.497054467332271e-06, 'optim.weight_decay': 0.1310974418271855, 'optim.beta1': 0.8999286110493901, 'optim.beta2': 0.9596494490253863, 'optim.eps': 7.211947131326399e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1161110268742517, 'sched.poly_power': 0.5006240589516635, 'train.clip_grad': 0.7845977074953974, 'model.dropout': 0.048923144049655885, 'model.attn_dropout': 0.26086342970147797, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8223979745597203, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.3403249229755906, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.1019894088486821, 'loss.cls.alpha': 0.3985508191485839, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 3514 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.346084677412493e-05
  Dropout: 0.29627787049117094
================================================================================

[I 2025-11-02 08:19:56,587] Trial 3514 pruned. Pruned at step 27 with metric 0.6462
[I 2025-11-02 08:19:57,278] Trial 3515 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-02 08:19:57,727] The parameter `tok.doc_stride` in Trial#3516 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 08:19:57,782] Trial 3516 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3517 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.2550830927263844e-05
  Dropout: 0.27105830509418727
================================================================================

[I 2025-11-02 08:26:50,787] Trial 3517 pruned. Pruned at step 13 with metric 0.5603
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3518 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 7.472490605933736e-05
  Dropout: 0.24547634880578006
================================================================================

[I 2025-11-02 08:27:38,056] Trial 3513 finished with value: 0.6476149548775247 and parameters: {'seed': 46149, 'model.name': 'bert-large-uncased', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.0904300567618379e-05, 'optim.weight_decay': 2.8786623109679676e-06, 'optim.beta1': 0.8401846718084955, 'optim.beta2': 0.9996954013620724, 'optim.eps': 8.251437678961647e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.051221915760460125, 'sched.poly_power': 0.7896549864394357, 'train.clip_grad': 0.947195163060096, 'model.dropout': 0.18434902579070317, 'model.attn_dropout': 0.21844408964755568, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8497011489700605, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.12256432370002798, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.993530827929057, 'loss.cls.alpha': 0.3246761126023299, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3519 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.7061633169264653e-05
  Dropout: 0.04040867949520174
================================================================================

[I 2025-11-02 08:57:22,578] Trial 3519 finished with value: 0.43989071038251365 and parameters: {'seed': 42960, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.7061633169264653e-05, 'optim.weight_decay': 0.00018614987829501347, 'optim.beta1': 0.9489202969803073, 'optim.beta2': 0.965634313992003, 'optim.eps': 3.8767988127029044e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.01004590299312804, 'sched.poly_power': 0.743434170966938, 'train.clip_grad': 1.1920619264481718, 'model.dropout': 0.04040867949520174, 'model.attn_dropout': 0.29425527567096216, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8129950245143993, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3295876105299426, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.463853497173204, 'loss.cls.alpha': 0.4749256173442717, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 08:57:23,079] Trial 3520 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[W 2025-11-02 08:57:23,517] The parameter `tok.doc_stride` in Trial#3521 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 08:57:23,568] Trial 3521 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-02 08:57:24,013] The parameter `tok.doc_stride` in Trial#3522 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3522 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.1319739419437594e-05
  Dropout: 0.021309819260960955
================================================================================

[I 2025-11-02 09:15:15,044] Trial 3518 finished with value: 0.45187165775401067 and parameters: {'seed': 49573, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.472490605933736e-05, 'optim.weight_decay': 0.05726821512633755, 'optim.beta1': 0.9410895915794971, 'optim.beta2': 0.9698078410099376, 'optim.eps': 2.8563528162062994e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.09399303961731079, 'sched.poly_power': 0.7142965196860992, 'train.clip_grad': 1.3883053479345893, 'model.dropout': 0.24547634880578006, 'model.attn_dropout': 0.19395425433021904, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8715590849970544, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.2540589871014917, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.046605210025265575, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 09:15:15,579] Trial 3523 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3524 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.2150269150285595e-05
  Dropout: 0.1185075210584853
================================================================================

[I 2025-11-02 09:19:05,486] Trial 3522 finished with value: 0.6776729559748428 and parameters: {'seed': 47623, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.1319739419437594e-05, 'optim.weight_decay': 0.06299868008123688, 'optim.beta1': 0.8712563411485863, 'optim.beta2': 0.9504012361964972, 'optim.eps': 5.428035268806792e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.13940355315752945, 'train.clip_grad': 0.6845322102409235, 'model.dropout': 0.021309819260960955, 'model.attn_dropout': 0.280156479672771, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8803316387894672, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.2623793280626412, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.08663282871020955, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 3525 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.8462407713660257e-05
  Dropout: 0.24092491693089502
================================================================================

[I 2025-11-02 09:21:56,989] Trial 3524 pruned. Pruned at step 11 with metric 0.5927
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3526 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.220898436666648e-05
  Dropout: 0.11775375398677448
================================================================================

[I 2025-11-02 09:25:26,482] Trial 3525 pruned. Pruned at step 9 with metric 0.6079
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3527 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.637707749702531e-05
  Dropout: 0.43620303276447314
================================================================================

[I 2025-11-02 09:28:38,570] Trial 3526 pruned. Pruned at step 27 with metric 0.5962

================================================================================
TRIAL 3528 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.118345021236505e-05
  Dropout: 0.48705638271892
================================================================================

[I 2025-11-02 09:32:11,349] Trial 3528 pruned. Pruned at step 11 with metric 0.5180
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3529 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.148914844693056e-06
  Dropout: 0.46270481298768973
================================================================================

[I 2025-11-02 09:37:27,312] Trial 3529 pruned. Pruned at step 18 with metric 0.5695
[W 2025-11-02 09:37:27,771] The parameter `tok.doc_stride` in Trial#3530 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 09:37:27,822] Trial 3530 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-02 09:37:28,294] Trial 3531 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3532 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.620276491472661e-05
  Dropout: 0.47892886143585994
================================================================================

[I 2025-11-02 09:37:52,685] Trial 3527 pruned. Pruned at step 9 with metric 0.5905
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3533 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.0989553304394519e-05
  Dropout: 0.3671719425773676
================================================================================

[I 2025-11-02 09:41:27,290] Trial 3532 pruned. Pruned at step 9 with metric 0.6002
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3534 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.0760454083281556e-05
  Dropout: 0.021872635492469913
================================================================================

[I 2025-11-02 09:41:56,784] Trial 3534 pruned. OOM: roberta-base bs=64 len=256
[I 2025-11-02 09:41:58,194] Trial 3533 pruned. OOM: roberta-large bs=12 len=384

[OOM] Trial 3534 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 64 with grad_accum=1)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 58.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3535 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0569821581852792e-05
  Dropout: 0.006562901789757834
================================================================================


[OOM] Trial 3533 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 58.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3536 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 6.156291648364215e-06
  Dropout: 0.4921897150680102
================================================================================

[I 2025-11-02 09:58:06,366] Trial 3536 finished with value: 0.7254175858808698 and parameters: {'seed': 39884, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.156291648364215e-06, 'optim.weight_decay': 0.0014713051559657026, 'optim.beta1': 0.9247162432693938, 'optim.beta2': 0.973038700525264, 'optim.eps': 1.1856316806475437e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.1026751509013318, 'train.clip_grad': 1.1211202144344443, 'model.dropout': 0.4921897150680102, 'model.attn_dropout': 0.026374746842436222, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.905093257864297, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.1138053950636837, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.12883488773071602, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 09:58:06,878] Trial 3537 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 55 (patience=20)

================================================================================
TRIAL 3538 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 9.587707210947349e-06
  Dropout: 0.11438988383754838
================================================================================

[I 2025-11-02 10:04:37,059] Trial 3535 finished with value: 0.7044121887916948 and parameters: {'seed': 46862, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.0569821581852792e-05, 'optim.weight_decay': 0.07946187585522466, 'optim.beta1': 0.8927791298256299, 'optim.beta2': 0.9922109742640121, 'optim.eps': 1.323126224865587e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.197620465719132, 'train.clip_grad': 0.8627843078020774, 'model.dropout': 0.006562901789757834, 'model.attn_dropout': 0.13985084006633589, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8519300979379851, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.42100135127708443, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.615696409319256, 'loss.cls.alpha': 0.4555530453875239, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 10:04:37,579] Trial 3539 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 3540 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 5.3879239146908535e-06
  Dropout: 0.3659123389141902
================================================================================

[I 2025-11-02 10:28:47,218] Trial 3540 pruned. Pruned at step 27 with metric 0.6297
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3541 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.899146371507136e-05
  Dropout: 0.4277811780317269
================================================================================

[I 2025-11-02 10:28:52,583] Trial 3541 pruned. OOM: roberta-base bs=64 len=192
[I 2025-11-02 10:28:53,866] Trial 3538 pruned. OOM: microsoft/deberta-v3-large bs=16 len=160
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3541 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 384 with grad_accum=6)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 77.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3542 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.5875383309668467e-05
  Dropout: 0.015574047998918858
================================================================================


[OOM] Trial 3538 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 48 with grad_accum=3)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 57.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3543 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.1700750132778265e-05
  Dropout: 0.17416106390702324
================================================================================

[I 2025-11-02 10:32:11,064] Trial 3542 pruned. Pruned at step 6 with metric 0.5404
[I 2025-11-02 10:32:11,591] Trial 3544 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3545 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.117793387181111e-05
  Dropout: 0.42284319230245226
================================================================================

[I 2025-11-02 10:40:58,064] Trial 3543 pruned. Pruned at step 12 with metric 0.5948
[I 2025-11-02 10:40:58,584] Trial 3546 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-02 10:40:59,070] Trial 3547 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3548 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.169296532914572e-06
  Dropout: 0.24387643152245314
================================================================================

[I 2025-11-02 10:41:32,740] Trial 3545 finished with value: 0.4305555555555556 and parameters: {'seed': 43590, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 7.117793387181111e-05, 'optim.weight_decay': 0.02240476294138689, 'optim.beta1': 0.9116499341843203, 'optim.beta2': 0.9613833627882556, 'optim.eps': 7.450449309490065e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.07692583914834802, 'sched.poly_power': 0.7579463589474446, 'train.clip_grad': 0.9191537590808883, 'model.dropout': 0.42284319230245226, 'model.attn_dropout': 0.265753008372758, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8895555111469131, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.26131358392241216, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.969697422822178, 'loss.cls.alpha': 0.44708435835140325, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3549 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 5.299390425276866e-05
  Dropout: 0.16176264832469817
================================================================================

[I 2025-11-02 11:03:43,993] Trial 3549 finished with value: 0.45478723404255317 and parameters: {'seed': 12190, 'model.name': 'bert-large-uncased', 'tok.max_length': 256, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.299390425276866e-05, 'optim.weight_decay': 0.00023121664084248698, 'optim.beta1': 0.9432149823239162, 'optim.beta2': 0.9884340252830685, 'optim.eps': 1.1152220485788083e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.07073972223624175, 'sched.poly_power': 0.5005635634717206, 'train.clip_grad': 1.3459741561970866, 'model.dropout': 0.16176264832469817, 'model.attn_dropout': 0.22505896809759673, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.9239054741751115, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.34074971800321796, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.0168813879561676, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3550 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 4.124023853429507e-05
  Dropout: 0.3954606800008632
================================================================================

[I 2025-11-02 11:14:27,910] Trial 3548 pruned. Pruned at step 9 with metric 0.6298

================================================================================
TRIAL 3551 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 8.592595140986333e-06
  Dropout: 0.09967840196961392
================================================================================

[I 2025-11-02 11:28:26,044] Trial 3550 finished with value: 0.43213296398891965 and parameters: {'seed': 51839, 'model.name': 'bert-large-uncased', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 4.124023853429507e-05, 'optim.weight_decay': 2.655691778792611e-05, 'optim.beta1': 0.8626943198027356, 'optim.beta2': 0.9793695235088912, 'optim.eps': 2.403637155665189e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.17038714008745148, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.0599463173123307, 'model.dropout': 0.3954606800008632, 'model.attn_dropout': 0.009025405610639789, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8841735826415729, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.49347285729419715, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.0492437761317618, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 11:28:26,547] Trial 3552 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3553 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 6.307242515241388e-05
  Dropout: 0.19904873221327707
================================================================================

[I 2025-11-02 11:37:20,443] Trial 3553 finished with value: 0.43213296398891965 and parameters: {'seed': 31434, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 6.307242515241388e-05, 'optim.weight_decay': 0.052909883527433275, 'optim.beta1': 0.9378747317711045, 'optim.beta2': 0.992339406196368, 'optim.eps': 1.882485807799863e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.11092062614191914, 'train.clip_grad': 1.2540630084810973, 'model.dropout': 0.19904873221327707, 'model.attn_dropout': 0.26542868914175544, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8871227887491527, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.29547533861084274, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.3394359177793955, 'loss.cls.alpha': 0.42332341614755525, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3554 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4917695273555524e-05
  Dropout: 0.38527449416853415
================================================================================

[I 2025-11-02 11:44:56,333] Trial 3554 pruned. Pruned at step 20 with metric 0.6159
[W 2025-11-02 11:44:56,806] The parameter `tok.doc_stride` in Trial#3555 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3555 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.94617990871868e-06
  Dropout: 0.330000061108988
================================================================================

[I 2025-11-02 11:47:05,270] Trial 3555 pruned. Pruned at step 9 with metric 0.6042
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3556 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.0663904585078987e-05
  Dropout: 0.031032703072899123
================================================================================

[I 2025-11-02 12:31:26,941] Trial 3551 finished with value: 0.6688408668353196 and parameters: {'seed': 49057, 'model.name': 'bert-large-uncased', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 8.592595140986333e-06, 'optim.weight_decay': 0.04549695527678608, 'optim.beta1': 0.9216928704153566, 'optim.beta2': 0.9740018872763884, 'optim.eps': 1.1132274205832162e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1975676808520854, 'sched.poly_power': 0.9223027758036819, 'train.clip_grad': 1.0458696923952369, 'model.dropout': 0.09967840196961392, 'model.attn_dropout': 0.21287017784673942, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8510770221680606, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.4575165040209895, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8111778984256066, 'loss.cls.alpha': 0.5016331287714157, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 3557 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.448083395461736e-06
  Dropout: 0.3960272319829434
================================================================================

[I 2025-11-02 12:32:02,675] Trial 3556 finished with value: 0.45187165775401067 and parameters: {'seed': 1758, 'model.name': 'roberta-large', 'tok.max_length': 352, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.0663904585078987e-05, 'optim.weight_decay': 0.08834507998877335, 'optim.beta1': 0.9366031527267266, 'optim.beta2': 0.97311582163217, 'optim.eps': 3.412245110508221e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.13553525352903384, 'sched.poly_power': 0.7680666919785089, 'train.clip_grad': 1.1697179837363172, 'model.dropout': 0.031032703072899123, 'model.attn_dropout': 0.21495189507970033, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8437215049632057, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.19450263168137133, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.997522508934909, 'loss.cls.alpha': 0.47897849812063925, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3558 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1234612823130577e-05
  Dropout: 0.34583959203429054
================================================================================

[I 2025-11-02 12:47:15,460] Trial 3558 finished with value: 0.7005550686532283 and parameters: {'seed': 13976, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.1234612823130577e-05, 'optim.weight_decay': 0.0013866546000317972, 'optim.beta1': 0.9294338850224969, 'optim.beta2': 0.9662360422290966, 'optim.eps': 1.2341793079039393e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.08224788211519557, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.246471802091376, 'model.dropout': 0.34583959203429054, 'model.attn_dropout': 0.23264260556045527, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.826417536343096, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.42356732725920654, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.3754914166026597, 'loss.cls.alpha': 0.5190874258576174, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 3559 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.3761170200923042e-05
  Dropout: 0.4445320084190859
================================================================================

[I 2025-11-02 12:54:37,727] Trial 3559 finished with value: 0.6654726368159204 and parameters: {'seed': 57032, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.3761170200923042e-05, 'optim.weight_decay': 0.035919622987013386, 'optim.beta1': 0.8571321855231948, 'optim.beta2': 0.9920755702341645, 'optim.eps': 3.471363784445782e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13400140534624783, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.3883923975310657, 'model.dropout': 0.4445320084190859, 'model.attn_dropout': 0.1455075836934521, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8083647273775094, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.2473629226292564, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8471023860371147, 'loss.cls.alpha': 0.40766689803081885, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3560 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.7389946506779944e-05
  Dropout: 0.0407013470100388
================================================================================

[I 2025-11-02 12:59:40,114] Trial 3560 pruned. Pruned at step 10 with metric 0.5375
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3561 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 4.36890155264901e-05
  Dropout: 0.39825918737873967
================================================================================

[I 2025-11-02 13:09:13,568] Trial 3557 finished with value: 0.6513605442176871 and parameters: {'seed': 57286, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.448083395461736e-06, 'optim.weight_decay': 0.004211836988493059, 'optim.beta1': 0.8964551829744207, 'optim.beta2': 0.9982550428382462, 'optim.eps': 6.055907306941071e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.0376691403437947, 'train.clip_grad': 0.5350207247685973, 'model.dropout': 0.3960272319829434, 'model.attn_dropout': 0.028441469349796175, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8905833941632962, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.10410366924039059, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.11499376656795235, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 13:09:14,068] Trial 3562 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 3563 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.3102914520943673e-05
  Dropout: 0.06581342430174877
================================================================================

[I 2025-11-02 13:22:16,723] Trial 3561 finished with value: 0.450402144772118 and parameters: {'seed': 44252, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 4.36890155264901e-05, 'optim.weight_decay': 0.007175763074136946, 'optim.beta1': 0.883163008456709, 'optim.beta2': 0.9798364029066311, 'optim.eps': 1.7868243631339603e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.1608468502726245, 'train.clip_grad': 1.1212392524778485, 'model.dropout': 0.39825918737873967, 'model.attn_dropout': 0.2174696993322013, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9181133058949389, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3890701085171536, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.177276117416005, 'loss.cls.alpha': 0.5473835770928092, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3564 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.1242003248544705e-05
  Dropout: 0.47186659238134987
================================================================================

[I 2025-11-02 13:22:33,680] Trial 3563 finished with value: 0.6315346667176565 and parameters: {'seed': 33837, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.3102914520943673e-05, 'optim.weight_decay': 7.03018550264189e-06, 'optim.beta1': 0.8786314538105429, 'optim.beta2': 0.9949467906672973, 'optim.eps': 2.4306581036739282e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.11310989514244704, 'train.clip_grad': 0.8715590787849588, 'model.dropout': 0.06581342430174877, 'model.attn_dropout': 0.25073758234992066, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8428027350425598, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.15208338579823022, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.6016383300081785, 'loss.cls.alpha': 0.6214394330480322, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 34 (patience=20)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3565 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.0598598169088568e-05
  Dropout: 0.2518994409858007
================================================================================

[I 2025-11-02 13:29:18,676] Trial 3564 pruned. Pruned at step 27 with metric 0.6402
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3566 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 3.697804023360822e-05
  Dropout: 0.12609008699987806
================================================================================

[I 2025-11-02 13:57:36,460] Trial 3565 pruned. Pruned at step 20 with metric 0.5962
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3567 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.5518258153649336e-05
  Dropout: 0.35707614123027653
================================================================================

[I 2025-11-02 14:02:34,873] Trial 3566 finished with value: 0.43989071038251365 and parameters: {'seed': 58025, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.697804023360822e-05, 'optim.weight_decay': 0.012790821104495208, 'optim.beta1': 0.8641137881479698, 'optim.beta2': 0.9729492127248336, 'optim.eps': 2.9094994433754463e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1586095432230753, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.0220199110098906, 'model.dropout': 0.12609008699987806, 'model.attn_dropout': 0.20682239871335176, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8357341704324244, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.21014503945921267, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.13259873051926632, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3568 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 3.0206837793057343e-05
  Dropout: 0.05709033875183992
================================================================================

[I 2025-11-02 14:16:17,937] Trial 3568 finished with value: 0.44141689373297005 and parameters: {'seed': 31445, 'model.name': 'xlm-roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.0206837793057343e-05, 'optim.weight_decay': 0.06742801890699023, 'optim.beta1': 0.9371941726433503, 'optim.beta2': 0.978508527063459, 'optim.eps': 1.0500896538002015e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.17951084360563094, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.374092079045758, 'model.dropout': 0.05709033875183992, 'model.attn_dropout': 0.18925295432347672, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8585990978848093, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.3345288070524179, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.09568157331738243, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3569 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.1464891476865658e-05
  Dropout: 0.10208489341883165
================================================================================

[I 2025-11-02 14:21:20,011] Trial 3567 finished with value: 0.4444444444444444 and parameters: {'seed': 27014, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.5518258153649336e-05, 'optim.weight_decay': 0.06951226673268222, 'optim.beta1': 0.9147841769063783, 'optim.beta2': 0.9840633435754208, 'optim.eps': 5.431683056913632e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.16554628260280843, 'train.clip_grad': 1.3830807629490158, 'model.dropout': 0.35707614123027653, 'model.attn_dropout': 0.20922977092309486, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.928032613549315, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.4520922045678794, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.17451511976264, 'loss.cls.alpha': 0.4593270893820802, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-02 14:21:20,479] The parameter `tok.doc_stride` in Trial#3570 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3570 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.5435855381539486e-05
  Dropout: 0.1970612932959676
================================================================================

[I 2025-11-02 14:27:26,927] Trial 3570 pruned. Pruned at step 9 with metric 0.6341
[I 2025-11-02 14:27:27,435] Trial 3571 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-02 14:27:27,880] The parameter `tok.doc_stride` in Trial#3572 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3572 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.0514513757792444e-05
  Dropout: 0.026787590372447077
================================================================================

[I 2025-11-02 14:35:40,084] Trial 3569 pruned. Pruned at step 10 with metric 0.5769
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3573 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 9.951847066568377e-06
  Dropout: 0.24243004206086094
================================================================================

[I 2025-11-02 14:37:54,318] Trial 3572 pruned. Pruned at step 9 with metric 0.6189
[W 2025-11-02 14:37:54,794] The parameter `tok.doc_stride` in Trial#3574 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3574 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.4364310379821592e-05
  Dropout: 0.31992316246829744
================================================================================

[I 2025-11-02 14:42:22,572] Trial 3573 pruned. Pruned at step 30 with metric 0.6082

================================================================================
TRIAL 3575 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.3575499377814026e-05
  Dropout: 0.3230427567921623
================================================================================

[I 2025-11-02 14:50:14,825] Trial 3575 pruned. Pruned at step 30 with metric 0.6273
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3576 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.3189978081290178e-05
  Dropout: 0.07394447838567675
================================================================================

[I 2025-11-02 14:50:21,168] Trial 3576 pruned. OOM: microsoft/deberta-v3-large bs=12 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3576 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 66.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3577 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.899387019525947e-05
  Dropout: 0.1752450525078114
================================================================================

[I 2025-11-02 14:55:55,495] Trial 3577 pruned. Pruned at step 10 with metric 0.5945
[W 2025-11-02 14:55:55,956] The parameter `tok.doc_stride` in Trial#3578 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 14:55:56,010] Trial 3578 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3579 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 8.104323284560342e-06
  Dropout: 0.3308474960730038
================================================================================

[I 2025-11-02 15:15:36,438] Trial 3579 finished with value: 0.6692183571616195 and parameters: {'seed': 48975, 'model.name': 'roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 8.104323284560342e-06, 'optim.weight_decay': 0.04271001043245389, 'optim.beta1': 0.8194486918132708, 'optim.beta2': 0.9919670207617155, 'optim.eps': 2.983465136936454e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.15180393390403824, 'sched.poly_power': 1.7316433794180288, 'train.clip_grad': 0.2585877059177247, 'model.dropout': 0.3308474960730038, 'model.attn_dropout': 0.1448310820933747, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8404700589123847, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.0949477035270561, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.4172213659074777, 'loss.cls.alpha': 0.12358029232880496, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-02 15:15:36,896] The parameter `tok.doc_stride` in Trial#3580 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 3580 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.4395022742729842e-05
  Dropout: 0.41362156397335637
================================================================================

[I 2025-11-02 15:21:06,549] Trial 3580 pruned. Pruned at step 9 with metric 0.5823
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3581 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.0776250218475733e-05
  Dropout: 0.09530498177535135
================================================================================

[I 2025-11-02 15:35:03,888] Trial 3581 pruned. Pruned at step 27 with metric 0.5405
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3582 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 4.073820186148267e-05
  Dropout: 0.11708527906963692
================================================================================

[I 2025-11-02 15:42:18,317] Trial 3574 pruned. Pruned at step 30 with metric 0.6069
[W 2025-11-02 15:42:18,793] The parameter `tok.doc_stride` in Trial#3583 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3583 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.434143471345708e-05
  Dropout: 0.05205071466272605
================================================================================

[I 2025-11-02 15:51:00,943] Trial 3583 pruned. Pruned at step 18 with metric 0.5884

================================================================================
TRIAL 3584 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.792346838496542e-05
  Dropout: 0.19135625914062254
================================================================================

[I 2025-11-02 15:53:26,904] Trial 3582 pruned. Pruned at step 12 with metric 0.5656
[I 2025-11-02 15:53:27,508] Trial 3585 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3586 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.646278739330829e-05
  Dropout: 0.24010175144320683
================================================================================

[I 2025-11-02 16:05:02,412] Trial 3586 pruned. Pruned at step 27 with metric 0.5929
[W 2025-11-02 16:05:02,876] The parameter `tok.doc_stride` in Trial#3587 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3587 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.325867306182025e-05
  Dropout: 0.36954457036375754
================================================================================

[I 2025-11-02 16:09:46,424] Trial 3587 finished with value: 0.45187165775401067 and parameters: {'seed': 51422, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.325867306182025e-05, 'optim.weight_decay': 0.00012442114705940225, 'optim.beta1': 0.8508939660431764, 'optim.beta2': 0.9829055208499516, 'optim.eps': 1.3839598876082537e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13986712092276793, 'sched.cosine_cycles': 4, 'train.clip_grad': 1.0952146046178775, 'model.dropout': 0.36954457036375754, 'model.attn_dropout': 0.05376113375418194, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9213876850023625, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.4812941822455195, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.992003233802162, 'loss.cls.alpha': 0.3160350744849921, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-02 16:09:46,887] The parameter `tok.doc_stride` in Trial#3588 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3588 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.5896548363145156e-05
  Dropout: 0.11998670022401417
================================================================================

[I 2025-11-02 16:12:49,901] Trial 3588 pruned. Pruned at step 6 with metric 0.5693
[W 2025-11-02 16:12:50,367] The parameter `tok.doc_stride` in Trial#3589 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 16:12:50,420] Trial 3589 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3590 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.1277417004313106e-05
  Dropout: 0.4208995745234
================================================================================

[I 2025-11-02 16:41:11,296] Trial 3584 pruned. Pruned at step 32 with metric 0.4459
[I 2025-11-02 16:41:11,836] Trial 3591 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-02 16:41:12,318] Trial 3592 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3593 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 6.510804490428334e-06
  Dropout: 0.11971325203588301
================================================================================

[I 2025-11-02 16:52:31,253] Trial 3590 finished with value: 0.44594594594594594 and parameters: {'seed': 8827, 'model.name': 'bert-large-uncased', 'tok.max_length': 352, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.1277417004313106e-05, 'optim.weight_decay': 0.14500950423833542, 'optim.beta1': 0.8749644892649286, 'optim.beta2': 0.9547990004290073, 'optim.eps': 6.05258561044889e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.06217774712392216, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.2012044620595106, 'model.dropout': 0.4208995745234, 'model.attn_dropout': 0.27639705543671444, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8140234672438811, 'head.pooling': 'mean', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.34272718890282683, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.644692910330465, 'loss.cls.alpha': 0.8301327674148129, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3594 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 4.226944903967496e-05
  Dropout: 0.08906017134077926
================================================================================

[I 2025-11-02 17:07:24,619] Trial 3593 pruned. Pruned at step 27 with metric 0.6562
[I 2025-11-02 17:07:25,418] Trial 3595 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3596 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.636572180114251e-05
  Dropout: 0.38679269801817157
================================================================================

[I 2025-11-02 17:07:30,835] Trial 3596 pruned. OOM: roberta-base bs=64 len=320
[W 2025-11-02 17:07:31,341] The parameter `tok.doc_stride` in Trial#3597 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 17:07:31,390] Trial 3597 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-02 17:07:31,878] Trial 3598 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-11-02 17:07:32,361] Trial 3599 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-02 17:07:32,813] The parameter `tok.doc_stride` in Trial#3600 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3596 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 60.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3600 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.990190803009788e-06
  Dropout: 0.12612535597360347
================================================================================

[I 2025-11-02 17:12:23,007] Trial 3594 finished with value: 0.44141689373297005 and parameters: {'seed': 19347, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 4.226944903967496e-05, 'optim.weight_decay': 0.09988124679365587, 'optim.beta1': 0.9491210360780015, 'optim.beta2': 0.9662065411044412, 'optim.eps': 2.1335937342087873e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.17216579404386542, 'sched.poly_power': 0.8785626769786345, 'train.clip_grad': 0.8516947667105982, 'model.dropout': 0.08906017134077926, 'model.attn_dropout': 0.19849024969934326, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.803146624380723, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.4870584438174284, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.954563359665563, 'loss.cls.alpha': 0.5638189834520007, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 17:12:23,523] Trial 3601 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3602 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.1501580898597285e-05
  Dropout: 0.2659032368153308
================================================================================

[I 2025-11-02 17:20:26,179] Trial 3602 pruned. Pruned at step 13 with metric 0.6125
[I 2025-11-02 17:20:26,689] Trial 3603 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3604 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.1017655732499535e-05
  Dropout: 0.20155691410545515
================================================================================

[I 2025-11-02 17:24:45,884] Trial 3604 finished with value: 0.6191613588110404 and parameters: {'seed': 51608, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.1017655732499535e-05, 'optim.weight_decay': 2.8428477971274487e-05, 'optim.beta1': 0.9146124400891095, 'optim.beta2': 0.9928512044022876, 'optim.eps': 1.547754794695901e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.03318317273037556, 'train.clip_grad': 0.7164988162410472, 'model.dropout': 0.20155691410545515, 'model.attn_dropout': 0.21143613775706369, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.882715519375683, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.10587966393397899, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.11228717533555888, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3605 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 3.0244747556700316e-05
  Dropout: 0.21298058618364513
================================================================================

[I 2025-11-02 17:25:10,258] Trial 3600 pruned. Pruned at step 9 with metric 0.6328
[I 2025-11-02 17:25:10,776] Trial 3606 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-02 17:25:11,262] Trial 3607 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3608 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.144404807416824e-05
  Dropout: 0.39851931651333844
================================================================================

[I 2025-11-02 17:27:08,418] Trial 3608 pruned. Pruned at step 9 with metric 0.6111

================================================================================
TRIAL 3609 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.7573293893417606e-05
  Dropout: 0.0031664461635079813
================================================================================

[I 2025-11-02 17:30:30,116] Trial 3609 pruned. Pruned at step 14 with metric 0.5742
[I 2025-11-02 17:30:30,614] Trial 3610 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-02 17:30:31,069] The parameter `tok.doc_stride` in Trial#3611 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 17:30:31,121] Trial 3611 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3612 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 0.00010986230517910906
  Dropout: 0.028259407887656414
================================================================================

[I 2025-11-02 17:43:48,795] Trial 3612 finished with value: 0.4562334217506631 and parameters: {'seed': 45166, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 0.00010986230517910906, 'optim.weight_decay': 0.014956734624859858, 'optim.beta1': 0.9434393238573987, 'optim.beta2': 0.9772442893614176, 'optim.eps': 1.5338856773511344e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06861241682450551, 'sched.poly_power': 0.7613876695195397, 'train.clip_grad': 1.3973400497660198, 'model.dropout': 0.028259407887656414, 'model.attn_dropout': 0.24655300219836102, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8982408730676481, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.26656680455022097, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.63874027308227, 'loss.cls.alpha': 0.5337235542535154, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 17:43:49,334] Trial 3613 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3614 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.424732648395152e-05
  Dropout: 0.09104546926377148
================================================================================

[I 2025-11-02 17:46:39,904] Trial 3614 pruned. Pruned at step 7 with metric 0.6273
[I 2025-11-02 17:46:40,410] Trial 3615 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-02 17:46:40,859] The parameter `tok.doc_stride` in Trial#3616 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3616 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 5.87250412428345e-06
  Dropout: 0.020133231417037666
================================================================================

[I 2025-11-02 17:47:33,182] Trial 3605 finished with value: 0.657129639514608 and parameters: {'seed': 5977, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.0244747556700316e-05, 'optim.weight_decay': 0.003402806460970225, 'optim.beta1': 0.9020808326666304, 'optim.beta2': 0.9680352738265029, 'optim.eps': 3.3646796034623264e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.12336746344269248, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1859746388244328, 'model.dropout': 0.21298058618364513, 'model.attn_dropout': 0.18456873362687806, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.872090617749597, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.21504543247750363, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.08180079762892398, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3617 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 0.00013651168655986668
  Dropout: 0.010914802878831634
================================================================================

[I 2025-11-02 17:47:38,846] Trial 3616 pruned. OOM: microsoft/deberta-v3-large bs=16 len=160

[OOM] Trial 3616 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 44.25 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3618 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 2.2496912892627322e-05
  Dropout: 0.25452481575967967
================================================================================

[I 2025-11-02 18:02:22,818] Trial 3618 pruned. Pruned at step 7 with metric 0.5656
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3619 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 1.0627507152308577e-05
  Dropout: 0.34751473617501727
================================================================================

[I 2025-11-02 18:02:31,252] Trial 3617 pruned. OOM: microsoft/deberta-v3-base bs=16 len=320
[I 2025-11-02 18:02:32,455] Trial 3619 pruned. OOM: microsoft/deberta-v3-large bs=16 len=160
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3617 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 16 (effective: 48 with grad_accum=3)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 88.25 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3620 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.1739926191751598e-05
  Dropout: 0.19275149677662246
================================================================================


[OOM] Trial 3619 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 88.25 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3621 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.201742754299662e-06
  Dropout: 0.28762294321998444
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-02 18:09:43,791] Trial 3620 pruned. Pruned at step 9 with metric 0.6069
[I 2025-11-02 18:09:44,315] Trial 3622 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3623 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 8.470000391571387e-06
  Dropout: 0.018032029341899798
================================================================================

[I 2025-11-02 18:18:03,859] Trial 3621 finished with value: 0.6722046722046722 and parameters: {'seed': 53429, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.201742754299662e-06, 'optim.weight_decay': 0.00017489568487600438, 'optim.beta1': 0.8483983936925065, 'optim.beta2': 0.9703018958698911, 'optim.eps': 1.5246715127780177e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.15359819725665855, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.5205871912003807, 'model.dropout': 0.28762294321998444, 'model.attn_dropout': 0.047561675726723596, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8272471056170663, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.43297146892452826, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.1760563743337045, 'loss.cls.alpha': 0.48662856814417016, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 18:18:04,379] Trial 3624 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-02 18:18:04,833] The parameter `tok.doc_stride` in Trial#3625 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 3625 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.682656543234711e-05
  Dropout: 0.4387892910015644
================================================================================

[I 2025-11-02 18:21:50,973] Trial 3625 pruned. Pruned at step 11 with metric 0.6125

================================================================================
TRIAL 3626 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.532649947795507e-06
  Dropout: 0.06947553811351179
================================================================================

[I 2025-11-02 18:22:20,469] Trial 3623 pruned. Pruned at step 15 with metric 0.6156
[W 2025-11-02 18:22:20,952] The parameter `tok.doc_stride` in Trial#3627 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3627 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 8.569008368771305e-06
  Dropout: 0.06313546810068668
================================================================================

[I 2025-11-02 18:25:22,170] Trial 3627 pruned. Pruned at step 10 with metric 0.6107

================================================================================
TRIAL 3628 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.070701423896067e-06
  Dropout: 0.3750431949152092
================================================================================

[I 2025-11-02 18:28:57,697] Trial 3626 pruned. Pruned at step 10 with metric 0.5905
[I 2025-11-02 18:28:58,210] Trial 3629 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-02 18:28:58,687] Trial 3630 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3631 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.3626979188207023e-05
  Dropout: 0.47352835378632124
================================================================================

[I 2025-11-02 18:48:58,752] Trial 3628 finished with value: 0.6041859746679024 and parameters: {'seed': 60815, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.070701423896067e-06, 'optim.weight_decay': 0.00016839318508986309, 'optim.beta1': 0.9231324844210748, 'optim.beta2': 0.9747483480373351, 'optim.eps': 6.542241429496698e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.11395042435394362, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.41174828473130987, 'model.dropout': 0.3750431949152092, 'model.attn_dropout': 0.12887124613585946, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9210190262200748, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.16730512845646323, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.13776303523114344, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 3632 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 4.224064598565736e-05
  Dropout: 0.3333628713598432
================================================================================

[I 2025-11-02 19:15:16,443] Trial 3631 finished with value: 0.4273743016759777 and parameters: {'seed': 33515, 'model.name': 'roberta-large', 'tok.max_length': 352, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.3626979188207023e-05, 'optim.weight_decay': 0.06753222944402729, 'optim.beta1': 0.8059729938860394, 'optim.beta2': 0.9955847066041209, 'optim.eps': 1.4611864931283339e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13357023283909142, 'sched.cosine_cycles': 4, 'train.clip_grad': 0.7525950541499891, 'model.dropout': 0.47352835378632124, 'model.attn_dropout': 0.15032083409420158, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8474133763044129, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.28572387287313766, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.569185709520454, 'loss.cls.alpha': 0.6645855399358264, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3633 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 8.055387210225087e-05
  Dropout: 0.2867700105161533
================================================================================

[I 2025-11-02 19:18:11,177] Trial 3632 pruned. Pruned at step 11 with metric 0.6199
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3634 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.5431676198269845e-05
  Dropout: 0.26216763978807744
================================================================================

[I 2025-11-02 19:18:20,209] Trial 3634 pruned. OOM: microsoft/deberta-v3-large bs=12 len=160
[W 2025-11-02 19:18:20,815] The parameter `tok.doc_stride` in Trial#3635 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-02 19:18:21,519] Trial 3633 pruned. OOM: microsoft/deberta-v3-large bs=8 len=160

[OOM] Trial 3634 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 44.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3635 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.7878333539362956e-05
  Dropout: 0.294346095585739
================================================================================


[OOM] Trial 3633 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 16 with grad_accum=2)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 42.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3636 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 8.7779670480954e-06
  Dropout: 0.42822560001480037
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-02 19:24:12,113] Trial 3635 pruned. Pruned at step 21 with metric 0.6380
[W 2025-11-02 19:24:12,602] The parameter `tok.doc_stride` in Trial#3637 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3637 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.9310963592245024e-05
  Dropout: 0.04631037717340351
================================================================================

[I 2025-11-02 19:25:48,235] Trial 3636 pruned. Pruned at step 9 with metric 0.5900
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3638 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 2.4076210402951932e-05
  Dropout: 0.38725127799960934
================================================================================

[I 2025-11-02 19:35:43,749] Trial 3637 pruned. Pruned at step 27 with metric 0.6174
[I 2025-11-02 19:35:44,281] Trial 3639 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3640 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 5.725614791770551e-06
  Dropout: 0.16434730530398617
================================================================================

[I 2025-11-02 19:41:29,446] Trial 3640 pruned. Pruned at step 12 with metric 0.6231

================================================================================
TRIAL 3641 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.8634321367221122e-05
  Dropout: 0.19123691825848282
================================================================================

[I 2025-11-02 19:42:26,187] Trial 3638 pruned. Pruned at step 27 with metric 0.5769
[I 2025-11-02 19:42:26,704] Trial 3642 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3643 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.0334614412482773e-05
  Dropout: 0.16140435287991914
================================================================================

[I 2025-11-02 19:47:09,389] Trial 3641 pruned. Pruned at step 8 with metric 0.6273
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3644 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.649345218748877e-05
  Dropout: 0.0003271084243892039
================================================================================

[I 2025-11-02 19:47:13,485] Trial 3644 pruned. OOM: roberta-base bs=64 len=224
[I 2025-11-02 19:47:14,799] Trial 3643 pruned. OOM: microsoft/deberta-v3-large bs=12 len=224
[I 2025-11-02 19:47:15,550] Trial 3646 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-02 19:47:16,152] Trial 3647 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)

[OOM] Trial 3644 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3645 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.651562091589615e-05
  Dropout: 0.049322929257446066
================================================================================


[OOM] Trial 3643 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

[I 2025-11-02 19:47:16,726] Trial 3648 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[W 2025-11-02 19:47:17,500] The parameter `tok.doc_stride` in Trial#3649 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3649 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.942477046567564e-05
  Dropout: 0.11699517695115921
================================================================================

[I 2025-11-02 19:50:56,972] Trial 3649 pruned. Pruned at step 7 with metric 0.5347
[I 2025-11-02 19:50:57,498] Trial 3650 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[W 2025-11-02 19:50:57,940] The parameter `tok.doc_stride` in Trial#3651 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3651 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.994834842148292e-05
  Dropout: 0.3221169601523908
================================================================================

[I 2025-11-02 20:09:48,031] Trial 3645 finished with value: 0.44141689373297005 and parameters: {'seed': 47467, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.651562091589615e-05, 'optim.weight_decay': 9.538411850870083e-05, 'optim.beta1': 0.852609579795228, 'optim.beta2': 0.9683047951412715, 'optim.eps': 2.2566478182152505e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.16775789238028274, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.5760092094284583, 'model.dropout': 0.049322929257446066, 'model.attn_dropout': 0.18810214054437732, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9387806961865351, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.3105667966867332, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.07189848062090909, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3652 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.9672303432065326e-05
  Dropout: 0.04396820177047975
================================================================================

[I 2025-11-02 20:30:41,336] Trial 3651 pruned. Pruned at step 32 with metric 0.5769
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3653 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 9.127732120674024e-06
  Dropout: 0.05710521981631784
================================================================================

[I 2025-11-02 21:00:49,211] Trial 3653 pruned. Pruned at step 7 with metric 0.6027
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3654 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.163099787928097e-05
  Dropout: 0.342355183816825
================================================================================

[I 2025-11-02 21:08:19,508] Trial 3652 pruned. Pruned at step 31 with metric 0.5014
[I 2025-11-02 21:08:20,223] Trial 3655 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3656 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 3.214863396053459e-05
  Dropout: 0.2535252138919546
================================================================================

[I 2025-11-02 21:20:29,940] Trial 3654 finished with value: 0.7521547521547521 and parameters: {'seed': 28558, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.163099787928097e-05, 'optim.weight_decay': 1.7543935995008115e-05, 'optim.beta1': 0.8696009791235331, 'optim.beta2': 0.9802176793322595, 'optim.eps': 9.462434214778224e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.19843113273319612, 'sched.poly_power': 0.8823580709657795, 'train.clip_grad': 0.9501529136910292, 'model.dropout': 0.342355183816825, 'model.attn_dropout': 0.09669614122158435, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9516479309666079, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.47921670484584755, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.9907473551882555, 'loss.cls.alpha': 0.4013693363923687, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 59 (patience=20)

================================================================================
TRIAL 3657 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.283372532783563e-05
  Dropout: 0.4265283085494836
================================================================================

[I 2025-11-02 21:26:14,821] Trial 3657 pruned. Pruned at step 11 with metric 0.5923
[I 2025-11-02 21:26:15,326] Trial 3658 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-11-02 21:26:15,814] Trial 3659 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3660 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.197160140170665e-05
  Dropout: 0.4076528756726721
================================================================================

[I 2025-11-02 21:35:04,282] Trial 3660 finished with value: 0.7124824684431977 and parameters: {'seed': 35303, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.197160140170665e-05, 'optim.weight_decay': 9.641355161231225e-06, 'optim.beta1': 0.9077561668191918, 'optim.beta2': 0.9852464858978017, 'optim.eps': 1.5618089936320963e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18937716240485605, 'sched.poly_power': 0.921528849494531, 'train.clip_grad': 0.8490112168454151, 'model.dropout': 0.4076528756726721, 'model.attn_dropout': 0.08236069188497412, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9257926589371184, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.28299677782038934, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.8844978307383125, 'loss.cls.alpha': 0.43916190440795505, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 3661 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.3936500227901174e-05
  Dropout: 0.4433106284009093
================================================================================

[I 2025-11-02 21:36:50,045] Trial 3656 finished with value: 0.4562334217506631 and parameters: {'seed': 8650, 'model.name': 'xlm-roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 3.214863396053459e-05, 'optim.weight_decay': 0.09357042565691502, 'optim.beta1': 0.9234355038341543, 'optim.beta2': 0.9741935691342777, 'optim.eps': 1.2082360834900418e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.12392853916725721, 'train.clip_grad': 1.4387705923589191, 'model.dropout': 0.2535252138919546, 'model.attn_dropout': 0.2866677090330094, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9335082546112289, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.4201185374554999, 'loss.cls.type': 'focal', 'loss.cls.gamma': 2.67824655625913, 'loss.cls.alpha': 0.4036006062756187, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 21:36:50,553] Trial 3662 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3663 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 8.59402017028416e-05
  Dropout: 0.2883336475784324
================================================================================

[I 2025-11-02 21:43:54,575] Trial 3663 finished with value: 0.4305555555555556 and parameters: {'seed': 59804, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 8.59402017028416e-05, 'optim.weight_decay': 0.11309912304056673, 'optim.beta1': 0.9342671466246084, 'optim.beta2': 0.9932680349547994, 'optim.eps': 1.876194928131263e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.15985748944746014, 'train.clip_grad': 1.4630837785924837, 'model.dropout': 0.2883336475784324, 'model.attn_dropout': 0.213287111807834, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9444906820735173, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4625797134931982, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.793229599176998, 'loss.cls.alpha': 0.28631792597445505, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 21:43:55,069] Trial 3664 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3665 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 1.528611436282615e-05
  Dropout: 0.49420404032727605
================================================================================

[I 2025-11-02 21:55:34,695] Trial 3665 pruned. Pruned at step 10 with metric 0.5341
[I 2025-11-02 21:55:35,202] Trial 3666 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3667 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 8.557190723116887e-06
  Dropout: 0.11829719387566998
================================================================================

[I 2025-11-02 22:06:30,451] Trial 3661 finished with value: 0.4209039548022599 and parameters: {'seed': 33238, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.3936500227901174e-05, 'optim.weight_decay': 0.046043053891702906, 'optim.beta1': 0.9346112764120933, 'optim.beta2': 0.9752557627213001, 'optim.eps': 7.776037484993776e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.061917885822511065, 'train.clip_grad': 1.4363507974136236, 'model.dropout': 0.4433106284009093, 'model.attn_dropout': 0.29994726801124016, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8213509009122156, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3148234675270642, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.875784435482793, 'loss.cls.alpha': 0.4428678008877613, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3668 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 3.9909544884033334e-05
  Dropout: 0.29337082603618936
================================================================================

[I 2025-11-02 22:32:39,998] Trial 3668 pruned. Pruned at step 15 with metric 0.4504
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3669 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 4.1400739240470016e-05
  Dropout: 0.183209176585201
================================================================================

[I 2025-11-02 22:36:49,955] Trial 3667 finished with value: 0.7031853281853282 and parameters: {'seed': 58462, 'model.name': 'roberta-large', 'tok.max_length': 288, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 8.557190723116887e-06, 'optim.weight_decay': 0.006727536368107771, 'optim.beta1': 0.8631015103760293, 'optim.beta2': 0.9644778925603306, 'optim.eps': 1.5772642111002089e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.16883726071783922, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.6679109093375825, 'model.dropout': 0.11829719387566998, 'model.attn_dropout': 0.2076591135250791, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8806712383316835, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.18529658675514427, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.906459101480377, 'loss.cls.alpha': 0.3349886646738401, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 33 (patience=20)

================================================================================
TRIAL 3670 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 4.72874119810337e-05
  Dropout: 0.08171387881858799
================================================================================

[I 2025-11-02 22:39:01,247] Trial 3670 pruned. Pruned at step 10 with metric 0.6014
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3671 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.7573289609514778e-05
  Dropout: 0.15654083274529634
================================================================================

[I 2025-11-02 22:51:43,259] Trial 3671 finished with value: 0.705701394585726 and parameters: {'seed': 21, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 32, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.7573289609514778e-05, 'optim.weight_decay': 3.409224034392868e-05, 'optim.beta1': 0.8660134787246313, 'optim.beta2': 0.9795276762054096, 'optim.eps': 2.8569279408651857e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18141171549881002, 'sched.poly_power': 0.8216047469480818, 'train.clip_grad': 1.002284887831934, 'model.dropout': 0.15654083274529634, 'model.attn_dropout': 0.11476005424851327, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9380809737665367, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4275269067893268, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.734092200359424, 'loss.cls.alpha': 0.3762791026617504, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 3672 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.305849862132215e-06
  Dropout: 0.42532745941073913
================================================================================

[I 2025-11-02 22:56:15,356] Trial 3672 finished with value: 0.6899159663865546 and parameters: {'seed': 39738, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.305849862132215e-06, 'optim.weight_decay': 2.0312596748055126e-06, 'optim.beta1': 0.8673927936292761, 'optim.beta2': 0.974302432332384, 'optim.eps': 4.5261429898405534e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1849703544446434, 'sched.poly_power': 0.7869628235698504, 'train.clip_grad': 1.3267915071022922, 'model.dropout': 0.42532745941073913, 'model.attn_dropout': 0.03772116267071427, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9322365849916302, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.3743607347180342, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.940813457083477, 'loss.cls.alpha': 0.3050962243932672, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-02 22:56:15,857] Trial 3673 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 3674 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.3350583135098828e-05
  Dropout: 0.05570985678478135
================================================================================

[I 2025-11-02 23:12:13,631] Trial 3674 finished with value: 0.6735668789808917 and parameters: {'seed': 51311, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.3350583135098828e-05, 'optim.weight_decay': 0.015594364617068318, 'optim.beta1': 0.8392969767891536, 'optim.beta2': 0.9693127327096925, 'optim.eps': 2.805209329095325e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.19880016662585087, 'sched.poly_power': 0.7908824365565926, 'train.clip_grad': 0.4539855462833427, 'model.dropout': 0.05570985678478135, 'model.attn_dropout': 0.23338772738551725, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.893177963617802, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.003171055290476628, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.984715135419356, 'loss.cls.alpha': 0.48715681666874827, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 3675 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.765570402347122e-05
  Dropout: 0.08237278803388602
================================================================================

[I 2025-11-02 23:21:19,113] Trial 3675 finished with value: 0.7092996660720003 and parameters: {'seed': 20012, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.765570402347122e-05, 'optim.weight_decay': 0.11331928540965149, 'optim.beta1': 0.8337388285316466, 'optim.beta2': 0.9766202405230184, 'optim.eps': 5.156088364267208e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.11939027045399733, 'train.clip_grad': 0.30928970050345517, 'model.dropout': 0.08237278803388602, 'model.attn_dropout': 0.21251507956000792, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8744375880788681, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.07359294028047104, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.902501267000131, 'loss.cls.alpha': 0.46083498892950586, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 3676 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.1918004541095234e-05
  Dropout: 0.2363585681854729
================================================================================

[I 2025-11-02 23:29:20,687] Trial 3676 pruned. Pruned at step 27 with metric 0.6492
[I 2025-11-02 23:29:21,217] Trial 3677 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-02 23:29:21,672] The parameter `tok.doc_stride` in Trial#3678 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3678 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.864167871691908e-05
  Dropout: 0.25728696067879525
================================================================================

[I 2025-11-02 23:31:30,453] Trial 3669 finished with value: 0.43526170798898073 and parameters: {'seed': 38938, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 4.1400739240470016e-05, 'optim.weight_decay': 0.00026250450204410154, 'optim.beta1': 0.8946889271837069, 'optim.beta2': 0.9761056934517726, 'optim.eps': 7.689777280484905e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.19124101699413912, 'sched.poly_power': 0.8342475192240696, 'train.clip_grad': 1.4457966316127842, 'model.dropout': 0.183209176585201, 'model.attn_dropout': 0.15457669815958822, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9264521044079115, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4225871349010102, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.986081393719095, 'loss.cls.alpha': 0.25920216926773554, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3679 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.013196446113232e-06
  Dropout: 0.11036975947581652
================================================================================

[I 2025-11-02 23:39:09,288] Trial 3679 pruned. Pruned at step 20 with metric 0.6343
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3680 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.3446938644292197e-05
  Dropout: 0.32186790737527365
================================================================================

[I 2025-11-02 23:39:59,218] Trial 3678 pruned. Pruned at step 29 with metric 0.5742
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3681 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 8.094735442347837e-06
  Dropout: 0.3693020608218847
================================================================================

[I 2025-11-02 23:53:43,050] Trial 3681 pruned. Pruned at step 9 with metric 0.5788
[I 2025-11-02 23:53:43,565] Trial 3682 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3683 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 8.50925407939896e-06
  Dropout: 0.2220533680700225
================================================================================

[I 2025-11-02 23:55:59,793] Trial 3683 pruned. Pruned at step 12 with metric 0.5807
[I 2025-11-02 23:56:00,308] Trial 3684 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3685 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.385211258609e-06
  Dropout: 0.42575599151749427
================================================================================

[I 2025-11-03 00:08:56,315] Trial 3685 finished with value: 0.699834479246244 and parameters: {'seed': 51272, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 6.385211258609e-06, 'optim.weight_decay': 2.3057104504156223e-05, 'optim.beta1': 0.8727340095054011, 'optim.beta2': 0.978877091567461, 'optim.eps': 8.257590350369114e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.167271781108583, 'sched.poly_power': 0.9253799231890341, 'train.clip_grad': 1.1009416770184552, 'model.dropout': 0.42575599151749427, 'model.attn_dropout': 0.10013897263372712, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9362503787663357, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.33846113789813675, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.11817950766728372, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-03 00:08:56,779] The parameter `tok.doc_stride` in Trial#3686 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 51 (patience=20)

================================================================================
TRIAL 3686 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.2528026058663174e-05
  Dropout: 0.3816222416667458
================================================================================

[I 2025-11-03 00:21:10,134] Trial 3680 finished with value: 0.6717198676203694 and parameters: {'seed': 5671, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.3446938644292197e-05, 'optim.weight_decay': 0.06442849918682003, 'optim.beta1': 0.9202819758111522, 'optim.beta2': 0.9644056220528556, 'optim.eps': 1.348813179296912e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.11130143144107751, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.4812981777825184, 'model.dropout': 0.32186790737527365, 'model.attn_dropout': 0.25582863228524116, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8151106099210275, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.4412838352551564, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.741377532917199, 'loss.cls.alpha': 0.35677775070387985, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 00:21:10,675] Trial 3687 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-11-03 00:21:11,172] Trial 3688 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-03 00:21:11,644] The parameter `tok.doc_stride` in Trial#3689 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 42 (patience=20)

================================================================================
TRIAL 3689 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.636984523452957e-05
  Dropout: 0.3662066478588454
================================================================================

[I 2025-11-03 00:21:27,652] Trial 3686 finished with value: 0.7136964500308551 and parameters: {'seed': 37371, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.2528026058663174e-05, 'optim.weight_decay': 1.43208592612867e-05, 'optim.beta1': 0.8651724705656659, 'optim.beta2': 0.9769947277317413, 'optim.eps': 2.0732631665085346e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1719088332934553, 'train.clip_grad': 1.2342694771916927, 'model.dropout': 0.3816222416667458, 'model.attn_dropout': 0.12581915301813973, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9398031149321934, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.39775235188323677, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.486512242132605, 'loss.cls.alpha': 0.6058082134563845, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 37 (patience=20)

================================================================================
TRIAL 3690 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 9.761813176724052e-06
  Dropout: 0.11832232563077466
================================================================================

[I 2025-11-03 00:30:07,894] Trial 3690 pruned. Pruned at step 18 with metric 0.6428
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3691 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.600335338415957e-05
  Dropout: 0.11429835182526364
================================================================================

[I 2025-11-03 00:34:30,770] Trial 3691 finished with value: 0.7009938604524948 and parameters: {'seed': 24809, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.600335338415957e-05, 'optim.weight_decay': 0.02109688192968861, 'optim.beta1': 0.8739275803882849, 'optim.beta2': 0.9752901190913095, 'optim.eps': 3.506999082921363e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.07335316753858247, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.7343025095067848, 'model.dropout': 0.11429835182526364, 'model.attn_dropout': 0.245986514983615, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8673566572629057, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.027346411368784313, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.037720707235577425, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-03 00:34:31,253] The parameter `tok.doc_stride` in Trial#3692 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 3692 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 4.452258254834126e-05
  Dropout: 0.43456249518802365
================================================================================

[I 2025-11-03 00:44:35,300] Trial 3692 finished with value: 0.43526170798898073 and parameters: {'seed': 46500, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 4.452258254834126e-05, 'optim.weight_decay': 6.651092279804891e-05, 'optim.beta1': 0.9023604899255081, 'optim.beta2': 0.9693282856150305, 'optim.eps': 3.773748262897667e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.06637424542362355, 'train.clip_grad': 0.7944357656454265, 'model.dropout': 0.43456249518802365, 'model.attn_dropout': 0.09584907314096452, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8928209932064076, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.08901416096944323, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.14100373277933415, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 00:44:35,825] Trial 3693 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3694 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 2.467232944452026e-05
  Dropout: 0.36156197418101427
================================================================================

[I 2025-11-03 00:51:43,523] Trial 3694 finished with value: 0.6123542388906398 and parameters: {'seed': 55872, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.467232944452026e-05, 'optim.weight_decay': 0.00011954133266446101, 'optim.beta1': 0.879835775362577, 'optim.beta2': 0.9867700842649948, 'optim.eps': 4.033226991976864e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.10177145054758835, 'sched.poly_power': 0.8868403387539955, 'train.clip_grad': 1.0258987917547144, 'model.dropout': 0.36156197418101427, 'model.attn_dropout': 0.10080034900386185, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9239814996316208, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4598025566145798, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.995529523065534, 'loss.cls.alpha': 0.5349615150571417, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 00:51:44,038] Trial 3695 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3696 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.1818291662429933e-05
  Dropout: 0.28933840678414163
================================================================================

[I 2025-11-03 00:55:06,502] Trial 3696 pruned. Pruned at step 9 with metric 0.6527
[I 2025-11-03 00:55:07,047] Trial 3697 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3698 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.5933116192658384e-05
  Dropout: 0.4213855692481803
================================================================================

[I 2025-11-03 00:58:47,599] Trial 3698 pruned. Pruned at step 9 with metric 0.5703
[I 2025-11-03 00:58:48,112] Trial 3699 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-03 00:58:48,641] Trial 3700 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3701 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.4188142297777655e-05
  Dropout: 0.00749717924166772
================================================================================

[I 2025-11-03 00:58:51,987] Trial 3689 pruned. Pruned at step 27 with metric 0.6640

================================================================================
TRIAL 3702 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.2308424689277293e-05
  Dropout: 0.36248773602217943
================================================================================

[I 2025-11-03 00:59:10,961] Trial 3702 pruned. OOM: bert-base-uncased bs=64 len=384
[I 2025-11-03 00:59:11,476] Trial 3703 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3702 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 64 with grad_accum=1)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 94.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3704 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.0487300919141353e-05
  Dropout: 0.42837749863312313
================================================================================

[I 2025-11-03 01:08:17,156] Trial 3701 pruned. Pruned at step 15 with metric 0.5878
[I 2025-11-03 01:08:17,914] Trial 3705 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3706 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.181600939524558e-05
  Dropout: 0.37337781902419204
================================================================================

[I 2025-11-03 01:17:48,442] Trial 3704 finished with value: 0.7054272134409196 and parameters: {'seed': 36039, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.0487300919141353e-05, 'optim.weight_decay': 0.08005169745547475, 'optim.beta1': 0.9364517609433122, 'optim.beta2': 0.9680862050821653, 'optim.eps': 4.240024073794907e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.09071471450225614, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.3227421499799732, 'model.dropout': 0.42837749863312313, 'model.attn_dropout': 0.2910731343594386, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8326061370173933, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.4155362540819877, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.10840834703655232, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 01:17:48,967] Trial 3707 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-03 01:17:49,455] Trial 3708 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-03 01:17:49,939] Trial 3709 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 01:17:50,424] Trial 3710 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-03 01:17:50,893] The parameter `tok.doc_stride` in Trial#3711 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 3711 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4561923685979424e-05
  Dropout: 0.08978529374856879
================================================================================

[I 2025-11-03 01:24:52,999] Trial 3711 pruned. Pruned at step 11 with metric 0.5788
[W 2025-11-03 01:24:53,486] The parameter `tok.doc_stride` in Trial#3712 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3712 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.1941428846356758e-05
  Dropout: 0.416048446238695
================================================================================

[I 2025-11-03 01:29:54,476] Trial 3712 pruned. Pruned at step 15 with metric 0.6602
[I 2025-11-03 01:29:54,984] Trial 3713 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3714 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.3386696009484236e-05
  Dropout: 0.014017364446814916
================================================================================

[I 2025-11-03 01:35:40,332] Trial 3706 pruned. Pruned at step 27 with metric 0.5917

================================================================================
TRIAL 3715 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.9868111800058776e-05
  Dropout: 0.41848006018333817
================================================================================

[I 2025-11-03 01:37:34,240] Trial 3714 pruned. Pruned at step 14 with metric 0.6215
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3716 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.1229534077061198e-05
  Dropout: 0.24673762608615157
================================================================================

[I 2025-11-03 01:49:32,531] Trial 3715 pruned. Pruned at step 27 with metric 0.5629
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3717 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.1669221686394498e-05
  Dropout: 0.13727229773219615
================================================================================

[I 2025-11-03 01:49:39,056] Trial 3717 pruned. OOM: microsoft/deberta-v3-large bs=12 len=384
[I 2025-11-03 01:49:39,689] Trial 3718 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)

[OOM] Trial 3717 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 110.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3719 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 6.512730915501094e-06
  Dropout: 0.42830724346384585
================================================================================

[I 2025-11-03 01:57:20,444] Trial 3719 pruned. Pruned at step 17 with metric 0.6407
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3720 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.7130561309164252e-05
  Dropout: 0.30437806382905647
================================================================================

[I 2025-11-03 02:01:52,787] Trial 3720 pruned. Pruned at step 17 with metric 0.5997

================================================================================
TRIAL 3721 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.821705669425172e-05
  Dropout: 0.1733379034931612
================================================================================

[I 2025-11-03 02:03:38,568] Trial 3716 pruned. Pruned at step 10 with metric 0.6128
[I 2025-11-03 02:03:39,088] Trial 3722 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[W 2025-11-03 02:03:39,544] The parameter `tok.doc_stride` in Trial#3723 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3723 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 7.983363902810637e-06
  Dropout: 0.327165978206549
================================================================================

[I 2025-11-03 02:07:42,173] Trial 3721 finished with value: 0.646364118895966 and parameters: {'seed': 27940, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.821705669425172e-05, 'optim.weight_decay': 0.017727706128936583, 'optim.beta1': 0.912399559569986, 'optim.beta2': 0.9667819226272089, 'optim.eps': 3.593633233406939e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.024302277153322914, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.3664723882309522, 'model.dropout': 0.1733379034931612, 'model.attn_dropout': 0.2378239854588251, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9130356585243444, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.29573932909291684, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.239922488547488, 'loss.cls.alpha': 0.8221190090142401, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 22 (patience=20)

================================================================================
TRIAL 3724 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 9.396017946422546e-06
  Dropout: 0.1698700409646749
================================================================================

[I 2025-11-03 02:20:57,931] Trial 3724 pruned. Pruned at step 12 with metric 0.5629
[I 2025-11-03 02:20:58,459] Trial 3725 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3726 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 9.332775236490072e-06
  Dropout: 0.3873250766792411
================================================================================

[I 2025-11-03 02:24:32,408] Trial 3726 pruned. Pruned at step 9 with metric 0.5900

================================================================================
TRIAL 3727 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1841859181229876e-05
  Dropout: 0.009402148852328075
================================================================================

[I 2025-11-03 02:30:12,380] Trial 3727 pruned. Pruned at step 11 with metric 0.6027
[I 2025-11-03 02:30:12,901] Trial 3728 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-03 02:30:13,411] Trial 3729 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3730 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 6.829192309259158e-06
  Dropout: 0.2522916144549003
================================================================================

[I 2025-11-03 02:34:02,013] Trial 3723 finished with value: 0.6272727272727272 and parameters: {'seed': 10252, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 7.983363902810637e-06, 'optim.weight_decay': 0.0026926721841226274, 'optim.beta1': 0.9101815396613727, 'optim.beta2': 0.9831875782243498, 'optim.eps': 4.944648486893342e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.023686906851005357, 'train.clip_grad': 0.45278827507460007, 'model.dropout': 0.327165978206549, 'model.attn_dropout': 0.05994107634648907, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9421387607261344, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.21165154932993044, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.147347295012259, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 44 (patience=20)

================================================================================
TRIAL 3731 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 6.110972588420159e-06
  Dropout: 0.14563374558992886
================================================================================

[I 2025-11-03 02:58:18,255] Trial 3730 pruned. Pruned at step 31 with metric 0.5742
[I 2025-11-03 02:58:18,795] Trial 3732 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-03 02:58:19,262] The parameter `tok.doc_stride` in Trial#3733 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 02:58:19,317] Trial 3733 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-03 02:58:19,773] The parameter `tok.doc_stride` in Trial#3734 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 02:58:19,825] Trial 3734 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 02:58:20,320] Trial 3735 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3736 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.8972172899141825e-06
  Dropout: 0.1215676667481506
================================================================================

[I 2025-11-03 03:01:09,557] Trial 3736 pruned. Pruned at step 10 with metric 0.5957
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3737 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.907048749742988e-06
  Dropout: 0.33571535622341897
================================================================================

[I 2025-11-03 03:05:20,183] Trial 3737 pruned. Pruned at step 10 with metric 0.6079
[W 2025-11-03 03:05:20,660] The parameter `tok.doc_stride` in Trial#3738 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3738 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.549469418910779e-06
  Dropout: 0.12343689137231646
================================================================================

[I 2025-11-03 03:08:54,843] Trial 3738 pruned. Pruned at step 13 with metric 0.5557
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3739 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.0108942894007367e-05
  Dropout: 0.4127804417400711
================================================================================

[I 2025-11-03 03:09:40,746] Trial 3731 pruned. Pruned at step 28 with metric 0.6512

================================================================================
TRIAL 3740 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.447761610533867e-06
  Dropout: 0.2182522497277774
================================================================================

[I 2025-11-03 03:13:45,134] Trial 3739 pruned. Pruned at step 9 with metric 0.5582
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3741 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.5437198817932042e-05
  Dropout: 0.37641895001013853
================================================================================

[I 2025-11-03 03:14:41,122] Trial 3740 pruned. Pruned at step 9 with metric 0.6027
[W 2025-11-03 03:14:41,613] The parameter `tok.doc_stride` in Trial#3742 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3742 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 4.084321586396231e-05
  Dropout: 0.052640372091616844
================================================================================

[I 2025-11-03 03:17:15,570] Trial 3741 pruned. Pruned at step 15 with metric 0.5593
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3743 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 0.00010477100964072771
  Dropout: 0.2329815585267579
================================================================================

[I 2025-11-03 03:23:15,840] Trial 3742 pruned. Pruned at step 9 with metric 0.5788

================================================================================
TRIAL 3744 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 6.437611329942385e-06
  Dropout: 0.14714816022835892
================================================================================

[I 2025-11-03 03:26:20,011] Trial 3743 finished with value: 0.4444444444444444 and parameters: {'seed': 32704, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 0.00010477100964072771, 'optim.weight_decay': 0.05598860447120358, 'optim.beta1': 0.8448289046048991, 'optim.beta2': 0.95979822029435, 'optim.eps': 8.452979575112677e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.17197380848749313, 'train.clip_grad': 1.4222602677644371, 'model.dropout': 0.2329815585267579, 'model.attn_dropout': 0.2889658913284036, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8486300939636205, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.32362165990369673, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.15381530363208587, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3745 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.0509358228510924e-05
  Dropout: 0.4483581362278909
================================================================================

[I 2025-11-03 03:47:21,818] Trial 3745 pruned. Pruned at step 27 with metric 0.6328
[I 2025-11-03 03:47:22,331] Trial 3746 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 03:47:22,824] Trial 3747 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3748 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 3.528461182621778e-05
  Dropout: 0.2981401826277491
================================================================================

[I 2025-11-03 03:52:56,291] Trial 3744 pruned. Pruned at step 11 with metric 0.5949
[W 2025-11-03 03:52:56,787] The parameter `tok.doc_stride` in Trial#3749 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3749 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 7.869866055650564e-06
  Dropout: 0.10326906479999635
================================================================================

[I 2025-11-03 04:24:19,677] Trial 3748 finished with value: 0.4383561643835616 and parameters: {'seed': 4818, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 3.528461182621778e-05, 'optim.weight_decay': 0.1914550664074412, 'optim.beta1': 0.893199527601376, 'optim.beta2': 0.98626667989608, 'optim.eps': 6.202316878677731e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.17597717440755126, 'train.clip_grad': 1.2232005139197895, 'model.dropout': 0.2981401826277491, 'model.attn_dropout': 0.2872453466688699, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8612682703483439, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4096307664793568, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5949407419843356, 'loss.cls.alpha': 0.3967859392965771, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3750 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.8454766623303463e-05
  Dropout: 0.3791308337746676
================================================================================

[I 2025-11-03 04:47:59,419] Trial 3750 finished with value: 0.4489247311827957 and parameters: {'seed': 58095, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.8454766623303463e-05, 'optim.weight_decay': 0.0001692113163295521, 'optim.beta1': 0.9103341646189483, 'optim.beta2': 0.9517515306983992, 'optim.eps': 3.2157122188311138e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.06515454122532108, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.3780198318723296, 'model.dropout': 0.3791308337746676, 'model.attn_dropout': 0.2100313464173238, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8769844194003404, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.49236825024900177, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.952320973927067, 'loss.cls.alpha': 0.393769805955462, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3751 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 3.215952115477406e-05
  Dropout: 0.24330521720077994
================================================================================

[I 2025-11-03 04:51:53,320] Trial 3751 pruned. Pruned at step 9 with metric 0.5923
[I 2025-11-03 04:51:53,832] Trial 3752 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3753 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.581502959998488e-05
  Dropout: 0.038038999455450445
================================================================================

[I 2025-11-03 04:54:27,966] Trial 3753 pruned. Pruned at step 7 with metric 0.6014
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3754 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 3.10990509258892e-05
  Dropout: 0.029474253353773856
================================================================================

[I 2025-11-03 05:15:49,168] Trial 3749 finished with value: 0.7356398069341586 and parameters: {'seed': 43859, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 7.869866055650564e-06, 'optim.weight_decay': 1.3790906145661491e-05, 'optim.beta1': 0.8661078193862322, 'optim.beta2': 0.9878204633385013, 'optim.eps': 5.490556809062293e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03769359361491925, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.0999795924967697, 'model.dropout': 0.10326906479999635, 'model.attn_dropout': 0.20765038224479504, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9286964849546941, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.15066127843383403, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.10674955487777284, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 3755 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 6.678208894027411e-06
  Dropout: 0.021009252859962066
================================================================================

[I 2025-11-03 05:33:15,938] Trial 3755 pruned. Pruned at step 9 with metric 0.6014
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3756 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.916468563285273e-06
  Dropout: 0.3317743041912576
================================================================================

[I 2025-11-03 05:44:52,604] Trial 3754 finished with value: 0.43989071038251365 and parameters: {'seed': 61088, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.10990509258892e-05, 'optim.weight_decay': 0.0033549759354538572, 'optim.beta1': 0.8197176797079999, 'optim.beta2': 0.9831240457833247, 'optim.eps': 1.0579026068168866e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.12936144664345767, 'train.clip_grad': 1.2569566371078422, 'model.dropout': 0.029474253353773856, 'model.attn_dropout': 0.1838929788965217, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8658220574359318, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.18900132583404708, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.12740177174090583, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3757 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 9.916731750846851e-06
  Dropout: 0.029764041952929532
================================================================================

[I 2025-11-03 05:49:14,783] Trial 3757 pruned. Pruned at step 18 with metric 0.5705
[I 2025-11-03 05:49:15,306] Trial 3758 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-03 05:49:15,807] Trial 3759 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3760 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 5.953865316453626e-06
  Dropout: 0.27660412493319486
================================================================================

[I 2025-11-03 06:09:56,728] Trial 3756 pruned. Pruned at step 10 with metric 0.6627
[I 2025-11-03 06:09:57,285] Trial 3761 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 06:09:57,776] Trial 3762 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3763 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.695173499644769e-06
  Dropout: 0.11555439829704293
================================================================================

[I 2025-11-03 06:24:35,636] Trial 3760 finished with value: 0.6725694444444444 and parameters: {'seed': 18790, 'model.name': 'roberta-large', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.953865316453626e-06, 'optim.weight_decay': 0.0003783633455274824, 'optim.beta1': 0.8435517275372707, 'optim.beta2': 0.97269317799783, 'optim.eps': 3.2168954879764634e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.18526593125283766, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.0798740040775452, 'model.dropout': 0.27660412493319486, 'model.attn_dropout': 0.08520733140999195, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8910941309008364, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.457678538367178, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.99231495702373, 'loss.cls.alpha': 0.22151258117491213, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 06:24:36,169] Trial 3764 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 3765 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.473038118914583e-05
  Dropout: 0.3841860430946342
================================================================================

[I 2025-11-03 06:58:25,228] Trial 3765 finished with value: 0.4368131868131868 and parameters: {'seed': 44666, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.473038118914583e-05, 'optim.weight_decay': 5.600522542598496e-06, 'optim.beta1': 0.8865452415681293, 'optim.beta2': 0.9748661611316848, 'optim.eps': 3.3189840307902673e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1753654159614143, 'sched.poly_power': 0.9663009369828035, 'train.clip_grad': 0.7390478822807318, 'model.dropout': 0.3841860430946342, 'model.attn_dropout': 0.1270249447199691, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9923118827904441, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.3650500072310525, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5457711111296275, 'loss.cls.alpha': 0.7195872604569172, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 06:58:25,745] Trial 3766 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3767 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.2622566191188916e-05
  Dropout: 0.06308643039191605
================================================================================

[I 2025-11-03 07:20:47,062] Trial 3767 pruned. Pruned at step 10 with metric 0.6231
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3768 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.4408759472584862e-05
  Dropout: 0.10954798764954776
================================================================================

[I 2025-11-03 07:46:15,830] Trial 3768 finished with value: 0.44141689373297005 and parameters: {'seed': 41958, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.4408759472584862e-05, 'optim.weight_decay': 0.015220131119892191, 'optim.beta1': 0.8843393208448216, 'optim.beta2': 0.9989265361660785, 'optim.eps': 2.4417097911640025e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.1256965856024854, 'train.clip_grad': 1.1681824608775393, 'model.dropout': 0.10954798764954776, 'model.attn_dropout': 0.1702376440713994, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9182989394579631, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.4964892226312484, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5705936983365065, 'loss.cls.alpha': 0.6125628148859543, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3769 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 9.067096297214004e-06
  Dropout: 0.10191710232234781
================================================================================

[I 2025-11-03 07:47:44,157] Trial 3763 finished with value: 0.7083451892199504 and parameters: {'seed': 42844, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.695173499644769e-06, 'optim.weight_decay': 1.1644992404909507e-05, 'optim.beta1': 0.824090488339777, 'optim.beta2': 0.9970439148427214, 'optim.eps': 1.2141585663924499e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.0453440265708843, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.417304554902335, 'model.dropout': 0.11555439829704293, 'model.attn_dropout': 0.20993486492013097, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8979753397617145, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.08748708067022862, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.08684085458224489, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-03 07:47:44,653] The parameter `tok.doc_stride` in Trial#3770 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 3770 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.910378044284277e-06
  Dropout: 0.19771667399210063
================================================================================

[I 2025-11-03 07:52:12,494] Trial 3770 pruned. Pruned at step 12 with metric 0.5347
[W 2025-11-03 07:52:12,976] The parameter `tok.doc_stride` in Trial#3771 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 07:52:13,030] Trial 3771 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3772 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.3075801828805744e-05
  Dropout: 0.09560466232615424
================================================================================

[I 2025-11-03 07:54:25,695] Trial 3769 pruned. Pruned at step 10 with metric 0.6359
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3773 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.030947272385869e-05
  Dropout: 0.3662332633560756
================================================================================

[I 2025-11-03 08:06:39,739] Trial 3772 finished with value: 0.7092656791679799 and parameters: {'seed': 55030, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 32, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.3075801828805744e-05, 'optim.weight_decay': 2.0822988126582007e-05, 'optim.beta1': 0.8505114481302664, 'optim.beta2': 0.9981304362010633, 'optim.eps': 1.604395918773325e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.05370458270511551, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.4223245535946822, 'model.dropout': 0.09560466232615424, 'model.attn_dropout': 0.12585735705142792, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9245201834577028, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.22807750681556826, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.05657876202515652, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 08:06:40,271] Trial 3774 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 08:06:40,773] Trial 3775 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 31 (patience=20)

================================================================================
TRIAL 3776 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.598471410404222e-05
  Dropout: 0.49673611756929714
================================================================================

[I 2025-11-03 08:33:47,031] Trial 3773 pruned. Pruned at step 27 with metric 0.5769

================================================================================
TRIAL 3777 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.415336141918835e-05
  Dropout: 0.2593713532529164
================================================================================

[I 2025-11-03 08:40:03,657] Trial 3777 pruned. Pruned at step 11 with metric 0.5521

================================================================================
TRIAL 3778 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.1175675992298439e-05
  Dropout: 0.049857885089144634
================================================================================

[I 2025-11-03 08:47:14,839] Trial 3778 pruned. Pruned at step 9 with metric 0.6469
[W 2025-11-03 08:47:15,513] The parameter `tok.doc_stride` in Trial#3779 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 08:47:15,564] Trial 3779 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3780 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.174689066584995e-05
  Dropout: 0.1514126638904542
================================================================================

[I 2025-11-03 08:51:10,215] Trial 3780 pruned. Pruned at step 12 with metric 0.5923
[W 2025-11-03 08:51:10,695] The parameter `tok.doc_stride` in Trial#3781 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3781 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 8.77945159221725e-06
  Dropout: 0.04478955002276677
================================================================================

[I 2025-11-03 09:01:13,113] Trial 3776 finished with value: 0.4241573033707865 and parameters: {'seed': 37815, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.598471410404222e-05, 'optim.weight_decay': 2.6096544831991597e-05, 'optim.beta1': 0.8626204231456059, 'optim.beta2': 0.997908309199406, 'optim.eps': 7.212867572684206e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.15945338234125733, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.4049168255263815, 'model.dropout': 0.49673611756929714, 'model.attn_dropout': 0.14417276028627035, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9023617035590434, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.3755623128971792, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.08485853244449505, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3782 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.0997808147419795e-05
  Dropout: 0.09809116683706404
================================================================================

[I 2025-11-03 09:05:34,393] Trial 3782 pruned. Pruned at step 15 with metric 0.5927
[I 2025-11-03 09:05:34,957] Trial 3783 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-03 09:05:35,469] The parameter `tok.doc_stride` in Trial#3784 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3784 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.9949635699860896e-05
  Dropout: 0.18536120154922398
================================================================================

[I 2025-11-03 09:11:31,657] Trial 3781 finished with value: 0.6607338017174083 and parameters: {'seed': 52744, 'model.name': 'xlm-roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.77945159221725e-06, 'optim.weight_decay': 0.0009182638071610242, 'optim.beta1': 0.823717601905612, 'optim.beta2': 0.9605649921445206, 'optim.eps': 1.2950292609766188e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.028259874103315784, 'train.clip_grad': 1.3391685996117915, 'model.dropout': 0.04478955002276677, 'model.attn_dropout': 0.16351834153496922, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8543722471706164, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.009933493311807193, 'loss.cls.type': 'focal', 'loss.cls.gamma': 2.916248237567739, 'loss.cls.alpha': 0.6607513073756367, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 42 (patience=20)

================================================================================
TRIAL 3785 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.56844382127469e-05
  Dropout: 0.04140161077800063
================================================================================

[I 2025-11-03 09:13:12,718] Trial 3785 pruned. Pruned at step 6 with metric 0.5208
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3786 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.245074945191743e-05
  Dropout: 0.47657968359618946
================================================================================

[I 2025-11-03 09:18:17,145] Trial 3786 pruned. Pruned at step 13 with metric 0.6042
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3787 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.79852943002133e-05
  Dropout: 0.42462546776864224
================================================================================

[I 2025-11-03 09:23:07,114] Trial 3787 pruned. Pruned at step 10 with metric 0.5743
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3788 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.301573512971306e-06
  Dropout: 0.3055017263558402
================================================================================

[I 2025-11-03 09:27:49,229] Trial 3784 finished with value: 0.6833487797343218 and parameters: {'seed': 50334, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.9949635699860896e-05, 'optim.weight_decay': 0.0010537250406830224, 'optim.beta1': 0.8689969629755596, 'optim.beta2': 0.9623694487804999, 'optim.eps': 1.6563528520772746e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.19050940417981904, 'train.clip_grad': 0.9408622331179132, 'model.dropout': 0.18536120154922398, 'model.attn_dropout': 0.2653174163883877, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8571972013014255, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.08165338357813554, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.05275723716061518, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 09:27:49,750] Trial 3789 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 3790 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.6968206621121065e-05
  Dropout: 0.016410011617971607
================================================================================

[I 2025-11-03 09:31:14,291] Trial 3790 pruned. Pruned at step 6 with metric 0.5667
[I 2025-11-03 09:31:14,809] Trial 3791 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-03 09:31:15,303] Trial 3792 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3793 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.7042938184364114e-05
  Dropout: 0.37739084684426677
================================================================================

[I 2025-11-03 09:47:15,559] Trial 3793 pruned. Pruned at step 9 with metric 0.5962
[I 2025-11-03 09:47:16,077] Trial 3794 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-03 09:47:16,540] The parameter `tok.doc_stride` in Trial#3795 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3795 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.42474733140117e-06
  Dropout: 0.12701300062714896
================================================================================

[I 2025-11-03 09:56:32,781] Trial 3788 pruned. Pruned at step 26 with metric 0.6411
[I 2025-11-03 09:56:33,342] Trial 3796 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-03 09:56:33,846] Trial 3797 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3798 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 9.841566945157462e-06
  Dropout: 0.17409766795813614
================================================================================

[I 2025-11-03 10:04:02,567] Trial 3795 pruned. Pruned at step 16 with metric 0.6231
[W 2025-11-03 10:04:03,066] The parameter `tok.doc_stride` in Trial#3799 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3799 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.619906487844208e-06
  Dropout: 0.006224039291923958
================================================================================

[I 2025-11-03 10:15:03,067] Trial 3798 pruned. Pruned at step 15 with metric 0.5780
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3800 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.7189307391709865e-05
  Dropout: 0.17594878799369887
================================================================================

[I 2025-11-03 10:16:47,310] Trial 3799 finished with value: 0.6678299887822511 and parameters: {'seed': 52510, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.619906487844208e-06, 'optim.weight_decay': 0.008484030726703435, 'optim.beta1': 0.8460837249484933, 'optim.beta2': 0.976941373778975, 'optim.eps': 9.464911828612168e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.09799029650917411, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.8927973298863188, 'model.dropout': 0.006224039291923958, 'model.attn_dropout': 0.05304047100581305, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9794784091188139, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.04524858153082074, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.5053902761102589, 'loss.cls.alpha': 0.2884946333271722, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 3801 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 7.011877402125453e-06
  Dropout: 0.2415823666204549
================================================================================

[I 2025-11-03 10:22:16,853] Trial 3800 pruned. Pruned at step 10 with metric 0.6299
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3802 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.809530373186854e-06
  Dropout: 0.11915512174568522
================================================================================

[I 2025-11-03 10:52:56,585] Trial 3801 pruned. Pruned at step 10 with metric 0.5807
[I 2025-11-03 10:52:57,124] Trial 3803 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3804 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.484443684749916e-05
  Dropout: 0.43548705881255534
================================================================================

[I 2025-11-03 10:54:40,125] Trial 3802 finished with value: 0.7009938604524948 and parameters: {'seed': 43584, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.809530373186854e-06, 'optim.weight_decay': 1.3262363942095312e-06, 'optim.beta1': 0.851342078766677, 'optim.beta2': 0.9744156407534496, 'optim.eps': 9.220711236826474e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.09438591734126148, 'train.clip_grad': 1.2050319571601729, 'model.dropout': 0.11915512174568522, 'model.attn_dropout': 0.26150932246596514, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9286769931651013, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.22599359797654456, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.10654115009270862, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 10:54:40,642] Trial 3805 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 3806 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.2525388753679577e-05
  Dropout: 0.36166408885237994
================================================================================

[I 2025-11-03 10:57:28,977] Trial 3804 pruned. Pruned at step 9 with metric 0.5946
[W 2025-11-03 10:57:29,469] The parameter `tok.doc_stride` in Trial#3807 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 10:57:29,522] Trial 3807 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-03 10:57:30,016] Trial 3808 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3809 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.439506688901204e-05
  Dropout: 0.22259457379304434
================================================================================

[I 2025-11-03 11:17:06,262] Trial 3806 pruned. Pruned at step 10 with metric 0.5917
[W 2025-11-03 11:17:06,763] The parameter `tok.doc_stride` in Trial#3810 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 11:17:06,815] Trial 3810 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3811 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 4.167194519930864e-05
  Dropout: 0.09116589930022513
================================================================================

[I 2025-11-03 11:19:49,601] Trial 3809 pruned. Pruned at step 16 with metric 0.5004
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3812 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.8309683346091505e-06
  Dropout: 0.18700490606889358
================================================================================

[I 2025-11-03 11:20:16,201] Trial 3811 pruned. Pruned at step 13 with metric 0.6384
[I 2025-11-03 11:20:16,731] Trial 3813 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3814 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 1.535652393774519e-05
  Dropout: 0.4263067306549869
================================================================================

[I 2025-11-03 11:20:22,817] Trial 3814 pruned. OOM: microsoft/deberta-v3-large bs=16 len=320
[I 2025-11-03 11:20:23,450] Trial 3815 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3814 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3816 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.7819352805334528e-05
  Dropout: 0.3543287877946594
================================================================================

[I 2025-11-03 11:53:50,089] Trial 3816 finished with value: 0.4273743016759777 and parameters: {'seed': 41606, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.7819352805334528e-05, 'optim.weight_decay': 1.167896639099293e-06, 'optim.beta1': 0.8578707196435417, 'optim.beta2': 0.9818652955502832, 'optim.eps': 8.148857428773822e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.17426121916888457, 'sched.poly_power': 0.7768278691203039, 'train.clip_grad': 0.93876968491865, 'model.dropout': 0.3543287877946594, 'model.attn_dropout': 0.055464762214582033, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9587459738576775, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.41681136264207963, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.993273345150383, 'loss.cls.alpha': 0.6449529866745721, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3817 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.037232884122743e-06
  Dropout: 0.44802900542382523
================================================================================

[I 2025-11-03 11:59:13,695] Trial 3817 pruned. Pruned at step 27 with metric 0.6223
[I 2025-11-03 11:59:14,224] Trial 3818 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3819 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.0743154884825107e-05
  Dropout: 0.07490811466253289
================================================================================

[I 2025-11-03 12:10:32,905] Trial 3819 finished with value: 0.700900323831892 and parameters: {'seed': 44195, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.0743154884825107e-05, 'optim.weight_decay': 6.275763562331534e-06, 'optim.beta1': 0.8492764906401411, 'optim.beta2': 0.9857555845033664, 'optim.eps': 4.542257040225479e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.12381994375198954, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.0337731446707634, 'model.dropout': 0.07490811466253289, 'model.attn_dropout': 0.2515579069188374, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8840796225150493, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.13921868698691495, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.0722216995045781, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 12:10:33,422] Trial 3820 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 3821 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 8.682493560829514e-06
  Dropout: 0.46396088627052634
================================================================================

[I 2025-11-03 12:13:25,696] Trial 3812 pruned. Pruned at step 14 with metric 0.6067
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3822 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 9.711577060197026e-06
  Dropout: 0.13086760121961527
================================================================================

[I 2025-11-03 12:17:50,251] Trial 3821 pruned. Pruned at step 27 with metric 0.5886
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3823 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.613072075088032e-06
  Dropout: 0.04544427941490095
================================================================================

[I 2025-11-03 12:41:27,738] Trial 3822 pruned. Pruned at step 7 with metric 0.6279
[I 2025-11-03 12:41:28,356] Trial 3824 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-03 12:41:28,814] The parameter `tok.doc_stride` in Trial#3825 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3825 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.775124035494177e-06
  Dropout: 0.06873022831975845
================================================================================

[I 2025-11-03 12:51:14,943] Trial 3825 pruned. Pruned at step 24 with metric 0.6341
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3826 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 4.380434186668623e-05
  Dropout: 0.3666149046472288
================================================================================

[I 2025-11-03 12:51:25,080] Trial 3826 pruned. OOM: microsoft/deberta-v3-large bs=16 len=160
[I 2025-11-03 12:51:25,327] Trial 3823 pruned. OOM: roberta-large bs=12 len=320
[I 2025-11-03 12:51:25,847] Trial 3827 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-03 12:51:26,078] The parameter `tok.doc_stride` in Trial#3828 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3823 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 46.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 3826 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 48 with grad_accum=3)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 46.19 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3828 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 3.639157904114939e-05
  Dropout: 0.4950332640149012
================================================================================


================================================================================
TRIAL 3829 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 1.1975566201679363e-05
  Dropout: 0.03899079303477716
================================================================================

[I 2025-11-03 13:00:57,673] Trial 3829 pruned. Pruned at step 7 with metric 0.5603
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3830 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.465465937483446e-06
  Dropout: 0.4226894257303704
================================================================================

[I 2025-11-03 13:03:59,899] Trial 3830 pruned. Pruned at step 10 with metric 0.5693

================================================================================
TRIAL 3831 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 6.354994165433145e-06
  Dropout: 0.39402857413326003
================================================================================

[I 2025-11-03 13:11:27,479] Trial 3831 pruned. Pruned at step 18 with metric 0.5957
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3832 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.1870973266135337e-05
  Dropout: 0.06501645175054648
================================================================================

[I 2025-11-03 13:15:44,866] Trial 3832 pruned. Pruned at step 10 with metric 0.6537
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3833 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 7.62616917658483e-06
  Dropout: 0.4763814612213152
================================================================================

[I 2025-11-03 13:17:24,533] Trial 3828 finished with value: 0.43370165745856354 and parameters: {'seed': 51644, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.639157904114939e-05, 'optim.weight_decay': 0.0014971496309441926, 'optim.beta1': 0.949953514508031, 'optim.beta2': 0.9633508772446621, 'optim.eps': 2.406512289806971e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.023998128741228135, 'sched.poly_power': 1.8787795013609017, 'train.clip_grad': 1.390216994656706, 'model.dropout': 0.4950332640149012, 'model.attn_dropout': 0.28197630437584453, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8530648496802786, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.4506917175406173, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.659581111088448, 'loss.cls.alpha': 0.8906138518364415, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3834 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.427667254583497e-05
  Dropout: 0.1688998999652375
================================================================================

[I 2025-11-03 13:19:16,080] Trial 3833 pruned. Pruned at step 8 with metric 0.6257

================================================================================
TRIAL 3835 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 4.023225731661832e-05
  Dropout: 0.2751595872771571
================================================================================

[I 2025-11-03 13:24:58,374] Trial 3835 pruned. Pruned at step 9 with metric 0.5416
[I 2025-11-03 13:24:58,897] Trial 3836 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-03 13:24:59,430] Trial 3837 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 13:24:59,953] Trial 3838 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3839 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.0404166415611007e-05
  Dropout: 0.16719903604873937
================================================================================

[I 2025-11-03 13:27:42,419] Trial 3839 pruned. Pruned at step 11 with metric 0.6210
[W 2025-11-03 13:27:42,905] The parameter `tok.doc_stride` in Trial#3840 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 13:27:42,961] Trial 3840 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-03 13:27:43,463] Trial 3841 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3842 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.8951643040334683e-05
  Dropout: 0.3006253553409006
================================================================================

[I 2025-11-03 13:30:54,476] Trial 3842 pruned. Pruned at step 9 with metric 0.6135
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3843 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.3027687190581895e-05
  Dropout: 0.3468816995278776
================================================================================

[I 2025-11-03 13:51:43,950] Trial 3843 pruned. Pruned at step 17 with metric 0.5905

================================================================================
TRIAL 3844 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0643645115970202e-05
  Dropout: 0.24641594578449427
================================================================================

[I 2025-11-03 13:53:54,311] Trial 3844 pruned. Pruned at step 7 with metric 0.5905
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3845 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 4.8733144486974566e-05
  Dropout: 0.03669839154658413
================================================================================

[I 2025-11-03 14:13:53,667] Trial 3845 finished with value: 0.4444444444444444 and parameters: {'seed': 50291, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 4.8733144486974566e-05, 'optim.weight_decay': 0.00029118237543088036, 'optim.beta1': 0.8363634832398275, 'optim.beta2': 0.9572813879727997, 'optim.eps': 3.046347992522148e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.14626638941842157, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.8249825236720352, 'model.dropout': 0.03669839154658413, 'model.attn_dropout': 0.15717688217724707, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.876269901200706, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.22824305826391367, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.09393793922476898, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3846 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 8.7596848341247e-06
  Dropout: 0.3148083687861908
================================================================================

[I 2025-11-03 14:35:16,339] Trial 3846 finished with value: 0.7699547511312217 and parameters: {'seed': 55099, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 8.7596848341247e-06, 'optim.weight_decay': 3.0388276259519623e-06, 'optim.beta1': 0.8631479113344981, 'optim.beta2': 0.9777471991003152, 'optim.eps': 1.627456494470541e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.1435308188810257, 'train.clip_grad': 1.0761432318042223, 'model.dropout': 0.3148083687861908, 'model.attn_dropout': 0.14389684595862645, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9418444971245853, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4803122254658507, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.683787926530646, 'loss.cls.alpha': 0.11294992376838406, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 51 (patience=20)

================================================================================
TRIAL 3847 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.3913080113133215e-05
  Dropout: 0.10054339639403387
================================================================================

[I 2025-11-03 14:39:24,153] Trial 3847 pruned. Pruned at step 10 with metric 0.6208

================================================================================
TRIAL 3848 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 5.377740046522643e-06
  Dropout: 0.35671335649878233
================================================================================

[I 2025-11-03 14:49:04,838] Trial 3848 pruned. Pruned at step 23 with metric 0.6437
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3849 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.736448638900947e-05
  Dropout: 0.02675844619138676
================================================================================

[I 2025-11-03 14:59:57,673] Trial 3849 pruned. Pruned at step 27 with metric 0.5923
[I 2025-11-03 14:59:58,216] Trial 3850 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-03 14:59:58,750] Trial 3851 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3852 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3046306010221223e-05
  Dropout: 0.1574740645115417
================================================================================

[I 2025-11-03 15:15:15,220] Trial 3852 finished with value: 0.6200185356811863 and parameters: {'seed': 46597, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.3046306010221223e-05, 'optim.weight_decay': 5.103130261719004e-06, 'optim.beta1': 0.8401609738807956, 'optim.beta2': 0.9753755861669787, 'optim.eps': 5.388161084006837e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18780585584208215, 'train.clip_grad': 0.5224004784186957, 'model.dropout': 0.1574740645115417, 'model.attn_dropout': 0.08987821513365261, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9170356025268678, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.4579695657000943, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.004350170477712453, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-03 15:15:15,718] The parameter `tok.doc_stride` in Trial#3853 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 58 (patience=20)

================================================================================
TRIAL 3853 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1109057633548809e-05
  Dropout: 0.44076052861891024
================================================================================

[I 2025-11-03 15:30:30,371] Trial 3853 finished with value: 0.7415966386554622 and parameters: {'seed': 50040, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.1109057633548809e-05, 'optim.weight_decay': 0.005142147202231891, 'optim.beta1': 0.8843599772612213, 'optim.beta2': 0.9573718112772218, 'optim.eps': 6.7310192422029645e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.01379675592200056, 'sched.poly_power': 0.7908845497047392, 'train.clip_grad': 1.414691265604649, 'model.dropout': 0.44076052861891024, 'model.attn_dropout': 0.2242596626866454, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9063729655663995, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3913829432315809, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.71883229018432, 'loss.cls.alpha': 0.1257721309855705, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 3854 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.360621683063669e-06
  Dropout: 0.36496430455554973
================================================================================

[I 2025-11-03 15:33:46,611] Trial 3834 finished with value: 0.6881946881946882 and parameters: {'seed': 49047, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.427667254583497e-05, 'optim.weight_decay': 0.007417084149443089, 'optim.beta1': 0.8443215737070573, 'optim.beta2': 0.9698136819669791, 'optim.eps': 4.0549114139676975e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.13286908031772815, 'train.clip_grad': 0.8361140938410861, 'model.dropout': 0.1688998999652375, 'model.attn_dropout': 0.2877556915496807, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8649264802417368, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.1973411334631381, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.08268622290598296, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 15:33:47,137] Trial 3855 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-03 15:33:47,613] The parameter `tok.doc_stride` in Trial#3856 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 52 (patience=20)

================================================================================
TRIAL 3856 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.952947595901493e-06
  Dropout: 0.36377700665793167
================================================================================

[I 2025-11-03 15:34:17,387] Trial 3854 pruned. Pruned at step 17 with metric 0.6027
[I 2025-11-03 15:34:17,959] Trial 3857 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3858 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 6.871346256296291e-06
  Dropout: 0.22097471006717942
================================================================================

[I 2025-11-03 15:41:39,782] Trial 3858 pruned. Pruned at step 17 with metric 0.6176
[I 2025-11-03 15:41:40,309] Trial 3859 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3860 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 5.595197645803753e-05
  Dropout: 0.42958393208535206
================================================================================

[I 2025-11-03 15:46:56,852] Trial 3856 finished with value: 0.7308464627741735 and parameters: {'seed': 50141, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.952947595901493e-06, 'optim.weight_decay': 0.01392521485047424, 'optim.beta1': 0.8712441209507877, 'optim.beta2': 0.9600451870889952, 'optim.eps': 2.5610365710406436e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.05020370928963817, 'sched.poly_power': 1.0010835504297877, 'train.clip_grad': 1.3880293530106376, 'model.dropout': 0.36377700665793167, 'model.attn_dropout': 0.2775669072013263, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9718458939857337, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.49689272821659447, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.609886316797317, 'loss.cls.alpha': 0.12946807657378429, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3861 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.754359407323506e-06
  Dropout: 0.37078464047607224
================================================================================

[I 2025-11-03 15:53:01,558] Trial 3861 pruned. Pruned at step 11 with metric 0.6359
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3862 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.822038956676922e-05
  Dropout: 0.3188358688672032
================================================================================

[I 2025-11-03 15:58:48,081] Trial 3862 pruned. Pruned at step 12 with metric 0.6148
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3863 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.5443228096656187e-05
  Dropout: 0.44312319102427045
================================================================================

[I 2025-11-03 16:02:58,091] Trial 3863 pruned. Pruned at step 7 with metric 0.5886
[W 2025-11-03 16:02:58,596] The parameter `tok.doc_stride` in Trial#3864 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3864 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 7.335176055537085e-06
  Dropout: 0.016986628272085094
================================================================================

[I 2025-11-03 16:03:13,281] Trial 3860 pruned. Pruned at step 14 with metric 0.6079
[I 2025-11-03 16:03:13,912] Trial 3865 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3866 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.5238962718269798e-05
  Dropout: 0.4574670020386117
================================================================================

[I 2025-11-03 16:07:01,004] Trial 3866 pruned. Pruned at step 17 with metric 0.6027

================================================================================
TRIAL 3867 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.3984525056232915e-05
  Dropout: 0.4333086266076439
================================================================================

[I 2025-11-03 16:13:26,612] Trial 3867 finished with value: 0.6852420520231214 and parameters: {'seed': 50944, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.3984525056232915e-05, 'optim.weight_decay': 0.001924183700065593, 'optim.beta1': 0.8398039847925576, 'optim.beta2': 0.9569778172341619, 'optim.eps': 1.4819340485971427e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.03836071167955098, 'sched.poly_power': 0.8454440848527071, 'train.clip_grad': 1.1956377680595176, 'model.dropout': 0.4333086266076439, 'model.attn_dropout': 0.16833019870668608, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8407732935348791, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3396096307408895, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.633386893760527, 'loss.cls.alpha': 0.17173153441324374, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3868 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 8.576459181490581e-06
  Dropout: 0.3071834481825145
================================================================================

[I 2025-11-03 16:26:50,967] Trial 3868 finished with value: 0.7199453551912569 and parameters: {'seed': 54671, 'model.name': 'roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 8.576459181490581e-06, 'optim.weight_decay': 1.2358366009207325e-06, 'optim.beta1': 0.8072793822460891, 'optim.beta2': 0.9813698012829054, 'optim.eps': 2.193503348482628e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.17019845517169052, 'train.clip_grad': 1.0660051055338347, 'model.dropout': 0.3071834481825145, 'model.attn_dropout': 0.1544847650116937, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9779985166436332, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.44655476165282343, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.858678309187593, 'loss.cls.alpha': 0.1069626976494929, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-03 16:26:51,451] The parameter `tok.doc_stride` in Trial#3869 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 3869 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.893118747356707e-06
  Dropout: 0.38519833527989034
================================================================================

[I 2025-11-03 16:30:10,450] Trial 3869 pruned. Pruned at step 20 with metric 0.6343
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3870 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.0640442640995463e-05
  Dropout: 0.3690113041716846
================================================================================

[I 2025-11-03 16:30:17,167] Trial 3870 pruned. OOM: roberta-base bs=64 len=384
[I 2025-11-03 16:30:17,704] Trial 3871 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-03 16:30:19,738] Trial 3864 pruned. OOM: roberta-large bs=8 len=128
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3870 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 30.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3872 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.584122115481039e-05
  Dropout: 0.2077497526865032
================================================================================


[OOM] Trial 3864 exceeded GPU memory:
  Model: roberta-large
  Batch size: 8 (effective: 64 with grad_accum=8)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 30.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3873 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.592655215492969e-06
  Dropout: 0.4752658609588785
================================================================================

[I 2025-11-03 16:30:29,835] Trial 3872 pruned. OOM: roberta-base bs=32 len=384
[I 2025-11-03 16:30:30,009] Trial 3873 pruned. OOM: roberta-base bs=64 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3873 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 30.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 3872 exceeded GPU memory:
  Model: roberta-base
  Batch size: 32 (effective: 64 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 30.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3875 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.5564349336165343e-05
  Dropout: 0.44491850764346624
================================================================================


================================================================================
TRIAL 3874 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.3297000271487556e-05
  Dropout: 0.30115863315513264
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-03 16:30:34,036] Trial 3875 pruned. OOM: roberta-base bs=64 len=352
[I 2025-11-03 16:30:34,664] Trial 3876 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-03 16:30:35,514] Trial 3874 pruned. OOM: roberta-base bs=64 len=384
[I 2025-11-03 16:30:36,446] Trial 3878 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[W 2025-11-03 16:30:38,397] The parameter `tok.doc_stride` in Trial#3879 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 3875 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 3874 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3877 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 6.213969643044865e-06
  Dropout: 0.4577536140369149
================================================================================


================================================================================
TRIAL 3879 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 5.858009511191091e-06
  Dropout: 0.3579165389606336
================================================================================

[I 2025-11-03 16:34:57,791] Trial 3879 pruned. Pruned at step 15 with metric 0.5847
[I 2025-11-03 16:34:58,341] Trial 3880 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-03 16:34:58,846] Trial 3881 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-03 16:34:59,366] Trial 3882 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 16:34:59,884] Trial 3883 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3884 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.057792593706103e-05
  Dropout: 0.3836231281121423
================================================================================

[I 2025-11-03 16:42:13,030] Trial 3884 pruned. Pruned at step 16 with metric 0.6514
[I 2025-11-03 16:42:13,553] Trial 3885 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3886 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.2701680963679807e-05
  Dropout: 0.34509949692176767
================================================================================

[I 2025-11-03 16:54:55,870] Trial 3886 finished with value: 0.6647697540554683 and parameters: {'seed': 53561, 'model.name': 'bert-base-uncased', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.2701680963679807e-05, 'optim.weight_decay': 2.548474062993227e-05, 'optim.beta1': 0.8616520233511658, 'optim.beta2': 0.9658218615761486, 'optim.eps': 1.0168574839856333e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.06608418654192849, 'train.clip_grad': 0.7972361398140305, 'model.dropout': 0.34509949692176767, 'model.attn_dropout': 0.15022327019555776, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9190954496806734, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.486330852045747, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.557132032025469, 'loss.cls.alpha': 0.2627296854786422, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 16:54:56,422] Trial 3887 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 16:54:56,926] Trial 3888 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 3889 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 4.2536945018521526e-05
  Dropout: 0.4563487788816323
================================================================================

[I 2025-11-03 16:55:45,971] Trial 3877 pruned. Pruned at step 10 with metric 0.6016
[I 2025-11-03 16:55:46,600] Trial 3890 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3891 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.3469377579544567e-05
  Dropout: 0.29082300236906033
================================================================================

[I 2025-11-03 16:59:26,699] Trial 3891 pruned. Pruned at step 13 with metric 0.6176

================================================================================
TRIAL 3892 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.820544740156858e-06
  Dropout: 0.28659565407150944
================================================================================

[I 2025-11-03 17:06:46,206] Trial 3889 pruned. Pruned at step 12 with metric 0.6232
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3893 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.131863958736976e-06
  Dropout: 0.3236995379931826
================================================================================

[I 2025-11-03 17:11:32,459] Trial 3892 pruned. Pruned at step 15 with metric 0.6231
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3894 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 2.1059956153346758e-05
  Dropout: 0.12859305847772828
================================================================================

[I 2025-11-03 17:21:36,919] Trial 3894 finished with value: 0.6450020264544416 and parameters: {'seed': 37459, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.1059956153346758e-05, 'optim.weight_decay': 0.03675977976921987, 'optim.beta1': 0.862594204133176, 'optim.beta2': 0.9686864102222889, 'optim.eps': 3.57126107515951e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1557756429403287, 'sched.poly_power': 0.8055498939346593, 'train.clip_grad': 0.8026611383846879, 'model.dropout': 0.12859305847772828, 'model.attn_dropout': 0.10500358595759751, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8981008583416092, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.09746932449030851, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.153977776713557, 'loss.cls.alpha': 0.163724848206731, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 17:21:37,440] Trial 3895 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 3896 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.100996893169267e-05
  Dropout: 0.31790244066681034
================================================================================

[I 2025-11-03 17:22:33,819] Trial 3893 finished with value: 0.7231645861467013 and parameters: {'seed': 53437, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 6.131863958736976e-06, 'optim.weight_decay': 8.900773624209933e-06, 'optim.beta1': 0.8739325764210394, 'optim.beta2': 0.9752048668105484, 'optim.eps': 3.1299840303656326e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.17405437052097786, 'train.clip_grad': 0.5274620630199122, 'model.dropout': 0.3236995379931826, 'model.attn_dropout': 0.07025978904446485, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9298241083652482, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.46651710792527934, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.46220219279705, 'loss.cls.alpha': 0.12185566530496508, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 43 (patience=20)

================================================================================
TRIAL 3897 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.7813023299073964e-05
  Dropout: 0.40401355227919145
================================================================================

[I 2025-11-03 17:25:50,060] Trial 3897 pruned. Pruned at step 10 with metric 0.6079
[W 2025-11-03 17:25:50,578] The parameter `tok.doc_stride` in Trial#3898 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3898 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 0.00012826242355152504
  Dropout: 0.39376502181649425
================================================================================

[I 2025-11-03 17:31:15,304] Trial 3898 finished with value: 0.4429347826086957 and parameters: {'seed': 45480, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 0.00012826242355152504, 'optim.weight_decay': 0.000421992645790466, 'optim.beta1': 0.8963493811336517, 'optim.beta2': 0.9829903544384597, 'optim.eps': 3.201009090825156e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13442752586568749, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.2805296915011632, 'model.dropout': 0.39376502181649425, 'model.attn_dropout': 0.058230513667634695, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9337170745779926, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.47988763606725887, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.970485427083456, 'loss.cls.alpha': 0.13277283154824174, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-03 17:31:15,969] The parameter `tok.doc_stride` in Trial#3899 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 17:31:16,025] Trial 3899 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-03 17:31:16,709] The parameter `tok.doc_stride` in Trial#3900 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3900 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.313163124051883e-06
  Dropout: 0.41463136173125825
================================================================================

[I 2025-11-03 17:32:41,325] Trial 3896 pruned. Pruned at step 9 with metric 0.6148
[W 2025-11-03 17:32:42,065] The parameter `tok.doc_stride` in Trial#3901 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3901 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.4883471753147434e-05
  Dropout: 0.41660048268733674
================================================================================

[I 2025-11-03 17:34:46,572] Trial 3901 pruned. Pruned at step 9 with metric 0.5766
[I 2025-11-03 17:34:47,253] Trial 3902 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-03 17:34:47,917] Trial 3903 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-03 17:34:48,580] Trial 3904 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3905 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 8.401647228752941e-06
  Dropout: 0.38191794415534386
================================================================================

[I 2025-11-03 17:50:06,085] Trial 3905 pruned. Pruned at step 27 with metric 0.6527

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3906 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.8997419112229174e-05
  Dropout: 0.15758821576326754
================================================================================

[I 2025-11-03 17:52:50,526] Trial 3906 pruned. Pruned at step 7 with metric 0.6164
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3907 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.0843095446060299e-05
  Dropout: 0.30167120461073904
================================================================================

[I 2025-11-03 18:06:13,524] Trial 3900 pruned. Pruned at step 27 with metric 0.6279
[I 2025-11-03 18:06:14,250] Trial 3908 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 100 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 3909 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.6356436117392095e-05
  Dropout: 0.40407313578146836
================================================================================

[I 2025-11-03 18:19:11,656] Trial 3909 finished with value: 0.779285099052541 and parameters: {'seed': 49110, 'model.name': 'roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.6356436117392095e-05, 'optim.weight_decay': 0.0006453281911887325, 'optim.beta1': 0.8541021426635973, 'optim.beta2': 0.9582667239830491, 'optim.eps': 6.418932610445782e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.009759282360161032, 'sched.poly_power': 0.7770811329481258, 'train.clip_grad': 1.2475367791526484, 'model.dropout': 0.40407313578146836, 'model.attn_dropout': 0.17579780105000262, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9791784646946095, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3194484331383342, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.483840080649495, 'loss.cls.alpha': 0.1935060127877405, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 3910 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.240463801592586e-06
  Dropout: 0.30736626945654144
================================================================================

[I 2025-11-03 18:25:30,802] Trial 3910 pruned. Pruned at step 12 with metric 0.6294
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3911 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.0558907357157028e-05
  Dropout: 0.45113541316735245
================================================================================

[I 2025-11-03 18:46:47,009] Trial 3907 finished with value: 0.7184065934065934 and parameters: {'seed': 46169, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.0843095446060299e-05, 'optim.weight_decay': 0.026944337023047927, 'optim.beta1': 0.8590316565389604, 'optim.beta2': 0.9717654570943394, 'optim.eps': 3.308654784421261e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18695773280706582, 'sched.poly_power': 0.7900011478571374, 'train.clip_grad': 0.4731356381303942, 'model.dropout': 0.30167120461073904, 'model.attn_dropout': 0.22006750356073523, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9015784614060409, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.18646597326803266, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.347280427037923, 'loss.cls.alpha': 0.22489435474162148, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 49 (patience=20)

================================================================================
TRIAL 3912 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 7.22089444171462e-06
  Dropout: 0.3634697654341245
================================================================================

[I 2025-11-03 18:54:05,693] Trial 3912 pruned. Pruned at step 19 with metric 0.6315
[I 2025-11-03 18:54:06,224] Trial 3913 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3914 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.5970007944320597e-05
  Dropout: 0.3674890455736966
================================================================================

[I 2025-11-03 18:54:11,792] Trial 3914 pruned. OOM: roberta-base bs=64 len=384
[I 2025-11-03 18:54:12,339] Trial 3915 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-03 18:54:12,804] The parameter `tok.doc_stride` in Trial#3916 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 3914 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 96.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3916 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.623506667002749e-05
  Dropout: 0.34220130680601735
================================================================================

[I 2025-11-03 19:02:08,562] Trial 3916 pruned. Pruned at step 11 with metric 0.6428

================================================================================
TRIAL 3917 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.1955086485231774e-05
  Dropout: 0.3498370578438293
================================================================================

[I 2025-11-03 19:05:04,525] Trial 3911 pruned. Pruned at step 27 with metric 0.6093
[I 2025-11-03 19:05:05,070] Trial 3918 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3919 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.3311811185624288e-05
  Dropout: 0.37758126815609994
================================================================================

[I 2025-11-03 19:19:07,585] Trial 3917 pruned. Pruned at step 33 with metric 0.6328
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3920 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 4.0872172845899246e-05
  Dropout: 0.017762175215593697
================================================================================

[I 2025-11-03 19:19:15,569] Trial 3920 pruned. OOM: microsoft/deberta-v3-large bs=16 len=256
[I 2025-11-03 19:19:16,229] Trial 3921 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)

[OOM] Trial 3920 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 56.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3922 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.475072208997099e-05
  Dropout: 0.39575641413072554
================================================================================

[I 2025-11-03 19:38:39,581] Trial 3922 finished with value: 0.7234672120447325 and parameters: {'seed': 12420, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.475072208997099e-05, 'optim.weight_decay': 3.8240076293043436e-05, 'optim.beta1': 0.8912738376427842, 'optim.beta2': 0.9614619021975029, 'optim.eps': 1.5255148201658027e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.02941852534870764, 'sched.poly_power': 0.8895068206405352, 'train.clip_grad': 1.4591983925982712, 'model.dropout': 0.39575641413072554, 'model.attn_dropout': 0.2407332443364328, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9644362614323767, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3706280426199875, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.296088161389114, 'loss.cls.alpha': 0.1204610790449603, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 19:38:40,100] Trial 3923 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 3924 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.2534290792706237e-05
  Dropout: 0.34912803286303024
================================================================================

[I 2025-11-03 19:41:29,812] Trial 3919 finished with value: 0.6453648112343543 and parameters: {'seed': 5895, 'model.name': 'bert-base-uncased', 'tok.max_length': 352, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 2.3311811185624288e-05, 'optim.weight_decay': 0.004327386410571074, 'optim.beta1': 0.8388885100933436, 'optim.beta2': 0.9581494549415314, 'optim.eps': 1.4542422963478979e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.009523101327070557, 'sched.poly_power': 1.0049718422420681, 'train.clip_grad': 1.2763383527053758, 'model.dropout': 0.37758126815609994, 'model.attn_dropout': 0.11345974815682827, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9699319835634124, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.43702776439114, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.605263722583976, 'loss.cls.alpha': 0.14640593776909389, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 19:41:30,348] Trial 3925 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3926 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.184773480750178e-06
  Dropout: 0.28637226279034034
================================================================================

[I 2025-11-03 19:43:07,455] Trial 3924 pruned. Pruned at step 9 with metric 0.5770
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3927 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.242658676508766e-05
  Dropout: 0.38084674343734426
================================================================================

[I 2025-11-03 19:51:13,630] Trial 3927 pruned. Pruned at step 20 with metric 0.6256
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3928 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.6488137694285736e-05
  Dropout: 0.13864697370890916
================================================================================

[I 2025-11-03 19:55:52,752] Trial 3928 pruned. Pruned at step 11 with metric 0.6340
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3929 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 4.939572783351035e-05
  Dropout: 0.12236830690614292
================================================================================

[I 2025-11-03 19:59:27,992] Trial 3929 pruned. Pruned at step 9 with metric 0.6383
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3930 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.0293072641006634e-05
  Dropout: 0.45259807053391476
================================================================================

[I 2025-11-03 20:05:16,033] Trial 3930 pruned. Pruned at step 14 with metric 0.6115
[I 2025-11-03 20:05:16,565] Trial 3931 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3932 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.46924306191205e-05
  Dropout: 0.3778036095304981
================================================================================

[I 2025-11-03 20:10:58,445] Trial 3932 finished with value: 0.6855828220858895 and parameters: {'seed': 37110, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.46924306191205e-05, 'optim.weight_decay': 0.0321680518294018, 'optim.beta1': 0.8856414608862186, 'optim.beta2': 0.962574852442977, 'optim.eps': 3.4069158991813784e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.006837814762399101, 'sched.poly_power': 0.8635000291025425, 'train.clip_grad': 1.0585758569721169, 'model.dropout': 0.3778036095304981, 'model.attn_dropout': 0.14294608882175763, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9281573845899437, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.2972801778150089, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.04054243545088864, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 3933 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.4944184966864057e-05
  Dropout: 0.39058292777670345
================================================================================

[I 2025-11-03 20:14:54,861] Trial 3926 pruned. Pruned at step 11 with metric 0.6079
[W 2025-11-03 20:14:55,379] The parameter `tok.doc_stride` in Trial#3934 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3934 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.69177643508014e-06
  Dropout: 0.2715015522888214
================================================================================

[I 2025-11-03 20:27:42,734] Trial 3934 pruned. Pruned at step 13 with metric 0.5988
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3935 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.876577174363513e-06
  Dropout: 0.17119043127081965
================================================================================

[I 2025-11-03 20:33:04,296] Trial 3935 pruned. Pruned at step 13 with metric 0.6492
[I 2025-11-03 20:33:04,822] Trial 3936 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-03 20:33:05,327] Trial 3937 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3938 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.3276792695866566e-05
  Dropout: 0.4215514463127191
================================================================================

[I 2025-11-03 20:40:49,235] Trial 3933 finished with value: 0.6398429157049847 and parameters: {'seed': 33305, 'model.name': 'bert-large-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.4944184966864057e-05, 'optim.weight_decay': 1.7086311104277863e-05, 'optim.beta1': 0.8760769958741791, 'optim.beta2': 0.9717846694545504, 'optim.eps': 1.3684415673400631e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.17158569241493912, 'sched.poly_power': 0.8562382011107106, 'train.clip_grad': 1.3597899048097286, 'model.dropout': 0.39058292777670345, 'model.attn_dropout': 0.10097711385125045, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9549906359320288, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4650139148572514, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.02762849744692226, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3939 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.5330577388087392e-05
  Dropout: 0.20923749572294242
================================================================================

[I 2025-11-03 20:44:21,674] Trial 3939 pruned. Pruned at step 10 with metric 0.6279
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3940 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.181643461381225e-05
  Dropout: 0.07319605893618901
================================================================================

[I 2025-11-03 20:46:26,242] Trial 3938 finished with value: 0.6785727943707216 and parameters: {'seed': 49120, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.3276792695866566e-05, 'optim.weight_decay': 3.8556219195061085e-06, 'optim.beta1': 0.8380921080374376, 'optim.beta2': 0.9980749445563664, 'optim.eps': 2.999633097272442e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1532890951561675, 'sched.poly_power': 0.8549423968425548, 'train.clip_grad': 1.0297171839979238, 'model.dropout': 0.4215514463127191, 'model.attn_dropout': 0.1427355981231338, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9370808936838836, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.37350490524621527, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.055721473852388306, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3941 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.722709037478517e-06
  Dropout: 0.08497492318137817
================================================================================

[I 2025-11-03 20:50:27,601] Trial 3940 finished with value: 0.6628934184863666 and parameters: {'seed': 61919, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.181643461381225e-05, 'optim.weight_decay': 0.03870094822674918, 'optim.beta1': 0.8638600393775816, 'optim.beta2': 0.964682675692583, 'optim.eps': 1.9276356257593138e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.14269004288142362, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.6719192052062879, 'model.dropout': 0.07319605893618901, 'model.attn_dropout': 0.13765483170383633, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.847455366020741, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.09297250001001883, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.657375704729706, 'loss.cls.alpha': 0.19606471432638276, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-03 20:50:28,112] The parameter `tok.doc_stride` in Trial#3942 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 20:50:28,164] Trial 3942 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3943 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.439709578683012e-06
  Dropout: 0.2977699738182807
================================================================================

[I 2025-11-03 20:55:12,396] Trial 3943 pruned. Pruned at step 8 with metric 0.5988
[I 2025-11-03 20:55:12,926] Trial 3944 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[W 2025-11-03 20:55:13,402] The parameter `tok.doc_stride` in Trial#3945 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3945 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.0803339737155174e-05
  Dropout: 0.44385099285747187
================================================================================

[I 2025-11-03 20:55:24,767] Trial 3941 pruned. Pruned at step 27 with metric 0.5962
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3946 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 9.529372085124059e-06
  Dropout: 0.2053984525450147
================================================================================

[I 2025-11-03 20:59:12,574] Trial 3946 pruned. Pruned at step 9 with metric 0.5567
[I 2025-11-03 20:59:13,131] Trial 3947 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3948 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.570093858874527e-06
  Dropout: 0.2421084107310204
================================================================================

[I 2025-11-03 21:02:42,025] Trial 3945 pruned. Pruned at step 9 with metric 0.5948

================================================================================
TRIAL 3949 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.353411534440513e-06
  Dropout: 0.20645400829240484
================================================================================

[I 2025-11-03 21:18:13,557] Trial 3949 finished with value: 0.6762848119437266 and parameters: {'seed': 25134, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.353411534440513e-06, 'optim.weight_decay': 1.2560814371885447e-06, 'optim.beta1': 0.8348496521289246, 'optim.beta2': 0.9954421151752694, 'optim.eps': 4.1953812159957506e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.16292018714918577, 'sched.poly_power': 0.826569996264686, 'train.clip_grad': 0.7373078871136082, 'model.dropout': 0.20645400829240484, 'model.attn_dropout': 0.1923195869679349, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8105903414054316, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.06862511611762266, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.9080255841667295, 'loss.cls.alpha': 0.10583192759667338, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 3950 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.4192427522892547e-05
  Dropout: 0.20145889210457518
================================================================================

[I 2025-11-03 21:20:25,588] Trial 3950 pruned. Pruned at step 10 with metric 0.5997
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3951 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 3.025237010265511e-05
  Dropout: 0.018620461887782415
================================================================================

[I 2025-11-03 21:38:37,618] Trial 3948 pruned. Pruned at step 12 with metric 0.6125
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3952 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.3788877035085855e-05
  Dropout: 0.3874592614078237
================================================================================

[I 2025-11-03 21:45:57,672] Trial 3951 finished with value: 0.4489247311827957 and parameters: {'seed': 64054, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 3.025237010265511e-05, 'optim.weight_decay': 0.0022629486606752167, 'optim.beta1': 0.8185649278552005, 'optim.beta2': 0.9617103349397848, 'optim.eps': 1.4467701144359876e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1368412806315916, 'train.clip_grad': 0.9038092226211527, 'model.dropout': 0.018620461887782415, 'model.attn_dropout': 0.22369918180226953, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8687925796466891, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.07240523864355261, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.03963471934136586, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3953 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.8869758605860372e-05
  Dropout: 0.47333353903990727
================================================================================

[I 2025-11-03 21:52:12,415] Trial 3952 finished with value: 0.6816770186335404 and parameters: {'seed': 45799, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.3788877035085855e-05, 'optim.weight_decay': 2.627972092141281e-06, 'optim.beta1': 0.8491170438126956, 'optim.beta2': 0.9696293270506947, 'optim.eps': 1.4691214360468087e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.16067477724256776, 'train.clip_grad': 1.24664358535238, 'model.dropout': 0.3874592614078237, 'model.attn_dropout': 0.09072192485992794, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9655871433314657, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.48741079094900486, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.52381039154938, 'loss.cls.alpha': 0.1025956194522186, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 21:52:12,966] Trial 3954 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3955 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 5.571215683561424e-06
  Dropout: 0.08770119856633797
================================================================================

[I 2025-11-03 21:52:19,172] Trial 3955 pruned. OOM: microsoft/deberta-v3-large bs=12 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3955 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 116.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3956 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.6322809916434286e-05
  Dropout: 0.30392873016084415
================================================================================

[I 2025-11-03 21:57:47,078] Trial 3956 pruned. Pruned at step 12 with metric 0.5688

================================================================================
TRIAL 3957 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 3.194656160821847e-05
  Dropout: 0.4030476181430863
================================================================================

[I 2025-11-03 22:13:39,803] Trial 3953 finished with value: 0.7204545454545455 and parameters: {'seed': 46064, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.8869758605860372e-05, 'optim.weight_decay': 0.0015875188181178844, 'optim.beta1': 0.8816949610734635, 'optim.beta2': 0.9781740072623253, 'optim.eps': 1.7205145867354417e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.1119964001888401, 'train.clip_grad': 0.6858210415301589, 'model.dropout': 0.47333353903990727, 'model.attn_dropout': 0.275254708313388, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8150256301308237, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.41693513386523273, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.235701556427997, 'loss.cls.alpha': 0.16465257034402483, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 22:13:40,351] Trial 3958 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[W 2025-11-03 22:13:40,821] The parameter `tok.doc_stride` in Trial#3959 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 3959 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.0608782054836447e-05
  Dropout: 0.43914411140900683
================================================================================

[I 2025-11-03 22:18:48,038] Trial 3959 pruned. Pruned at step 10 with metric 0.6556
[I 2025-11-03 22:18:48,578] Trial 3960 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-03 22:18:49,087] Trial 3961 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3962 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.2540351456461099e-05
  Dropout: 0.35863932493303236
================================================================================

[I 2025-11-03 22:21:44,761] Trial 3962 pruned. Pruned at step 10 with metric 0.5905
[I 2025-11-03 22:21:45,295] Trial 3963 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3964 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 8.15387835745087e-06
  Dropout: 0.4499640594736664
================================================================================

[I 2025-11-03 22:35:41,841] Trial 3964 pruned. Pruned at step 28 with metric 0.5512
[W 2025-11-03 22:35:42,342] The parameter `tok.doc_stride` in Trial#3965 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3965 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.4104423859214028e-05
  Dropout: 0.032705863860742736
================================================================================

[I 2025-11-03 22:39:54,696] Trial 3957 finished with value: 0.44141689373297005 and parameters: {'seed': 53269, 'model.name': 'bert-large-uncased', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.194656160821847e-05, 'optim.weight_decay': 0.0010060656431026573, 'optim.beta1': 0.8591198253175659, 'optim.beta2': 0.9608493435755832, 'optim.eps': 1.0665012232819545e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.03340689580678658, 'sched.poly_power': 0.7817127282819194, 'train.clip_grad': 1.2713345680195693, 'model.dropout': 0.4030476181430863, 'model.attn_dropout': 0.20346454781584072, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9080428491588914, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.22609112264973963, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.953336459052078, 'loss.cls.alpha': 0.2848293370804623, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-03 22:39:55,198] The parameter `tok.doc_stride` in Trial#3966 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 3966 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2608740357686094e-05
  Dropout: 0.3729370924795387
================================================================================

[I 2025-11-03 22:46:13,321] Trial 3966 pruned. Pruned at step 8 with metric 0.5474
[I 2025-11-03 22:46:13,865] Trial 3967 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3968 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 8.689309292747891e-06
  Dropout: 0.3650204431950966
================================================================================

[I 2025-11-03 22:48:08,883] Trial 3965 finished with value: 0.6369256474519632 and parameters: {'seed': 52207, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.4104423859214028e-05, 'optim.weight_decay': 0.0009562348830867759, 'optim.beta1': 0.888091039610534, 'optim.beta2': 0.9845959037534339, 'optim.eps': 2.5582115815510927e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.11212840611982947, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.9194984146877587, 'model.dropout': 0.032705863860742736, 'model.attn_dropout': 0.11431103902740687, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9169148392000693, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.06460296204602324, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.04458916631577692, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 22:48:09,434] Trial 3969 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-03 22:48:09,946] Trial 3970 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3971 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.1832504733122908e-05
  Dropout: 0.1627480363737337
================================================================================

[I 2025-11-03 22:57:21,633] Trial 3971 pruned. Pruned at step 17 with metric 0.6556

================================================================================
TRIAL 3972 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 9.295952629239557e-06
  Dropout: 0.27705756522280095
================================================================================

[I 2025-11-03 23:05:36,790] Trial 3972 pruned. Pruned at step 12 with metric 0.5713
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3973 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.7019672851415346e-05
  Dropout: 0.4182397415298593
================================================================================

[I 2025-11-03 23:09:07,913] Trial 3968 pruned. Pruned at step 11 with metric 0.6069

================================================================================
TRIAL 3974 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.6916364372737354e-05
  Dropout: 0.3411804824960994
================================================================================

[I 2025-11-03 23:17:54,159] Trial 3974 pruned. Pruned at step 10 with metric 0.5486
[I 2025-11-03 23:17:54,695] Trial 3975 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-03 23:17:55,176] The parameter `tok.doc_stride` in Trial#3976 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-03 23:17:55,228] Trial 3976 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-03 23:17:55,695] The parameter `tok.doc_stride` in Trial#3977 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 3977 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 8.2439156447749e-06
  Dropout: 0.028983890653609193
================================================================================

[I 2025-11-03 23:31:31,000] Trial 3977 finished with value: 0.7256109220237867 and parameters: {'seed': 55092, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.2439156447749e-06, 'optim.weight_decay': 0.00597806554935387, 'optim.beta1': 0.8833309038285553, 'optim.beta2': 0.963724918543352, 'optim.eps': 5.309461964021442e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1632881049816572, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.332234154445724, 'model.dropout': 0.028983890653609193, 'model.attn_dropout': 0.27110109023316775, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8440790199155429, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.08139406405938708, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.700741408500348, 'loss.cls.alpha': 0.12021411190771022, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 3978 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 8.698836657373284e-06
  Dropout: 0.3661711309106398
================================================================================

[I 2025-11-03 23:31:54,650] Trial 3978 pruned. OOM: roberta-base bs=64 len=352
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3978 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 512 with grad_accum=8)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 76.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3979 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.394734438782496e-05
  Dropout: 0.24890148338549556
================================================================================

[I 2025-11-03 23:35:13,531] Trial 3973 pruned. Pruned at step 16 with metric 0.6174

================================================================================
TRIAL 3980 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 3.654534027741974e-05
  Dropout: 0.020334357537972927
================================================================================

[I 2025-11-03 23:38:57,287] Trial 3980 pruned. Pruned at step 9 with metric 0.5941
[I 2025-11-03 23:38:57,821] Trial 3981 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3982 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 6.294742182305311e-06
  Dropout: 0.40432580585053685
================================================================================

[I 2025-11-03 23:45:50,529] Trial 3982 pruned. Pruned at step 10 with metric 0.5801
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 3983 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.9468102314664112e-05
  Dropout: 0.08193890716365529
================================================================================

[I 2025-11-03 23:58:29,549] Trial 3983 finished with value: 0.7105882352941176 and parameters: {'seed': 57527, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.9468102314664112e-05, 'optim.weight_decay': 0.0006221919360858622, 'optim.beta1': 0.9194893533285659, 'optim.beta2': 0.9961557807819774, 'optim.eps': 2.36677803411781e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0819866242306623, 'sched.poly_power': 0.774306027964909, 'train.clip_grad': 1.109859349044446, 'model.dropout': 0.08193890716365529, 'model.attn_dropout': 0.2700772667612794, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8180430227877857, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.3448383210044275, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.025238522567498, 'loss.cls.alpha': 0.12630993409199578, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-03 23:58:30,075] Trial 3984 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 3985 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.9549595450467546e-05
  Dropout: 0.21716571153870012
================================================================================

[I 2025-11-04 00:08:52,788] Trial 3985 finished with value: 0.7090322580645161 and parameters: {'seed': 43613, 'model.name': 'roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 2.9549595450467546e-05, 'optim.weight_decay': 0.0008344432177232676, 'optim.beta1': 0.8423617367645312, 'optim.beta2': 0.9598627078583268, 'optim.eps': 7.702754169892346e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.018212078630292963, 'sched.poly_power': 0.9386860506304793, 'train.clip_grad': 0.6115653081292505, 'model.dropout': 0.21716571153870012, 'model.attn_dropout': 0.19481588424966745, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9756400211156513, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.3132949848668913, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5934593796268715, 'loss.cls.alpha': 0.11361918891593095, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 00:08:53,286] The parameter `tok.doc_stride` in Trial#3986 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 3986 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 4.5773694812189206e-05
  Dropout: 0.3016960372989331
================================================================================

[I 2025-11-04 00:11:03,879] Trial 3979 finished with value: 0.6762848119437266 and parameters: {'seed': 32759, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.394734438782496e-05, 'optim.weight_decay': 1.1413039854158577e-05, 'optim.beta1': 0.9095871891257805, 'optim.beta2': 0.9741689874882027, 'optim.eps': 3.089408748493099e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.11195490865791564, 'train.clip_grad': 1.0390278116568779, 'model.dropout': 0.24890148338549556, 'model.attn_dropout': 0.09675393530844723, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9726438609342638, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.4913563190001274, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.72910864090765, 'loss.cls.alpha': 0.20330027975819162, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 3987 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 9.551160797370958e-06
  Dropout: 0.45189577029018446
================================================================================

[I 2025-11-04 00:25:07,499] Trial 3986 pruned. Pruned at step 27 with metric 0.5545

================================================================================
TRIAL 3988 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.0161733847924223e-05
  Dropout: 0.06122212982065789
================================================================================

[I 2025-11-04 00:26:01,160] Trial 3987 pruned. Pruned at step 10 with metric 0.5593
[I 2025-11-04 00:26:01,796] Trial 3989 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3990 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.707734689315726e-06
  Dropout: 0.22181546499733976
================================================================================

[I 2025-11-04 00:26:55,118] Trial 3988 pruned. Pruned at step 9 with metric 0.6390
[W 2025-11-04 00:26:55,646] The parameter `tok.doc_stride` in Trial#3991 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 00:26:55,699] Trial 3991 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 3992 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 6.60266490107539e-06
  Dropout: 0.07292635249387194
================================================================================

[I 2025-11-04 00:27:03,035] Trial 3990 pruned. OOM: bert-base-uncased bs=24 len=192
[I 2025-11-04 00:27:03,214] Trial 3992 pruned. OOM: microsoft/deberta-v3-large bs=16 len=352
[I 2025-11-04 00:27:04,034] Trial 3993 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[W 2025-11-04 00:27:04,613] The parameter `tok.doc_stride` in Trial#3995 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 3992 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 62.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 3990 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 24 (effective: 48 with grad_accum=2)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 62.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 3994 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.8669689182747658e-05
  Dropout: 0.3137512112919976
================================================================================


================================================================================
TRIAL 3995 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.3758585564346183e-05
  Dropout: 0.4164794642589565
================================================================================

[I 2025-11-04 00:40:26,698] Trial 3994 pruned. Pruned at step 27 with metric 0.6571
[I 2025-11-04 00:40:27,259] Trial 3996 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 3997 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.2437414679483935e-06
  Dropout: 0.4979822777667562
================================================================================

[I 2025-11-04 00:48:26,624] Trial 3997 finished with value: 0.5859363990793367 and parameters: {'seed': 37887, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 7.2437414679483935e-06, 'optim.weight_decay': 0.059106511475606485, 'optim.beta1': 0.8334391787675859, 'optim.beta2': 0.9715722995882831, 'optim.eps': 4.979035007386999e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.011901613901965953, 'train.clip_grad': 1.1602354337956553, 'model.dropout': 0.4979822777667562, 'model.attn_dropout': 0.23910479311803678, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9915422185277342, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.39732343062355147, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.94207077153491, 'loss.cls.alpha': 0.26396279787162336, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 00:48:27,163] Trial 3998 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 3999 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.002041559890327e-06
  Dropout: 0.18347012538801216
================================================================================

[I 2025-11-04 00:49:45,322] Trial 3995 finished with value: 0.7461904761904762 and parameters: {'seed': 40722, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.3758585564346183e-05, 'optim.weight_decay': 0.04441247566219795, 'optim.beta1': 0.8235630003590697, 'optim.beta2': 0.963291416070782, 'optim.eps': 2.6746874265160597e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.015935334112614247, 'sched.poly_power': 0.7755866003635599, 'train.clip_grad': 1.31534771865878, 'model.dropout': 0.4164794642589565, 'model.attn_dropout': 0.13421249553038256, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9683351736002718, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.1892801579876054, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.660956967716299, 'loss.cls.alpha': 0.20240336665613007, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 00:49:45,873] Trial 4000 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 4001 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.218174538802548e-05
  Dropout: 0.3339004739739735
================================================================================

[I 2025-11-04 00:57:25,174] Trial 4001 finished with value: 0.4605263157894737 and parameters: {'seed': 37151, 'model.name': 'roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.218174538802548e-05, 'optim.weight_decay': 0.011274329324291227, 'optim.beta1': 0.852477861275064, 'optim.beta2': 0.961974982482001, 'optim.eps': 2.2751176425509633e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.014141007868882574, 'train.clip_grad': 1.1098914947058272, 'model.dropout': 0.3339004739739735, 'model.attn_dropout': 0.08256536385803559, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9923800527573727, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.26372507660643046, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5277523270062705, 'loss.cls.alpha': 0.22359476763815153, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 00:57:25,726] Trial 4002 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-04 00:57:26,208] The parameter `tok.doc_stride` in Trial#4003 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4003 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.625446816561764e-05
  Dropout: 0.4508939999219446
================================================================================

[I 2025-11-04 01:03:01,820] Trial 3999 pruned. Pruned at step 11 with metric 0.5917
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4004 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.4063624718078046e-05
  Dropout: 0.30988932494496785
================================================================================

[I 2025-11-04 01:07:34,394] Trial 4004 pruned. Pruned at step 9 with metric 0.5927
[W 2025-11-04 01:07:34,894] The parameter `tok.doc_stride` in Trial#4005 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4005 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.133458599797842e-05
  Dropout: 0.3937295981232822
================================================================================

[I 2025-11-04 01:09:53,415] Trial 4003 pruned. Pruned at step 9 with metric 0.5845
[W 2025-11-04 01:09:54,041] The parameter `tok.doc_stride` in Trial#4006 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4006 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.7049765628767743e-05
  Dropout: 0.445940447688139
================================================================================

[I 2025-11-04 01:12:54,119] Trial 4006 pruned. Pruned at step 11 with metric 0.5880
[W 2025-11-04 01:12:54,652] The parameter `tok.doc_stride` in Trial#4007 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4007 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.879933632382203e-05
  Dropout: 0.465983811736788
================================================================================

[I 2025-11-04 01:14:40,991] Trial 4005 pruned. Pruned at step 18 with metric 0.6588

================================================================================
TRIAL 4008 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.1441681706374889e-05
  Dropout: 0.44488281466740914
================================================================================

[I 2025-11-04 01:14:47,197] Trial 4008 pruned. OOM: bert-base-uncased bs=64 len=384
[I 2025-11-04 01:14:48,888] Trial 4007 pruned. OOM: roberta-large bs=16 len=128

[OOM] Trial 4008 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 64 with grad_accum=1)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 56.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4009 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.1354455949002414e-05
  Dropout: 0.39042490680535413
================================================================================


[OOM] Trial 4007 exceeded GPU memory:
  Model: roberta-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 36.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4010 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.186552609563995e-05
  Dropout: 0.22294561593379644
================================================================================

[I 2025-11-04 01:21:40,143] Trial 4009 pruned. Pruned at step 27 with metric 0.5997

================================================================================
TRIAL 4011 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.9705956111148486e-05
  Dropout: 0.10814139397254299
================================================================================

[I 2025-11-04 01:23:57,433] Trial 4010 pruned. Pruned at step 10 with metric 0.5878
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4012 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.413805119740603e-05
  Dropout: 0.4581369211358825
================================================================================

[I 2025-11-04 01:29:56,021] Trial 4012 pruned. Pruned at step 10 with metric 0.6257
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4013 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.5791974152399237e-05
  Dropout: 0.414746702679411
================================================================================

[I 2025-11-04 01:38:41,817] Trial 4013 pruned. Pruned at step 13 with metric 0.5629
[I 2025-11-04 01:38:42,359] Trial 4014 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-04 01:38:42,848] The parameter `tok.doc_stride` in Trial#4015 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 01:38:42,900] Trial 4015 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4016 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.734237852744714e-05
  Dropout: 0.3913291629016025
================================================================================

[I 2025-11-04 01:44:50,667] Trial 4016 pruned. Pruned at step 13 with metric 0.5839
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4017 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 9.75686871821636e-06
  Dropout: 0.4417385079566231
================================================================================

[I 2025-11-04 01:49:35,274] Trial 4011 finished with value: 0.6602065536146883 and parameters: {'seed': 42897, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.9705956111148486e-05, 'optim.weight_decay': 0.0007690307337751138, 'optim.beta1': 0.8540241130546452, 'optim.beta2': 0.9598905229251214, 'optim.eps': 1.0045632631733858e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.16860428723118628, 'train.clip_grad': 0.8806295809585731, 'model.dropout': 0.10814139397254299, 'model.attn_dropout': 0.2272419368228362, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8439945784014719, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.3000437456697068, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.0037472294537786166, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 01:49:35,817] Trial 4018 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-04 01:49:36,349] Trial 4019 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-04 01:49:36,868] Trial 4020 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-04 01:49:37,401] Trial 4021 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-04 01:49:37,919] Trial 4022 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 39 (patience=20)

================================================================================
TRIAL 4023 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 6.049688075537272e-06
  Dropout: 0.3543610108963425
================================================================================

[I 2025-11-04 01:56:03,623] Trial 4017 pruned. Pruned at step 11 with metric 0.5997
[I 2025-11-04 01:56:04,172] Trial 4024 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-04 01:56:04,651] The parameter `tok.doc_stride` in Trial#4025 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4025 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.08356524548645e-05
  Dropout: 0.14105940039627923
================================================================================

[I 2025-11-04 02:00:33,178] Trial 4023 pruned. Pruned at step 9 with metric 0.6156
[W 2025-11-04 02:00:33,703] The parameter `tok.doc_stride` in Trial#4026 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4026 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.579679902337607e-05
  Dropout: 0.3767542607658133
================================================================================

[I 2025-11-04 02:04:48,183] Trial 4025 pruned. Pruned at step 16 with metric 0.5884

================================================================================
TRIAL 4027 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.4224440210721844e-06
  Dropout: 0.3829831732671123
================================================================================

[I 2025-11-04 02:09:31,732] Trial 4027 pruned. Pruned at step 14 with metric 0.6215
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4028 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.3271977560471294e-05
  Dropout: 0.45874221884852606
================================================================================

[I 2025-11-04 02:19:23,205] Trial 4028 pruned. Pruned at step 27 with metric 0.6402

================================================================================
TRIAL 4029 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2861989741385341e-05
  Dropout: 0.29527233010845866
================================================================================

[I 2025-11-04 02:28:39,200] Trial 4029 pruned. Pruned at step 14 with metric 0.6002
[W 2025-11-04 02:28:39,722] The parameter `tok.doc_stride` in Trial#4030 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4030 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.9692680894254455e-05
  Dropout: 0.4938830631938509
================================================================================

[I 2025-11-04 02:36:45,020] Trial 4026 pruned. Pruned at step 81 with metric 0.5945
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4031 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.530969250211021e-06
  Dropout: 0.3231387354525888
================================================================================

[I 2025-11-04 02:41:05,017] Trial 4031 pruned. Pruned at step 9 with metric 0.5785

================================================================================
TRIAL 4032 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1034488594530565e-05
  Dropout: 0.2812910599658696
================================================================================

[I 2025-11-04 02:45:25,730] Trial 4032 pruned. Pruned at step 12 with metric 0.6315
[I 2025-11-04 02:45:26,272] Trial 4033 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4034 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.2417043563332073e-05
  Dropout: 0.49036995124252536
================================================================================

[I 2025-11-04 02:54:11,071] Trial 4034 pruned. Pruned at step 16 with metric 0.6238
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4035 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 3.775046812243063e-05
  Dropout: 0.478650112976611
================================================================================

[I 2025-11-04 03:04:43,237] Trial 4030 pruned. Pruned at step 18 with metric 0.5595

================================================================================
TRIAL 4036 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.984849990449033e-05
  Dropout: 0.17784824552204487
================================================================================

[I 2025-11-04 03:05:49,201] Trial 4035 pruned. Pruned at step 27 with metric 0.5693

================================================================================
TRIAL 4037 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.450754226681176e-05
  Dropout: 0.0698107119632733
================================================================================

[I 2025-11-04 03:08:34,983] Trial 4036 pruned. Pruned at step 11 with metric 0.5347
[I 2025-11-04 03:08:35,544] Trial 4038 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4039 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.3527562319354017e-05
  Dropout: 0.4199542506515261
================================================================================

[I 2025-11-04 03:11:38,473] Trial 4039 pruned. Pruned at step 12 with metric 0.5991
[W 2025-11-04 03:11:38,971] The parameter `tok.doc_stride` in Trial#4040 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4040 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 7.569439881563842e-06
  Dropout: 0.28310923455016174
================================================================================

[I 2025-11-04 03:16:40,900] Trial 4040 pruned. Pruned at step 11 with metric 0.5249

================================================================================
TRIAL 4041 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.0970408671230181e-05
  Dropout: 0.15155162483879703
================================================================================

[I 2025-11-04 03:18:46,618] Trial 4037 pruned. Pruned at step 10 with metric 0.5801
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4042 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.217596982441498e-05
  Dropout: 0.20989641746418433
================================================================================

[I 2025-11-04 03:20:46,517] Trial 4041 pruned. Pruned at step 9 with metric 0.6210
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4043 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.0199099690008667e-05
  Dropout: 0.3188767249552644
================================================================================

[I 2025-11-04 03:31:59,202] Trial 4043 pruned. Pruned at step 18 with metric 0.6242
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4044 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 7.691286153099559e-06
  Dropout: 0.4471281987219219
================================================================================

[I 2025-11-04 03:38:40,285] Trial 4042 pruned. Pruned at step 11 with metric 0.5900
[I 2025-11-04 03:38:40,832] Trial 4045 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4046 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 4.1135952787815006e-05
  Dropout: 0.36953297959320536
================================================================================

[I 2025-11-04 03:50:44,629] Trial 4046 pruned. Pruned at step 10 with metric 0.5878
[I 2025-11-04 03:50:45,159] Trial 4047 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4048 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 4.029289715822579e-05
  Dropout: 0.49815839143236706
================================================================================

[I 2025-11-04 04:06:34,489] Trial 4044 pruned. Pruned at step 10 with metric 0.6077
[I 2025-11-04 04:06:35,129] Trial 4049 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4050 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 9.476246566527941e-06
  Dropout: 0.42916594944745723
================================================================================

[I 2025-11-04 04:26:27,046] Trial 4050 pruned. Pruned at step 17 with metric 0.6450
[I 2025-11-04 04:26:27,769] Trial 4051 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4052 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.1503979520672442e-05
  Dropout: 0.33545534808676575
================================================================================

[I 2025-11-04 04:39:45,423] Trial 4052 finished with value: 0.6125082182774491 and parameters: {'seed': 40677, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.1503979520672442e-05, 'optim.weight_decay': 0.0002713463854129015, 'optim.beta1': 0.8340176567944341, 'optim.beta2': 0.9920180923910918, 'optim.eps': 5.639694980349962e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1999460254718436, 'sched.cosine_cycles': 4, 'train.clip_grad': 0.5192822733779067, 'model.dropout': 0.33545534808676575, 'model.attn_dropout': 0.031220932223537577, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8915798763812363, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.4715998077975537, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.298852041923784, 'loss.cls.alpha': 0.19303843870443887, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 04:39:45,944] The parameter `tok.doc_stride` in Trial#4053 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 37 (patience=20)

================================================================================
TRIAL 4053 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 2.0532917474254548e-05
  Dropout: 0.4264551314188489
================================================================================

[I 2025-11-04 04:55:26,235] Trial 4048 finished with value: 0.4474393530997305 and parameters: {'seed': 9791, 'model.name': 'bert-large-uncased', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 4.029289715822579e-05, 'optim.weight_decay': 0.01731803524086929, 'optim.beta1': 0.8521232624392537, 'optim.beta2': 0.9726355664584454, 'optim.eps': 2.226652530267581e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.003931951055342395, 'sched.poly_power': 0.8930186923217625, 'train.clip_grad': 1.3713117199651452, 'model.dropout': 0.49815839143236706, 'model.attn_dropout': 0.11048920204781812, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9280588375979965, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.15067342219600027, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.483824224784898, 'loss.cls.alpha': 0.3009345222327405, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 04:55:26,749] The parameter `tok.doc_stride` in Trial#4054 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4054 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.321767795898109e-06
  Dropout: 0.4644753941043893
================================================================================

[I 2025-11-04 04:59:21,573] Trial 4054 pruned. Pruned at step 11 with metric 0.5886
[I 2025-11-04 04:59:22,109] Trial 4055 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-04 04:59:22,632] Trial 4056 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4057 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.241348536923853e-05
  Dropout: 0.34492539417384216
================================================================================

[I 2025-11-04 05:04:10,575] Trial 4057 pruned. Pruned at step 10 with metric 0.5211

================================================================================
TRIAL 4058 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 9.20948907589675e-06
  Dropout: 0.3751742850112658
================================================================================

[I 2025-11-04 05:21:56,182] Trial 4053 pruned. Pruned at step 13 with metric 0.5975
[I 2025-11-04 05:21:56,825] Trial 4059 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-04 05:21:57,346] Trial 4060 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4061 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 0.00011790605094007572
  Dropout: 0.08387690233250585
================================================================================

[I 2025-11-04 05:30:52,795] Trial 4061 finished with value: 0.4368131868131868 and parameters: {'seed': 51928, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 0.00011790605094007572, 'optim.weight_decay': 0.0003423079239681697, 'optim.beta1': 0.942627168791006, 'optim.beta2': 0.9751790791694173, 'optim.eps': 2.968103012753777e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.05863969697002552, 'sched.poly_power': 0.7467748208517515, 'train.clip_grad': 1.2489654272672885, 'model.dropout': 0.08387690233250585, 'model.attn_dropout': 0.25651208501360956, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8444697476229661, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.2717067613099234, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.06060037716562725, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 05:30:53,378] Trial 4062 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-04 05:30:53,884] The parameter `tok.doc_stride` in Trial#4063 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4063 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 5.53103286212267e-06
  Dropout: 0.24757100944357635
================================================================================

[I 2025-11-04 05:36:09,132] Trial 4063 pruned. Pruned at step 27 with metric 0.5615

================================================================================
TRIAL 4064 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 4.052370089693644e-05
  Dropout: 0.4449225478182107
================================================================================

[I 2025-11-04 05:38:23,851] Trial 4058 finished with value: 0.6468561584840655 and parameters: {'seed': 17866, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.20948907589675e-06, 'optim.weight_decay': 0.01844178178569374, 'optim.beta1': 0.8443212548490061, 'optim.beta2': 0.9628467532535115, 'optim.eps': 1.2650212406223334e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.01078356986489733, 'sched.poly_power': 1.0029629334514447, 'train.clip_grad': 1.4321360364769777, 'model.dropout': 0.3751742850112658, 'model.attn_dropout': 0.299240811013569, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8945698029262079, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3402523533818688, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.922575496784465, 'loss.cls.alpha': 0.2157334552180993, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 05:38:24,365] The parameter `tok.doc_stride` in Trial#4065 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 52 (patience=20)

================================================================================
TRIAL 4065 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3394554700599685e-05
  Dropout: 0.4149694454095801
================================================================================

[I 2025-11-04 05:44:54,488] Trial 4065 pruned. Pruned at step 10 with metric 0.5927
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4066 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.482381863466571e-06
  Dropout: 0.3816848125697292
================================================================================

[I 2025-11-04 05:58:10,662] Trial 4066 finished with value: 0.6868055555555556 and parameters: {'seed': 62301, 'model.name': 'roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 9.482381863466571e-06, 'optim.weight_decay': 0.017209629868696875, 'optim.beta1': 0.8685279027559806, 'optim.beta2': 0.953865791397752, 'optim.eps': 2.0673486914576284e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.049752395076841635, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.395623988972126, 'model.dropout': 0.3816848125697292, 'model.attn_dropout': 0.23661914357150562, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9128073131748003, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.4875733918321221, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.389098012378558, 'loss.cls.alpha': 0.12703042202994846, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 05:58:11,220] Trial 4067 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-04 05:58:11,734] The parameter `tok.doc_stride` in Trial#4068 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 4068 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.581507828626718e-05
  Dropout: 0.36120556612181676
================================================================================

[I 2025-11-04 06:02:58,146] Trial 4064 finished with value: 0.44594594594594594 and parameters: {'seed': 57081, 'model.name': 'xlm-roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 4.052370089693644e-05, 'optim.weight_decay': 0.019634371853742293, 'optim.beta1': 0.8532933239999396, 'optim.beta2': 0.968478959746313, 'optim.eps': 5.5566307434997216e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.05715111771624171, 'sched.poly_power': 0.7462566890205208, 'train.clip_grad': 1.0438443804700839, 'model.dropout': 0.4449225478182107, 'model.attn_dropout': 0.14337452898939576, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9459626976869363, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.3124122153744745, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.98161632056781, 'loss.cls.alpha': 0.2169804519792538, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 06:02:58,703] Trial 4069 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-04 06:02:59,220] Trial 4070 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4071 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.078175841457036e-05
  Dropout: 0.40266694700389355
================================================================================

[I 2025-11-04 06:06:28,333] Trial 4071 pruned. Pruned at step 22 with metric 0.6290
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4072 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.0286756753827621e-05
  Dropout: 0.3997274990721333
================================================================================

[I 2025-11-04 06:08:42,764] Trial 4072 pruned. Pruned at step 11 with metric 0.6069

================================================================================
TRIAL 4073 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.177640666759449e-05
  Dropout: 0.405972202169799
================================================================================

[I 2025-11-04 06:14:36,895] Trial 4073 pruned. Pruned at step 11 with metric 0.6359
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4074 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.1228575419572962e-05
  Dropout: 0.43013982486611874
================================================================================

[I 2025-11-04 06:23:56,990] Trial 4074 pruned. Pruned at step 27 with metric 0.5997

================================================================================
TRIAL 4075 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 0.00010298029446574487
  Dropout: 0.13306453351685438
================================================================================

[I 2025-11-04 06:34:39,593] Trial 4075 finished with value: 0.4209039548022599 and parameters: {'seed': 6839, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 0.00010298029446574487, 'optim.weight_decay': 0.003968937970744745, 'optim.beta1': 0.9010217940857952, 'optim.beta2': 0.9848774691116664, 'optim.eps': 1.6670174065761748e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.014958883428462966, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.352122558022181, 'model.dropout': 0.13306453351685438, 'model.attn_dropout': 0.2866738307890651, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8490313243170298, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.3941676976795852, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.843602773163471, 'loss.cls.alpha': 0.1015479759801689, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4076 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.045258412821394e-05
  Dropout: 0.391286364809412
================================================================================

[I 2025-11-04 06:37:21,589] Trial 4076 pruned. Pruned at step 12 with metric 0.5510
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4077 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.536490200511382e-05
  Dropout: 0.4889694219206413
================================================================================

[I 2025-11-04 06:41:58,999] Trial 4077 pruned. Pruned at step 9 with metric 0.5725
[I 2025-11-04 06:41:59,540] Trial 4078 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4079 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.685289805979193e-05
  Dropout: 0.29316105720743274
================================================================================

[I 2025-11-04 06:44:56,552] Trial 4079 pruned. Pruned at step 9 with metric 0.6164

================================================================================
TRIAL 4080 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.15431531149828e-05
  Dropout: 0.4473758827582205
================================================================================

[I 2025-11-04 06:45:24,827] Trial 4068 pruned. Pruned at step 27 with metric 0.6530
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4081 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.546167304138609e-05
  Dropout: 0.4877456397490618
================================================================================

[I 2025-11-04 06:51:02,673] Trial 4080 pruned. Pruned at step 19 with metric 0.6229
[I 2025-11-04 06:51:03,228] Trial 4082 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-04 06:51:03,706] The parameter `tok.doc_stride` in Trial#4083 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4083 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.1376548950623943e-05
  Dropout: 0.3724958123699387
================================================================================

[I 2025-11-04 07:01:08,942] Trial 4083 finished with value: 0.7295138888888888 and parameters: {'seed': 61184, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.1376548950623943e-05, 'optim.weight_decay': 0.02752990161411038, 'optim.beta1': 0.8922140589426785, 'optim.beta2': 0.956581085362852, 'optim.eps': 1.1742853143651116e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.08535699593162463, 'sched.poly_power': 0.7365819571712613, 'train.clip_grad': 1.4480658937686801, 'model.dropout': 0.3724958123699387, 'model.attn_dropout': 0.21079425613824784, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8799807208145842, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.43476338777960444, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.511195329622764, 'loss.cls.alpha': 0.17947005030053828, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 07:01:09,493] Trial 4084 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 39 (patience=20)

================================================================================
TRIAL 4085 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.0409688209077154e-05
  Dropout: 0.16735999828554657
================================================================================

[I 2025-11-04 07:07:49,720] Trial 4081 pruned. Pruned at step 16 with metric 0.5743

================================================================================
TRIAL 4086 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 6.15486461152579e-06
  Dropout: 0.4996010158043283
================================================================================

[I 2025-11-04 07:16:50,941] Trial 4086 pruned. Pruned at step 15 with metric 0.5780
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4087 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.126837013725376e-05
  Dropout: 0.36722052231524016
================================================================================

[I 2025-11-04 07:38:50,498] Trial 4087 finished with value: 0.6916792000325189 and parameters: {'seed': 36553, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 6.126837013725376e-05, 'optim.weight_decay': 0.0011001255702863134, 'optim.beta1': 0.853206818304243, 'optim.beta2': 0.9730392452179154, 'optim.eps': 4.109578741988978e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.015206997138366046, 'train.clip_grad': 1.2030993718419065, 'model.dropout': 0.36722052231524016, 'model.attn_dropout': 0.15138238895062256, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9887793473468534, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.17195788918508476, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.6882724074177755, 'loss.cls.alpha': 0.20139369273298502, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 4088 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.46662987372944e-06
  Dropout: 0.27012852838452256
================================================================================

[I 2025-11-04 07:53:50,189] Trial 4088 pruned. Pruned at step 18 with metric 0.6112
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4089 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 2.2572692284434524e-05
  Dropout: 0.27179880247979077
================================================================================

[I 2025-11-04 08:05:46,652] Trial 4089 pruned. Pruned at step 17 with metric 0.6254
[I 2025-11-04 08:05:47,191] Trial 4090 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-04 08:05:47,685] The parameter `tok.doc_stride` in Trial#4091 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4091 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.1703028293447657e-05
  Dropout: 0.09429616756216846
================================================================================

[I 2025-11-04 08:08:28,326] Trial 4091 pruned. Pruned at step 9 with metric 0.6562
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4092 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.6405299497823777e-05
  Dropout: 0.4945858244990478
================================================================================

[I 2025-11-04 08:19:01,583] Trial 4085 finished with value: 0.6952974289238165 and parameters: {'seed': 60795, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.0409688209077154e-05, 'optim.weight_decay': 1.1045560442979735e-06, 'optim.beta1': 0.8572144186823624, 'optim.beta2': 0.9884035789404132, 'optim.eps': 4.3312413462356554e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.14399770917308713, 'train.clip_grad': 1.4949665316035976, 'model.dropout': 0.16735999828554657, 'model.attn_dropout': 0.14360219523182177, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9272538890251704, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.39088929421652113, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.376990353355661, 'loss.cls.alpha': 0.18171987799093664, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 08:19:02,141] Trial 4093 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-04 08:19:02,628] The parameter `tok.doc_stride` in Trial#4094 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 08:19:02,682] Trial 4094 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 46 (patience=20)

================================================================================
TRIAL 4095 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.1263499522286841e-05
  Dropout: 0.06192077858543937
================================================================================

[I 2025-11-04 08:23:41,276] Trial 4092 pruned. Pruned at step 15 with metric 0.5842
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4096 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.138743913815247e-05
  Dropout: 0.38611585632369116
================================================================================

[I 2025-11-04 08:29:38,096] Trial 4095 pruned. Pruned at step 27 with metric 0.6197
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4097 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.129033170787981e-05
  Dropout: 0.41717720610839065
================================================================================

[I 2025-11-04 08:47:22,008] Trial 4097 pruned. Pruned at step 27 with metric 0.6176
[W 2025-11-04 08:47:22,519] The parameter `tok.doc_stride` in Trial#4098 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4098 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.854701697884967e-06
  Dropout: 0.05251733178969692
================================================================================

[I 2025-11-04 08:48:11,913] Trial 4096 finished with value: 0.7092656791679799 and parameters: {'seed': 32881, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.138743913815247e-05, 'optim.weight_decay': 0.0016674097179411693, 'optim.beta1': 0.802225968205647, 'optim.beta2': 0.9619167845857662, 'optim.eps': 6.698154173142086e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0013199760222031218, 'sched.poly_power': 0.9263877570106939, 'train.clip_grad': 1.0155150128392996, 'model.dropout': 0.38611585632369116, 'model.attn_dropout': 0.21021621234255117, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9719112186680924, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.15932720363877295, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1996903211528615, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 44 (patience=20)

================================================================================
TRIAL 4099 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.213661944753499e-05
  Dropout: 0.41674784624025124
================================================================================

[I 2025-11-04 08:55:38,956] Trial 4099 pruned. Pruned at step 13 with metric 0.5558
[W 2025-11-04 08:55:39,493] The parameter `tok.doc_stride` in Trial#4100 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4100 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.4925125151189166e-05
  Dropout: 0.3281088064901434
================================================================================

[I 2025-11-04 09:06:58,266] Trial 4098 pruned. Pruned at step 27 with metric 0.6328

================================================================================
TRIAL 4101 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.0166541581425122e-05
  Dropout: 0.37692868905428867
================================================================================

[I 2025-11-04 09:07:51,869] Trial 4100 finished with value: 0.6769618657421999 and parameters: {'seed': 58353, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.4925125151189166e-05, 'optim.weight_decay': 0.0008959039150657266, 'optim.beta1': 0.8188600753381898, 'optim.beta2': 0.98415524558174, 'optim.eps': 2.0853750864974513e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1986292667463938, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.225477313593561, 'model.dropout': 0.3281088064901434, 'model.attn_dropout': 0.13471161266325438, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.950801986890393, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.4564209686684273, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.4183788106270359, 'loss.cls.alpha': 0.3619478771308, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4102 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.7950556678228544e-05
  Dropout: 0.4106256456922356
================================================================================

[I 2025-11-04 09:17:05,386] Trial 4102 pruned. Pruned at step 14 with metric 0.5603
[I 2025-11-04 09:17:05,954] Trial 4103 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4104 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.03022203574008e-05
  Dropout: 0.09583150607644228
================================================================================

[I 2025-11-04 09:28:55,633] Trial 4101 pruned. Pruned at step 27 with metric 0.5770
[I 2025-11-04 09:28:56,192] Trial 4105 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-04 09:28:56,714] Trial 4106 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[W 2025-11-04 09:28:57,200] The parameter `tok.doc_stride` in Trial#4107 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4107 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.179231871116786e-05
  Dropout: 0.36263632244093114
================================================================================

[I 2025-11-04 09:31:36,211] Trial 4107 pruned. Pruned at step 9 with metric 0.6121
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4108 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.8757373758578805e-05
  Dropout: 0.15568137460581996
================================================================================

[I 2025-11-04 09:40:50,277] Trial 4108 finished with value: 0.6437437934458788 and parameters: {'seed': 60385, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.8757373758578805e-05, 'optim.weight_decay': 0.024565506552303926, 'optim.beta1': 0.8592541250539456, 'optim.beta2': 0.9645258687615132, 'optim.eps': 3.5657970005477537e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18657786459483813, 'sched.poly_power': 1.0487336941165906, 'train.clip_grad': 0.733429962372381, 'model.dropout': 0.15568137460581996, 'model.attn_dropout': 0.06883216818680124, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9135996286871728, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.2506363691897614, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.975301430862021, 'loss.cls.alpha': 0.3131838758794432, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4109 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 5.2726140388639546e-06
  Dropout: 0.48827472754285783
================================================================================

[I 2025-11-04 09:51:11,584] Trial 4104 pruned. Pruned at step 27 with metric 0.6338
[I 2025-11-04 09:51:12,391] Trial 4110 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-04 09:51:12,919] Trial 4111 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-04 09:51:13,433] Trial 4112 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4113 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 7.220547485028525e-05
  Dropout: 0.45689417821978107
================================================================================

[I 2025-11-04 09:51:18,351] Trial 4113 pruned. OOM: bert-large-uncased bs=12 len=256
[I 2025-11-04 09:51:18,959] Trial 4114 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-04 09:51:19,445] The parameter `tok.doc_stride` in Trial#4115 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 4113 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4115 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.4382483058053374e-05
  Dropout: 0.21592826427971537
================================================================================

[I 2025-11-04 09:58:06,011] Trial 4109 pruned. Pruned at step 11 with metric 0.6159
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4116 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 6.846586699227164e-05
  Dropout: 0.07487167898415549
================================================================================

[I 2025-11-04 09:58:12,639] Trial 4116 pruned. OOM: microsoft/deberta-v3-large bs=16 len=384
[I 2025-11-04 09:58:14,774] Trial 4115 pruned. OOM: xlm-roberta-base bs=8 len=128
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4116 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 38.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4117 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.92303982543491e-05
  Dropout: 0.29141493808266516
================================================================================


[OOM] Trial 4115 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 524.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4118 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 0.00010240398444830837
  Dropout: 0.4118473884005679
================================================================================

[I 2025-11-04 10:10:22,457] Trial 4118 finished with value: 0.4489247311827957 and parameters: {'seed': 59749, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 0.00010240398444830837, 'optim.weight_decay': 0.18266975752225248, 'optim.beta1': 0.9202839821838004, 'optim.beta2': 0.9696241889889539, 'optim.eps': 7.595757239930981e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.09746705304288793, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.9438867280069085, 'model.dropout': 0.4118473884005679, 'model.attn_dropout': 0.24036204878420425, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8740135039452117, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.43063200362940157, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.98843643753636, 'loss.cls.alpha': 0.7758952502161756, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 10:10:23,021] Trial 4119 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4120 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.1403599639612013e-05
  Dropout: 0.4304711673620278
================================================================================

[I 2025-11-04 10:11:33,156] Trial 4117 pruned. Pruned at step 27 with metric 0.6583
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4121 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 3.851693545985878e-05
  Dropout: 0.3762522403207604
================================================================================

[I 2025-11-04 10:11:37,134] Trial 4121 pruned. OOM: roberta-base bs=64 len=384
[I 2025-11-04 10:11:37,695] Trial 4122 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)

[OOM] Trial 4121 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 64 with grad_accum=1)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 136.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4123 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.2412531375537017e-05
  Dropout: 0.27773483838955415
================================================================================

[I 2025-11-04 10:11:41,815] Trial 4120 pruned. OOM: roberta-base bs=32 len=352
[I 2025-11-04 10:11:42,029] Trial 4123 pruned. OOM: bert-base-uncased bs=64 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4120 exceeded GPU memory:
  Model: roberta-base
  Batch size: 32 (effective: 64 with grad_accum=2)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 132.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 116.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4123 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 296.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4124 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.3015591588470734e-05
  Dropout: 0.4136148153069691
================================================================================


================================================================================
TRIAL 4125 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.4513157176480242e-05
  Dropout: 0.39997055593108377
================================================================================

[I 2025-11-04 10:24:33,493] Trial 4125 finished with value: 0.7205335101875392 and parameters: {'seed': 64236, 'model.name': 'roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.4513157176480242e-05, 'optim.weight_decay': 0.00030261354119007466, 'optim.beta1': 0.8373848596726507, 'optim.beta2': 0.9604205779299742, 'optim.eps': 5.954021710850635e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.054617132163733983, 'sched.poly_power': 0.7174104736907497, 'train.clip_grad': 1.478593370664985, 'model.dropout': 0.39997055593108377, 'model.attn_dropout': 0.27130534995831035, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9431427055854286, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.2531618107276073, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.06501173154016152, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 10:24:34,008] The parameter `tok.doc_stride` in Trial#4126 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4126 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.6821616047880308e-05
  Dropout: 0.43450421964054053
================================================================================

[I 2025-11-04 10:25:14,638] Trial 4124 pruned. Pruned at step 9 with metric 0.6343
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4127 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.0652334868166937e-05
  Dropout: 0.2951117319543323
================================================================================

[I 2025-11-04 10:30:52,913] Trial 4127 pruned. Pruned at step 9 with metric 0.6083
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4128 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.149378925041718e-06
  Dropout: 0.3614720115488447
================================================================================

[I 2025-11-04 10:35:03,828] Trial 4128 pruned. Pruned at step 7 with metric 0.6250
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4129 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 9.787718248434132e-06
  Dropout: 0.4668616852434939
================================================================================

[I 2025-11-04 10:38:23,760] Trial 4129 pruned. Pruned at step 10 with metric 0.5962
[W 2025-11-04 10:38:24,271] The parameter `tok.doc_stride` in Trial#4130 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 10:38:24,325] Trial 4130 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-04 10:38:24,849] Trial 4131 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-04 10:38:25,398] Trial 4132 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-04 10:38:25,890] The parameter `tok.doc_stride` in Trial#4133 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 10:38:25,944] Trial 4133 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4134 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.335056848437616e-06
  Dropout: 0.054049550658356885
================================================================================

[I 2025-11-04 10:44:21,635] Trial 4134 finished with value: 0.6769618657421999 and parameters: {'seed': 51126, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 7.335056848437616e-06, 'optim.weight_decay': 4.300127826102238e-06, 'optim.beta1': 0.8673246726034163, 'optim.beta2': 0.9805923925581983, 'optim.eps': 1.2263880835285902e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1275749955405332, 'sched.poly_power': 0.8622312386112245, 'train.clip_grad': 0.7256166265678013, 'model.dropout': 0.054049550658356885, 'model.attn_dropout': 0.20193235061104248, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8090420409661506, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.04260477353092561, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.647289440990053, 'loss.cls.alpha': 0.8425558714772402, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 10:44:22,168] The parameter `tok.doc_stride` in Trial#4135 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 4135 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.593290382884022e-05
  Dropout: 0.37735405240814773
================================================================================

[I 2025-11-04 10:45:57,672] Trial 4126 pruned. Pruned at step 27 with metric 0.5308
[W 2025-11-04 10:45:58,202] The parameter `tok.doc_stride` in Trial#4136 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4136 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2532331237850358e-05
  Dropout: 0.4565398181012132
================================================================================

[I 2025-11-04 10:49:32,802] Trial 4136 pruned. Pruned at step 8 with metric 0.5688
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4137 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 6.0300744095676306e-06
  Dropout: 0.21021815065255145
================================================================================

[I 2025-11-04 10:55:18,019] Trial 4137 pruned. Pruned at step 11 with metric 0.5383
[W 2025-11-04 10:55:18,526] The parameter `tok.doc_stride` in Trial#4138 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4138 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 1.1315689365612585e-05
  Dropout: 0.4516680400952936
================================================================================

[I 2025-11-04 10:57:09,029] Trial 4135 pruned. Pruned at step 13 with metric 0.6372
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4139 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 3.4705502915021565e-05
  Dropout: 0.20975795081431856
================================================================================

[I 2025-11-04 11:05:29,323] Trial 4138 pruned. Pruned at step 12 with metric 0.5949
[I 2025-11-04 11:05:29,968] Trial 4140 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-04 11:05:30,462] The parameter `tok.doc_stride` in Trial#4141 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4141 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.3511715307693672e-05
  Dropout: 0.45747308420609734
================================================================================

[I 2025-11-04 11:07:16,096] Trial 4139 pruned. Pruned at step 15 with metric 0.6252

================================================================================
TRIAL 4142 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.9920880711798695e-05
  Dropout: 0.39819981293959894
================================================================================

[I 2025-11-04 11:10:24,875] Trial 4141 pruned. Pruned at step 12 with metric 0.6200
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4143 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 7.077113679133747e-05
  Dropout: 0.2717346312384488
================================================================================

[I 2025-11-04 11:18:57,315] Trial 4143 pruned. Pruned at step 27 with metric 0.5945
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4144 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.896659710985036e-06
  Dropout: 0.08935398143372456
================================================================================

[I 2025-11-04 11:20:38,234] Trial 4142 pruned. Pruned at step 13 with metric 0.6512
[W 2025-11-04 11:20:38,759] The parameter `tok.doc_stride` in Trial#4145 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4145 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.9851153228204783e-05
  Dropout: 0.26659239853156663
================================================================================

[I 2025-11-04 11:23:23,427] Trial 4144 pruned. Pruned at step 16 with metric 0.6494
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4146 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.4911031134815167e-05
  Dropout: 0.2086409546906992
================================================================================

[I 2025-11-04 11:28:11,086] Trial 4146 pruned. Pruned at step 15 with metric 0.5688

================================================================================
TRIAL 4147 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.367331587462166e-05
  Dropout: 0.3513828184284701
================================================================================

[I 2025-11-04 11:31:50,602] Trial 4145 pruned. Pruned at step 15 with metric 0.5629
[W 2025-11-04 11:31:51,122] The parameter `tok.doc_stride` in Trial#4148 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4148 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6209714824633388e-05
  Dropout: 0.4800347618880006
================================================================================

[I 2025-11-04 11:40:53,804] Trial 4148 pruned. Pruned at step 18 with metric 0.5866
[I 2025-11-04 11:40:54,355] Trial 4149 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-04 11:40:54,897] Trial 4150 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4151 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.38387050194783e-05
  Dropout: 0.1366037908983164
================================================================================

[I 2025-11-04 11:43:26,182] Trial 4147 finished with value: 0.667516218721038 and parameters: {'seed': 54782, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.367331587462166e-05, 'optim.weight_decay': 3.1720349803426898e-06, 'optim.beta1': 0.8796229023961312, 'optim.beta2': 0.9784771014245337, 'optim.eps': 3.4984876802764793e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1885513990101859, 'sched.poly_power': 0.7161072590376008, 'train.clip_grad': 1.0716307224672583, 'model.dropout': 0.3513828184284701, 'model.attn_dropout': 0.06691598776202373, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9608980368863577, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.4565736715536897, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.36362259889099, 'loss.cls.alpha': 0.26274568641905255, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 11:43:26,702] The parameter `tok.doc_stride` in Trial#4152 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 37 (patience=20)

================================================================================
TRIAL 4152 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.1355597465605034e-05
  Dropout: 0.33131993988263825
================================================================================

[I 2025-11-04 11:44:03,202] Trial 4151 pruned. Pruned at step 9 with metric 0.5819
[I 2025-11-04 11:44:03,770] Trial 4153 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-04 11:44:04,282] Trial 4154 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4155 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 3.741974749443079e-05
  Dropout: 0.40888258749649936
================================================================================

[I 2025-11-04 11:53:24,885] Trial 4155 pruned. Pruned at step 27 with metric 0.6242

================================================================================
TRIAL 4156 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.0371839930556418e-05
  Dropout: 0.30495061221770986
================================================================================

[I 2025-11-04 11:59:06,044] Trial 4156 pruned. Pruned at step 16 with metric 0.6472
[I 2025-11-04 11:59:06,599] Trial 4157 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4158 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 8.231506568770935e-06
  Dropout: 0.13990225643199602
================================================================================

[I 2025-11-04 11:59:49,222] Trial 4152 pruned. Pruned at step 13 with metric 0.6076
[I 2025-11-04 11:59:49,807] Trial 4159 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-04 11:59:50,347] Trial 4160 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4161 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 0.00010454345780436654
  Dropout: 0.382869860145982
================================================================================

[I 2025-11-04 12:04:01,868] Trial 4161 pruned. Pruned at step 16 with metric 0.6338
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4162 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 6.809306647781545e-06
  Dropout: 0.36769306756613923
================================================================================

[I 2025-11-04 12:10:21,362] Trial 4162 pruned. Pruned at step 19 with metric 0.6002
[I 2025-11-04 12:10:21,916] Trial 4163 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4164 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.1883434922420367e-05
  Dropout: 0.4530046423318992
================================================================================

[I 2025-11-04 12:19:44,781] Trial 4164 pruned. Pruned at step 9 with metric 0.5880
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4165 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 3.4145292137829565e-05
  Dropout: 0.3686478900003511
================================================================================

[I 2025-11-04 12:23:33,709] Trial 4165 pruned. Pruned at step 8 with metric 0.5629
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4166 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 9.531040428055944e-06
  Dropout: 0.40475249105900174
================================================================================

[I 2025-11-04 12:31:46,830] Trial 4158 pruned. Pruned at step 14 with metric 0.5474
[W 2025-11-04 12:31:47,467] The parameter `tok.doc_stride` in Trial#4167 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 12:31:47,525] Trial 4167 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-04 12:31:48,073] Trial 4168 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4169 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 3.825782239785918e-05
  Dropout: 0.09141155420787866
================================================================================

[I 2025-11-04 12:34:35,318] Trial 4166 finished with value: 0.6822143373867512 and parameters: {'seed': 27611, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.531040428055944e-06, 'optim.weight_decay': 7.114717789058154e-05, 'optim.beta1': 0.8690570083021243, 'optim.beta2': 0.974344911984629, 'optim.eps': 6.936246582767466e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.19094679665301387, 'train.clip_grad': 0.6569321905522141, 'model.dropout': 0.40475249105900174, 'model.attn_dropout': 0.1350756984590218, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9662103387754756, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.45792177033331416, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.5605433322307793, 'loss.cls.alpha': 0.6587809573321818, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 12:34:35,890] Trial 4170 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[W 2025-11-04 12:34:36,376] The parameter `tok.doc_stride` in Trial#4171 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 4171 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.4246733268663378e-05
  Dropout: 0.03728453564355634
================================================================================

[I 2025-11-04 12:36:56,103] Trial 4169 pruned. Pruned at step 11 with metric 0.5593
[W 2025-11-04 12:36:56,627] The parameter `tok.doc_stride` in Trial#4172 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4172 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.0159505308907651e-05
  Dropout: 0.34513635253879965
================================================================================

[I 2025-11-04 12:47:14,707] Trial 4171 pruned. Pruned at step 27 with metric 0.6435
[W 2025-11-04 12:47:15,237] The parameter `tok.doc_stride` in Trial#4173 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 12:47:15,288] Trial 4173 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-04 12:47:15,815] Trial 4174 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[W 2025-11-04 12:47:16,312] The parameter `tok.doc_stride` in Trial#4175 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4175 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.922140112535885e-05
  Dropout: 0.04489150891020767
================================================================================

[I 2025-11-04 12:59:27,917] Trial 4175 pruned. Pruned at step 28 with metric 0.4337
[I 2025-11-04 12:59:28,472] Trial 4176 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4177 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.1330369604634872e-05
  Dropout: 0.07590196885201439
================================================================================

[I 2025-11-04 13:12:18,182] Trial 4172 finished with value: 0.7054272134409196 and parameters: {'seed': 52185, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.0159505308907651e-05, 'optim.weight_decay': 0.0016126906755265114, 'optim.beta1': 0.887293749589031, 'optim.beta2': 0.9528332302900391, 'optim.eps': 1.2158842929333785e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0010788075158880486, 'sched.poly_power': 1.0243471913612894, 'train.clip_grad': 1.2297571048791167, 'model.dropout': 0.34513635253879965, 'model.attn_dropout': 0.07694341524853987, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9156804994600439, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.42903756397702075, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.8219975539749225, 'loss.cls.alpha': 0.345393557282397, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 4178 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.2679662566968356e-05
  Dropout: 0.46853122246557755
================================================================================

[I 2025-11-04 13:12:32,049] Trial 4178 pruned. OOM: roberta-base bs=64 len=352

[OOM] Trial 4178 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 138.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4179 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.6842487620801548e-05
  Dropout: 0.4247953725522757
================================================================================

[I 2025-11-04 13:21:31,633] Trial 4177 pruned. Pruned at step 20 with metric 0.5957
[W 2025-11-04 13:21:32,168] The parameter `tok.doc_stride` in Trial#4180 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4180 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.0586828730322206e-05
  Dropout: 0.3641517185243154
================================================================================

[I 2025-11-04 13:26:33,600] Trial 4180 finished with value: 0.7359674269807637 and parameters: {'seed': 9572, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.0586828730322206e-05, 'optim.weight_decay': 0.0034717340172369846, 'optim.beta1': 0.8645781307205738, 'optim.beta2': 0.9535592365014914, 'optim.eps': 6.817039380063008e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.009717283259326186, 'sched.poly_power': 0.6441788403099071, 'train.clip_grad': 1.2250961819792023, 'model.dropout': 0.3641517185243154, 'model.attn_dropout': 0.28026842374282396, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9673533152613795, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.2809795770536325, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.489869497163282, 'loss.cls.alpha': 0.10826246934830823, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 13:26:34,146] Trial 4181 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 4182 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 9.427520311408489e-06
  Dropout: 0.38580729690491744
================================================================================

[I 2025-11-04 13:36:42,980] Trial 4179 pruned. Pruned at step 10 with metric 0.6052
[I 2025-11-04 13:36:43,541] Trial 4183 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-04 13:36:44,036] The parameter `tok.doc_stride` in Trial#4184 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4184 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.7762570427180636e-05
  Dropout: 0.4388912950488516
================================================================================

[I 2025-11-04 13:40:18,019] Trial 4184 pruned. Pruned at step 9 with metric 0.5910
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4185 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.427957571138441e-05
  Dropout: 0.44617220103242994
================================================================================

[I 2025-11-04 14:06:04,885] Trial 4182 finished with value: 0.7341956294867336 and parameters: {'seed': 65114, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.427520311408489e-06, 'optim.weight_decay': 0.0006676327837722388, 'optim.beta1': 0.8394478304020256, 'optim.beta2': 0.9634814713287625, 'optim.eps': 5.802859568938609e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.024138263616540593, 'train.clip_grad': 1.2562749904581516, 'model.dropout': 0.38580729690491744, 'model.attn_dropout': 0.21436738078122025, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9944331286174041, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.22068571995939257, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.227467535509614, 'loss.cls.alpha': 0.2943522480356613, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 14:06:05,435] Trial 4186 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 4187 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.694921226132751e-05
  Dropout: 0.3679171090553781
================================================================================

[I 2025-11-04 14:08:33,295] Trial 4187 pruned. Pruned at step 9 with metric 0.6390

================================================================================
TRIAL 4188 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.7001230296633436e-05
  Dropout: 0.45586034848399637
================================================================================

[I 2025-11-04 14:16:37,350] Trial 4185 finished with value: 0.43213296398891965 and parameters: {'seed': 46572, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.427957571138441e-05, 'optim.weight_decay': 0.10751372395551097, 'optim.beta1': 0.8433133355547904, 'optim.beta2': 0.9835379207434123, 'optim.eps': 3.1472160584224864e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06775545355272625, 'sched.poly_power': 0.9964736546838926, 'train.clip_grad': 1.437477811266262, 'model.dropout': 0.44617220103242994, 'model.attn_dropout': 0.17716425657620938, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9848229003544153, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.3054126091685711, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5188293284329575, 'loss.cls.alpha': 0.8113402702367662, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 14:16:37,901] Trial 4189 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4190 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4563769689891686e-05
  Dropout: 0.3336386435766699
================================================================================

[I 2025-11-04 14:16:55,039] Trial 4188 pruned. Pruned at step 13 with metric 0.5905
[W 2025-11-04 14:16:55,577] The parameter `tok.doc_stride` in Trial#4191 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4191 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.8479593528664802e-05
  Dropout: 0.48273888921088376
================================================================================

[I 2025-11-04 14:25:10,859] Trial 4190 pruned. Pruned at step 15 with metric 0.5486
[W 2025-11-04 14:25:11,386] The parameter `tok.doc_stride` in Trial#4192 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4192 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.749499697120272e-05
  Dropout: 0.185671453647648
================================================================================

[I 2025-11-04 14:25:36,350] Trial 4191 pruned. Pruned at step 30 with metric 0.6077
[I 2025-11-04 14:25:36,917] Trial 4193 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4194 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.559397756782048e-06
  Dropout: 0.16975252382044428
================================================================================

[I 2025-11-04 14:39:15,224] Trial 4192 pruned. Pruned at step 27 with metric 0.5629

================================================================================
TRIAL 4195 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 8.28000552060093e-06
  Dropout: 0.33333812493987214
================================================================================

[I 2025-11-04 14:46:40,254] Trial 4194 pruned. Pruned at step 27 with metric 0.5510
[W 2025-11-04 14:46:40,783] The parameter `tok.doc_stride` in Trial#4196 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4196 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.165206732006266e-05
  Dropout: 0.4091544570447043
================================================================================

[I 2025-11-04 14:50:32,334] Trial 4195 pruned. Pruned at step 10 with metric 0.5521
[I 2025-11-04 14:50:32,900] Trial 4197 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-04 14:50:33,427] Trial 4198 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-04 14:50:33,915] The parameter `tok.doc_stride` in Trial#4199 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 14:50:33,970] Trial 4199 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-04 14:50:34,487] The parameter `tok.doc_stride` in Trial#4200 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4200 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.8950202856981094e-05
  Dropout: 0.2694803331939424
================================================================================

[I 2025-11-04 14:58:37,534] Trial 4200 finished with value: 0.6036585365853658 and parameters: {'seed': 52751, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.8950202856981094e-05, 'optim.weight_decay': 0.00010394174193554367, 'optim.beta1': 0.8482327223781195, 'optim.beta2': 0.9503275746591586, 'optim.eps': 3.507887804487467e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.08429622466788786, 'sched.poly_power': 0.6569045972776386, 'train.clip_grad': 1.4928359531947315, 'model.dropout': 0.2694803331939424, 'model.attn_dropout': 0.18279808663244396, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9547992133613692, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.39076219738571344, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.95665340976221, 'loss.cls.alpha': 0.19994186376767278, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 14:58:38,103] Trial 4201 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[W 2025-11-04 14:58:38,595] The parameter `tok.doc_stride` in Trial#4202 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4202 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.820179342458655e-05
  Dropout: 0.2653070426379866
================================================================================

[I 2025-11-04 15:03:04,585] Trial 4202 pruned. Pruned at step 21 with metric 0.5801
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4203 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.358642671661297e-06
  Dropout: 0.10915021649717589
================================================================================

[I 2025-11-04 15:11:16,028] Trial 4203 finished with value: 0.6095238095238096 and parameters: {'seed': 10020, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 6.358642671661297e-06, 'optim.weight_decay': 7.718984354753327e-06, 'optim.beta1': 0.8693134900149205, 'optim.beta2': 0.9828820000548852, 'optim.eps': 5.976830281120405e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.14865609694896137, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9112121430291433, 'model.dropout': 0.10915021649717589, 'model.attn_dropout': 0.15857021197632315, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8284273607490544, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.03848014445439027, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.427294516204536, 'loss.cls.alpha': 0.5635813702333834, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 15:11:16,574] Trial 4204 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-04 15:11:17,100] Trial 4205 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 4206 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 9.830493509337988e-06
  Dropout: 0.42089335515723814
================================================================================

[I 2025-11-04 15:22:19,674] Trial 4206 pruned. Pruned at step 27 with metric 0.5945
[W 2025-11-04 15:22:20,185] The parameter `tok.doc_stride` in Trial#4207 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4207 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.2971213150665596e-05
  Dropout: 0.33301594101241666
================================================================================

[I 2025-11-04 15:25:21,498] Trial 4207 pruned. Pruned at step 9 with metric 0.6556
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4208 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 6.8402743893971675e-06
  Dropout: 0.04117148378444764
================================================================================

[I 2025-11-04 15:25:25,689] Trial 4208 pruned. OOM: roberta-base bs=64 len=352
[I 2025-11-04 15:25:26,273] Trial 4209 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-04 15:25:26,805] Trial 4210 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-04 15:25:27,301] The parameter `tok.doc_stride` in Trial#4211 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 4208 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 64 with grad_accum=1)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 192.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4211 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.23314059417669e-05
  Dropout: 0.48589269781054084
================================================================================

[I 2025-11-04 15:27:17,118] Trial 4211 pruned. Pruned at step 8 with metric 0.6093

================================================================================
TRIAL 4212 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.326444272249525e-06
  Dropout: 0.3734105213314483
================================================================================

[I 2025-11-04 15:40:25,306] Trial 4196 finished with value: 0.7049242424242425 and parameters: {'seed': 53960, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.165206732006266e-05, 'optim.weight_decay': 3.0057696410352197e-05, 'optim.beta1': 0.8350340461249067, 'optim.beta2': 0.9653038536574576, 'optim.eps': 2.7094472446215055e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.03402919500134208, 'sched.poly_power': 0.818923598375235, 'train.clip_grad': 1.0281533362922275, 'model.dropout': 0.4091544570447043, 'model.attn_dropout': 0.049782850735571926, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9517417848324046, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.34960992865613, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5255909712608995, 'loss.cls.alpha': 0.3224515526956172, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4213 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.006976270566124e-05
  Dropout: 0.4449303068110939
================================================================================

[I 2025-11-04 15:43:03,463] Trial 4213 pruned. Pruned at step 10 with metric 0.5886
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4214 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.2685009495198284e-05
  Dropout: 0.464953282873073
================================================================================

[I 2025-11-04 15:56:42,088] Trial 4212 finished with value: 0.7149011227570299 and parameters: {'seed': 58661, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.326444272249525e-06, 'optim.weight_decay': 0.0003998968106808023, 'optim.beta1': 0.8971054106614512, 'optim.beta2': 0.9596240108920906, 'optim.eps': 2.7477387535326405e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.016717477280939082, 'sched.poly_power': 0.6454914757445422, 'train.clip_grad': 1.3727567180067468, 'model.dropout': 0.3734105213314483, 'model.attn_dropout': 0.17295326557776966, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.855109130420325, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.31356383026435375, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.515684628724677, 'loss.cls.alpha': 0.512480571954554, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 15:56:42,614] The parameter `tok.doc_stride` in Trial#4215 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 15:56:42,668] Trial 4215 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 4216 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.4278571983423717e-05
  Dropout: 0.4580805528492231
================================================================================

[I 2025-11-04 16:01:07,546] Trial 4214 pruned. Pruned at step 14 with metric 0.5703
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4217 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 9.277897319185e-06
  Dropout: 0.4661605856115303
================================================================================

[I 2025-11-04 16:06:11,383] Trial 4216 pruned. Pruned at step 18 with metric 0.5008

================================================================================
TRIAL 4218 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.3795587537134846e-05
  Dropout: 0.008428670489325968
================================================================================

[I 2025-11-04 16:11:02,536] Trial 4218 pruned. Pruned at step 18 with metric 0.6435
[W 2025-11-04 16:11:03,059] The parameter `tok.doc_stride` in Trial#4219 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4219 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.704426404332984e-06
  Dropout: 0.2528486582973103
================================================================================

[I 2025-11-04 16:22:30,889] Trial 4219 pruned. Pruned at step 11 with metric 0.5703

================================================================================
TRIAL 4220 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.00496657039391e-05
  Dropout: 0.3415211936090118
================================================================================

[I 2025-11-04 16:30:00,646] Trial 4220 pruned. Pruned at step 17 with metric 0.5240
[I 2025-11-04 16:30:01,201] Trial 4221 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-04 16:30:01,734] Trial 4222 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4223 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.7219661355601118e-05
  Dropout: 0.32622070699455347
================================================================================

[I 2025-11-04 16:35:12,835] Trial 4217 finished with value: 0.6434782608695653 and parameters: {'seed': 61028, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.277897319185e-06, 'optim.weight_decay': 0.010998070845210036, 'optim.beta1': 0.873485815610931, 'optim.beta2': 0.965149491325016, 'optim.eps': 1.0753700102354912e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0520440477315126, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.486053025434637, 'model.dropout': 0.4661605856115303, 'model.attn_dropout': 0.10584586875066448, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.95528564341039, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.31146028842143103, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.955496840686981, 'loss.cls.alpha': 0.218232839658557, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4224 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.081754814529679e-05
  Dropout: 0.4135437798577542
================================================================================

[I 2025-11-04 16:41:30,257] Trial 4224 pruned. Pruned at step 27 with metric 0.6254
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4225 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.2025073778026243e-05
  Dropout: 0.4277163054344987
================================================================================

[I 2025-11-04 16:47:12,638] Trial 4225 finished with value: 0.7307591279222485 and parameters: {'seed': 34783, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.2025073778026243e-05, 'optim.weight_decay': 0.000643771160816684, 'optim.beta1': 0.8795929887091868, 'optim.beta2': 0.9812584987061234, 'optim.eps': 2.442175526471878e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.05874448869266902, 'train.clip_grad': 0.9714937991174186, 'model.dropout': 0.4277163054344987, 'model.attn_dropout': 0.016477186226039255, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8803792797226357, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.1127859856545517, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.08017058855153572, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 150 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 4226 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.29643579663883e-05
  Dropout: 0.3286508393778066
================================================================================

[I 2025-11-04 16:50:33,127] Trial 4226 pruned. Pruned at step 9 with metric 0.5416

================================================================================
[GPU RESET] Performing periodic GPU reset after 150 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 4227 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.40145510921836e-05
  Dropout: 0.10260139536961699
================================================================================

[I 2025-11-04 16:54:17,232] Trial 4227 pruned. Pruned at step 9 with metric 0.5997
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
[GPU RESET] Performing periodic GPU reset after 150 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 4228 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 9.688772014933006e-06
  Dropout: 0.3950327999684711
================================================================================

[I 2025-11-04 16:58:54,552] Trial 4223 finished with value: 0.6608099590293098 and parameters: {'seed': 24489, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.7219661355601118e-05, 'optim.weight_decay': 5.146732177913174e-05, 'optim.beta1': 0.906480940155536, 'optim.beta2': 0.9700650124111737, 'optim.eps': 1.0343124864915836e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.19817794453858276, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.433014873541468, 'model.dropout': 0.32622070699455347, 'model.attn_dropout': 0.047670493305314345, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9217241405812098, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.38095197064891606, 'loss.cls.type': 'focal', 'loss.cls.gamma': 2.661487813301715, 'loss.cls.alpha': 0.6207573862540858, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-04 16:58:55,075] The parameter `tok.doc_stride` in Trial#4229 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4229 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.071749060847204e-05
  Dropout: 0.4987594975343853
================================================================================

[I 2025-11-04 17:33:44,431] Trial 4229 finished with value: 0.7415694925937599 and parameters: {'seed': 19986, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.071749060847204e-05, 'optim.weight_decay': 0.021172886547627702, 'optim.beta1': 0.9031209204292303, 'optim.beta2': 0.9510637575031522, 'optim.eps': 5.543781658557542e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.0975131922471461, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1523015168134585, 'model.dropout': 0.4987594975343853, 'model.attn_dropout': 0.21945198709218544, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8270633026009483, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.457952707880607, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.456454181160292, 'loss.cls.alpha': 0.10442368824982899, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 4230 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.6125408908224736e-05
  Dropout: 0.3855919944055836
================================================================================

[I 2025-11-04 17:45:37,286] Trial 4230 pruned. Pruned at step 11 with metric 0.5827
[I 2025-11-04 17:45:37,837] Trial 4231 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4232 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 9.607870024791729e-06
  Dropout: 0.4250617617943707
================================================================================

[I 2025-11-04 17:51:17,486] Trial 4232 pruned. Pruned at step 27 with metric 0.6388
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4233 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 8.327147686352536e-06
  Dropout: 0.38222633199624134
================================================================================

[I 2025-11-04 18:00:58,006] Trial 4228 pruned. Pruned at step 27 with metric 0.6254
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4234 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 3.627461557959012e-05
  Dropout: 0.2235933930419121
================================================================================

[I 2025-11-04 18:14:53,785] Trial 4234 finished with value: 0.4444444444444444 and parameters: {'seed': 12801, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.627461557959012e-05, 'optim.weight_decay': 0.0013252406706557068, 'optim.beta1': 0.8212128853450986, 'optim.beta2': 0.978017468411918, 'optim.eps': 1.629882838449772e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1606243414148812, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.1346279929547332, 'model.dropout': 0.2235933930419121, 'model.attn_dropout': 0.023127019064234235, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9163207308813187, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.48862646815448235, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.870170171681317, 'loss.cls.alpha': 0.17518655922388185, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 18:14:54,337] Trial 4235 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-04 18:14:54,866] Trial 4236 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-04 18:14:55,404] Trial 4237 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-04 18:14:55,931] Trial 4238 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4239 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 6.449568068045452e-05
  Dropout: 0.43696116606696134
================================================================================

[I 2025-11-04 18:21:12,397] Trial 4239 finished with value: 0.42896935933147634 and parameters: {'seed': 22087, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.449568068045452e-05, 'optim.weight_decay': 0.045308551011125894, 'optim.beta1': 0.8412363440209848, 'optim.beta2': 0.9614511422114095, 'optim.eps': 5.113883397352498e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.008474901353944846, 'sched.poly_power': 0.9608352392838895, 'train.clip_grad': 1.4432020490540372, 'model.dropout': 0.43696116606696134, 'model.attn_dropout': 0.22964668280969908, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9699468308753205, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.168952915891566, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.097716118351925, 'loss.cls.alpha': 0.1566331431191189, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4240 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 9.335467485893396e-06
  Dropout: 0.40709746029877314
================================================================================

[I 2025-11-04 18:37:25,692] Trial 4240 pruned. Pruned at step 18 with metric 0.6402
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4241 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.5934616962998356e-05
  Dropout: 0.4709103884684672
================================================================================

[I 2025-11-04 18:43:25,296] Trial 4241 pruned. Pruned at step 8 with metric 0.6294

================================================================================
TRIAL 4242 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.12401560966676e-05
  Dropout: 0.39345882347324856
================================================================================

[I 2025-11-04 18:59:17,363] Trial 4242 finished with value: 0.7254175858808698 and parameters: {'seed': 39550, 'model.name': 'bert-base-uncased', 'tok.max_length': 288, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.12401560966676e-05, 'optim.weight_decay': 0.00010762322810196407, 'optim.beta1': 0.8605008316030123, 'optim.beta2': 0.9886909098797183, 'optim.eps': 1.5159930100226122e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1497147471810262, 'sched.poly_power': 0.7706178041796631, 'train.clip_grad': 0.8996120591833113, 'model.dropout': 0.39345882347324856, 'model.attn_dropout': 0.02866517828313027, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9083944799281329, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4725406271472179, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.542909826930951, 'loss.cls.alpha': 0.14127917590841238, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 49 (patience=20)

================================================================================
TRIAL 4243 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.3092156917359502e-05
  Dropout: 0.44279904072500315
================================================================================

[I 2025-11-04 19:23:09,833] Trial 4243 pruned. Pruned at step 27 with metric 0.6290
[W 2025-11-04 19:23:10,355] The parameter `tok.doc_stride` in Trial#4244 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4244 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.271906577830693e-05
  Dropout: 0.4625016403071301
================================================================================

[I 2025-11-04 19:37:03,992] Trial 4244 finished with value: 0.4368131868131868 and parameters: {'seed': 15564, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.271906577830693e-05, 'optim.weight_decay': 0.1500245180772994, 'optim.beta1': 0.9017407104200057, 'optim.beta2': 0.9660763803021231, 'optim.eps': 1.9429201199250722e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.040884847343511985, 'train.clip_grad': 1.1969914220172724, 'model.dropout': 0.4625016403071301, 'model.attn_dropout': 0.25872332528848574, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8345613589144663, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.31358942833544406, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.741743018042426, 'loss.cls.alpha': 0.1048633733673163, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4245 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 4.289816216522204e-05
  Dropout: 0.23472669216786846
================================================================================

[I 2025-11-04 19:38:28,625] Trial 4233 pruned. Pruned at step 16 with metric 0.6009
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4246 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.0867661303859952e-05
  Dropout: 0.3960540421469515
================================================================================

[I 2025-11-04 19:42:28,317] Trial 4245 pruned. Pruned at step 27 with metric 0.6027
[W 2025-11-04 19:42:28,849] The parameter `tok.doc_stride` in Trial#4247 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-04 19:42:28,901] Trial 4247 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4248 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.3616322343259101e-05
  Dropout: 0.35367590488897965
================================================================================

[I 2025-11-04 19:45:23,568] Trial 4246 pruned. Pruned at step 8 with metric 0.5666

================================================================================
TRIAL 4249 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.205680565693726e-06
  Dropout: 0.20368671495809523
================================================================================

[I 2025-11-04 19:51:14,632] Trial 4248 pruned. Pruned at step 14 with metric 0.6600

================================================================================
TRIAL 4250 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.696744332032674e-05
  Dropout: 0.07173652262876107
================================================================================

[I 2025-11-04 19:56:21,733] Trial 4250 finished with value: 0.7005550686532283 and parameters: {'seed': 52483, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.696744332032674e-05, 'optim.weight_decay': 0.00023722711403106165, 'optim.beta1': 0.880250384018038, 'optim.beta2': 0.9793851260928613, 'optim.eps': 1.0519672542330351e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1255267208089841, 'sched.poly_power': 0.7509303169071118, 'train.clip_grad': 0.5223925518485526, 'model.dropout': 0.07173652262876107, 'model.attn_dropout': 0.1432017350160438, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9503981407201334, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.1557161856717914, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.044245560072119085, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4251 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 5.741812545509288e-06
  Dropout: 0.4947018933312248
================================================================================

[I 2025-11-04 20:05:01,442] Trial 4251 pruned. Pruned at step 27 with metric 0.5579
[I 2025-11-04 20:05:01,999] Trial 4252 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4253 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.108355382321891e-05
  Dropout: 0.17267176584650626
================================================================================

[I 2025-11-04 20:12:09,615] Trial 4253 pruned. Pruned at step 27 with metric 0.6537

================================================================================
TRIAL 4254 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.2169401855273304e-05
  Dropout: 0.3508748728001975
================================================================================

[I 2025-11-04 20:27:50,650] Trial 4254 finished with value: 0.6115953012504737 and parameters: {'seed': 51936, 'model.name': 'bert-base-uncased', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.2169401855273304e-05, 'optim.weight_decay': 0.05800515044742668, 'optim.beta1': 0.8459209088542323, 'optim.beta2': 0.9958438228067604, 'optim.eps': 3.517643886753408e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.14782775363142184, 'train.clip_grad': 0.35600436419538395, 'model.dropout': 0.3508748728001975, 'model.attn_dropout': 0.14452022675713821, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8969041016862336, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.16304610994273758, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.018824352771472777, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 20:27:51,210] Trial 4255 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4256 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 5.345350760294492e-06
  Dropout: 0.2966432146252142
================================================================================

[I 2025-11-04 20:27:58,611] Trial 4249 pruned. Pruned at step 27 with metric 0.6273
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4257 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 0.00010717717958634781
  Dropout: 0.09871071042762188
================================================================================

[I 2025-11-04 20:31:24,301] Trial 4256 pruned. Pruned at step 12 with metric 0.6629

================================================================================
TRIAL 4258 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 8.10511897091384e-06
  Dropout: 0.39350161786232873
================================================================================

[I 2025-11-04 20:41:28,330] Trial 4258 pruned. Pruned at step 13 with metric 0.5923

================================================================================
TRIAL 4259 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 8.709094354566834e-06
  Dropout: 0.1996130472792494
================================================================================

[I 2025-11-04 20:48:22,740] Trial 4259 pruned. Pruned at step 27 with metric 0.6294
[I 2025-11-04 20:48:23,301] Trial 4260 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4261 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 5.609671075332277e-06
  Dropout: 0.17792189421785198
================================================================================

[I 2025-11-04 21:00:01,443] Trial 4261 pruned. Pruned at step 27 with metric 0.6156
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4262 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 5.031115268392601e-06
  Dropout: 0.02015625104429061
================================================================================

[I 2025-11-04 21:04:35,908] Trial 4262 pruned. Pruned at step 23 with metric 0.6125
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4263 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.0353860006284615e-05
  Dropout: 0.08578864935909161
================================================================================

[I 2025-11-04 21:35:34,488] Trial 4257 finished with value: 0.4429347826086957 and parameters: {'seed': 202, 'model.name': 'roberta-large', 'tok.max_length': 320, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 0.00010717717958634781, 'optim.weight_decay': 6.588781109771574e-05, 'optim.beta1': 0.9452302853121706, 'optim.beta2': 0.98654941179101, 'optim.eps': 1.0621224396618616e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.07200671299864214, 'sched.poly_power': 0.8423628126347014, 'train.clip_grad': 1.0915091980919218, 'model.dropout': 0.09871071042762188, 'model.attn_dropout': 0.16501481611890528, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.9134542688525474, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.26296719676653796, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.052020740295072775, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4264 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.3600894102454696e-05
  Dropout: 0.24771462342774078
================================================================================

[I 2025-11-04 21:52:45,200] Trial 4264 finished with value: 0.7073609731876862 and parameters: {'seed': 1086, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.3600894102454696e-05, 'optim.weight_decay': 0.07234156868291067, 'optim.beta1': 0.946890707660936, 'optim.beta2': 0.989381425399449, 'optim.eps': 1.0609080962100717e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.15061688380120294, 'train.clip_grad': 1.2300352597260584, 'model.dropout': 0.24771462342774078, 'model.attn_dropout': 0.24018442004308393, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9067851086869793, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.4645105055649489, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7473518759670865, 'loss.cls.alpha': 0.18420021608938375, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-04 21:52:45,756] Trial 4265 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4266 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 9.386817766877977e-06
  Dropout: 0.40064597695251636
================================================================================

[I 2025-11-04 21:54:47,663] Trial 4263 finished with value: 0.7437499999999999 and parameters: {'seed': 10184, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.0353860006284615e-05, 'optim.weight_decay': 2.236584024907851e-05, 'optim.beta1': 0.9458887637260721, 'optim.beta2': 0.966718812262927, 'optim.eps': 2.486582226189263e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.038952499213550804, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4714951332501751, 'model.dropout': 0.08578864935909161, 'model.attn_dropout': 0.23622755393444947, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8103030320405169, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3610313745006175, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.659854865252844, 'loss.cls.alpha': 0.10499970069489903, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 46 (patience=20)

================================================================================
TRIAL 4267 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.764688712484091e-05
  Dropout: 0.10257594668150287
================================================================================

[I 2025-11-04 22:05:41,909] Trial 4266 pruned. Pruned at step 10 with metric 0.6341
[I 2025-11-04 22:05:42,729] Trial 4268 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4269 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.1083242295846881e-05
  Dropout: 0.026742593336687187
================================================================================

[I 2025-11-04 22:17:54,170] Trial 4267 finished with value: 0.43989071038251365 and parameters: {'seed': 11311, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.764688712484091e-05, 'optim.weight_decay': 0.0003408125358859396, 'optim.beta1': 0.929749896911091, 'optim.beta2': 0.9898510449518373, 'optim.eps': 3.4987098328508156e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.06382921116948695, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.14316563715235, 'model.dropout': 0.10257594668150287, 'model.attn_dropout': 0.15993831047305568, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8862295587101535, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.2707867282548864, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.384091328930656, 'loss.cls.alpha': 0.1467769249447035, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4270 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 3.896928850531553e-05
  Dropout: 0.049298455283439596
================================================================================

[I 2025-11-04 22:25:50,937] Trial 4269 pruned. Pruned at step 12 with metric 0.6027
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4271 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 8.854558659432996e-06
  Dropout: 0.4699391632839852
================================================================================

[I 2025-11-04 22:34:20,361] Trial 4270 finished with value: 0.4533333333333333 and parameters: {'seed': 23705, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.896928850531553e-05, 'optim.weight_decay': 0.004205574337551897, 'optim.beta1': 0.9324006133043302, 'optim.beta2': 0.9808390741263465, 'optim.eps': 1.8916799534298525e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.046125357276641056, 'sched.poly_power': 1.0275960384385594, 'train.clip_grad': 1.1986202853644015, 'model.dropout': 0.049298455283439596, 'model.attn_dropout': 0.24296929396153844, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8279360744001397, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.42326166544307914, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.03370528766538143, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4272 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 8.031905509589767e-06
  Dropout: 0.10645256190742441
================================================================================

[I 2025-11-04 22:42:07,660] Trial 4272 pruned. Pruned at step 27 with metric 0.6672
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4273 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.893466352202361e-05
  Dropout: 0.16821899089668335
================================================================================

[I 2025-11-04 23:15:02,542] Trial 4273 finished with value: 0.6402246402246403 and parameters: {'seed': 10903, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.893466352202361e-05, 'optim.weight_decay': 0.00027072882924273007, 'optim.beta1': 0.9111449028797493, 'optim.beta2': 0.971806694454945, 'optim.eps': 1.8759192631270373e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.10495449779399103, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9954149737166971, 'model.dropout': 0.16821899089668335, 'model.attn_dropout': 0.13945636973378794, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8133355268403437, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.21513651007116552, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.015363568272871, 'loss.cls.alpha': 0.13182244236890223, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 22 (patience=20)

================================================================================
TRIAL 4274 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.4621578486932986e-05
  Dropout: 0.11171947556418468
================================================================================

[I 2025-11-04 23:21:10,713] Trial 4274 pruned. Pruned at step 14 with metric 0.6428
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4275 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 2.1950376610971628e-05
  Dropout: 0.14743701779224985
================================================================================

[I 2025-11-04 23:28:07,578] Trial 4275 pruned. Pruned at step 9 with metric 0.6396
[I 2025-11-04 23:28:08,208] Trial 4276 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4277 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.2489487460487093e-05
  Dropout: 0.0001799048825069649
================================================================================

[I 2025-11-04 23:32:45,886] Trial 4271 pruned. Pruned at step 27 with metric 0.6167
[I 2025-11-04 23:32:46,558] Trial 4278 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-04 23:32:47,061] The parameter `tok.doc_stride` in Trial#4279 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4279 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 5.8015272119810446e-06
  Dropout: 0.36836436970400105
================================================================================

[I 2025-11-04 23:46:14,277] Trial 4279 pruned. Pruned at step 20 with metric 0.5968
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4280 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.5791067039439075e-05
  Dropout: 0.057767475577868245
================================================================================

[I 2025-11-04 23:46:41,945] Trial 4277 finished with value: 0.4444444444444444 and parameters: {'seed': 1283, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.2489487460487093e-05, 'optim.weight_decay': 0.00010209536570742043, 'optim.beta1': 0.9368236328407678, 'optim.beta2': 0.9724142466869877, 'optim.eps': 2.4239713721992792e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0281968011651519, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2522034136453026, 'model.dropout': 0.0001799048825069649, 'model.attn_dropout': 0.2791645464809934, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8009950884315794, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.4564596379812007, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.740751248147068, 'loss.cls.alpha': 0.10979714161159745, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4281 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 8.205823785495095e-06
  Dropout: 0.04452212179307
================================================================================

[I 2025-11-05 00:00:15,709] Trial 4281 pruned. Pruned at step 11 with metric 0.6093
[I 2025-11-05 00:00:16,300] Trial 4282 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4283 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 4.400269554417556e-05
  Dropout: 0.43937146010864325
================================================================================

[I 2025-11-05 00:16:53,014] Trial 4283 finished with value: 0.4533333333333333 and parameters: {'seed': 31566, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 4.400269554417556e-05, 'optim.weight_decay': 0.00034488246739874993, 'optim.beta1': 0.8415852826140425, 'optim.beta2': 0.9513268673388906, 'optim.eps': 3.37715049151192e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.022105343172674197, 'train.clip_grad': 1.47425738594915, 'model.dropout': 0.43937146010864325, 'model.attn_dropout': 0.19709425056910201, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9284486732517708, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.2551981891299887, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.3929242935076775, 'loss.cls.alpha': 0.10089413102730745, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4284 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 2.622417814611918e-05
  Dropout: 0.301907431750339
================================================================================

[I 2025-11-05 00:39:12,328] Trial 4280 finished with value: 0.4444444444444444 and parameters: {'seed': 3092, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.5791067039439075e-05, 'optim.weight_decay': 0.0013833390759627493, 'optim.beta1': 0.8784581584318168, 'optim.beta2': 0.9801593270146842, 'optim.eps': 4.95811027431932e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.17097814160904826, 'train.clip_grad': 0.4842653552388398, 'model.dropout': 0.057767475577868245, 'model.attn_dropout': 0.28580342989823354, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.82075981980552, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.04247359344152746, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.440786295239693, 'loss.cls.alpha': 0.17228124620091, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4285 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.2919678876477196e-05
  Dropout: 0.4987191644896274
================================================================================

[I 2025-11-05 00:47:05,392] Trial 4284 finished with value: 0.4533333333333333 and parameters: {'seed': 12171, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.622417814611918e-05, 'optim.weight_decay': 2.0257912309390498e-05, 'optim.beta1': 0.9184811402839593, 'optim.beta2': 0.9870346132302823, 'optim.eps': 9.888596939806972e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.15166103442295797, 'sched.poly_power': 1.0474138344060582, 'train.clip_grad': 0.7830508635378124, 'model.dropout': 0.301907431750339, 'model.attn_dropout': 0.16150907640906617, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.903933661066175, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.4817552624525948, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.6889644436177558, 'loss.cls.alpha': 0.15265956964657532, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4286 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.7130207766917505e-05
  Dropout: 0.100307782951483
================================================================================

[I 2025-11-05 00:52:48,676] Trial 4285 pruned. Pruned at step 7 with metric 0.4993
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4287 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 3.0285569232043912e-05
  Dropout: 0.49533768825733804
================================================================================

[I 2025-11-05 00:57:49,160] Trial 4286 pruned. Pruned at step 8 with metric 0.6069
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4288 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.603371629750316e-06
  Dropout: 0.4867846824050452
================================================================================

[I 2025-11-05 01:09:44,873] Trial 4288 pruned. Pruned at step 10 with metric 0.6077
[I 2025-11-05 01:09:45,423] Trial 4289 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4290 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.0884781938736293e-05
  Dropout: 0.41539442706964286
================================================================================

[I 2025-11-05 01:09:48,904] Trial 4290 pruned. OOM: roberta-base bs=64 len=384

[OOM] Trial 4290 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 384 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 176.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4291 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.0862444796620248e-05
  Dropout: 0.49965992200056986
================================================================================

[I 2025-11-05 01:40:30,435] Trial 4287 finished with value: 0.4383561643835616 and parameters: {'seed': 3147, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.0285569232043912e-05, 'optim.weight_decay': 0.0005103550532322336, 'optim.beta1': 0.915392066460556, 'optim.beta2': 0.9559992290616978, 'optim.eps': 3.969930176820165e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1285454734538546, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1680118430662607, 'model.dropout': 0.49533768825733804, 'model.attn_dropout': 0.2630771064966502, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8316896246980107, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.4743938358302415, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.074228346713673, 'loss.cls.alpha': 0.21810105263969765, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4292 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.9853595977288853e-05
  Dropout: 0.49308686531942625
================================================================================

[I 2025-11-05 02:32:17,268] Trial 4292 pruned. Pruned at step 15 with metric 0.6250
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4293 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 2.778745634776853e-05
  Dropout: 0.4311279998077855
================================================================================

[I 2025-11-05 03:03:14,748] Trial 4291 finished with value: 0.7155273152205668 and parameters: {'seed': 36681, 'model.name': 'bert-large-uncased', 'tok.max_length': 320, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.0862444796620248e-05, 'optim.weight_decay': 0.0011492747651087297, 'optim.beta1': 0.8492925496357835, 'optim.beta2': 0.9595733485855799, 'optim.eps': 1.7387566251557933e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.014434086466827187, 'sched.poly_power': 0.9810387487894455, 'train.clip_grad': 0.9300842469799528, 'model.dropout': 0.49965992200056986, 'model.attn_dropout': 0.10546555559550036, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9452917235905295, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.11486196076439817, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.9633103105948218, 'loss.cls.alpha': 0.13603019340946265, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 67 (patience=20)

================================================================================
TRIAL 4294 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 3.0030507505098092e-05
  Dropout: 0.4392242098015511
================================================================================

[I 2025-11-05 03:27:40,248] Trial 4293 pruned. Pruned at step 30 with metric 0.5905
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4295 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.4627019620248067e-05
  Dropout: 0.35439096818260807
================================================================================

[I 2025-11-05 03:37:59,529] Trial 4295 pruned. Pruned at step 13 with metric 0.6315
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4296 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.8472510732314464e-06
  Dropout: 0.4439449870989171
================================================================================

[I 2025-11-05 03:57:44,888] Trial 4294 finished with value: 0.7184065934065934 and parameters: {'seed': 20682, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.0030507505098092e-05, 'optim.weight_decay': 0.0002602718616064893, 'optim.beta1': 0.872352723781242, 'optim.beta2': 0.9608943580494301, 'optim.eps': 9.953904834296265e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.11431296812200469, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.264827538099568, 'model.dropout': 0.4392242098015511, 'model.attn_dropout': 0.18059110929268196, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8590180305353191, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.4321701208229651, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.515711865120191, 'loss.cls.alpha': 0.100375132467943, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-05 03:57:45,425] The parameter `tok.doc_stride` in Trial#4297 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 43 (patience=20)

================================================================================
TRIAL 4297 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.219247880000319e-06
  Dropout: 0.33039114006270665
================================================================================

[I 2025-11-05 04:12:53,909] Trial 4297 finished with value: 0.6798780487804879 and parameters: {'seed': 51366, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.219247880000319e-06, 'optim.weight_decay': 0.01846654251369684, 'optim.beta1': 0.8335379309007669, 'optim.beta2': 0.9774482599252297, 'optim.eps': 2.19692447815521e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.11062778577288154, 'sched.poly_power': 0.9369691590245421, 'train.clip_grad': 0.8617154649310355, 'model.dropout': 0.33039114006270665, 'model.attn_dropout': 0.1219458392988096, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9160093350373218, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.20027081568074778, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.230728627488197, 'loss.cls.alpha': 0.10112733879645949, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 53 (patience=20)

================================================================================
TRIAL 4298 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 7.6180112576689504e-06
  Dropout: 0.10004412480617526
================================================================================

[I 2025-11-05 04:16:09,520] Trial 4296 finished with value: 0.6041859746679024 and parameters: {'seed': 21800, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.8472510732314464e-06, 'optim.weight_decay': 0.022802506300161184, 'optim.beta1': 0.8908250680696784, 'optim.beta2': 0.9553824477435561, 'optim.eps': 7.253785312706993e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06540207523651906, 'sched.poly_power': 0.9652593121004146, 'train.clip_grad': 0.800796166977167, 'model.dropout': 0.4439449870989171, 'model.attn_dropout': 0.09118841652791324, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.84356969442281, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.44828848311901187, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.717182364754312, 'loss.cls.alpha': 0.18869173515846657, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4299 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.2286971413161645e-06
  Dropout: 0.35243279151510737
================================================================================

[I 2025-11-05 04:25:49,080] Trial 4298 finished with value: 0.6738636363636363 and parameters: {'seed': 17907, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.6180112576689504e-06, 'optim.weight_decay': 0.0001193244603121441, 'optim.beta1': 0.9340918776093453, 'optim.beta2': 0.9759987820559773, 'optim.eps': 2.717021237772029e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.04871511986921859, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2989373795903718, 'model.dropout': 0.10004412480617526, 'model.attn_dropout': 0.17622613466957637, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8012963730339668, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.2682318160006144, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.502668939003431, 'loss.cls.alpha': 0.18645761422427407, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 04:25:49,647] Trial 4300 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 4301 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.507994028277492e-05
  Dropout: 0.44747331728589057
================================================================================

[I 2025-11-05 04:35:56,017] Trial 4301 finished with value: 0.6411257743064906 and parameters: {'seed': 46016, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.507994028277492e-05, 'optim.weight_decay': 0.036139694300122364, 'optim.beta1': 0.9296242864486356, 'optim.beta2': 0.9568664141193407, 'optim.eps': 5.260013717672465e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0039845777647484655, 'sched.poly_power': 1.1110206619355965, 'train.clip_grad': 1.2235339498056717, 'model.dropout': 0.44747331728589057, 'model.attn_dropout': 0.1711776536165341, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9240820069555686, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4208849359447093, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.9867016812530487, 'loss.cls.alpha': 0.23023915974948292, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 4302 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.737378592509941e-06
  Dropout: 0.47797101309934353
================================================================================

[I 2025-11-05 04:37:40,047] Trial 4302 pruned. Pruned at step 9 with metric 0.4634
[I 2025-11-05 04:37:40,638] Trial 4303 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-05 04:37:41,176] Trial 4304 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4305 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.3442049136285584e-05
  Dropout: 0.4006542194215884
================================================================================

[I 2025-11-05 04:57:07,557] Trial 4305 finished with value: 0.4444444444444444 and parameters: {'seed': 14083, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.3442049136285584e-05, 'optim.weight_decay': 0.010710782899291078, 'optim.beta1': 0.9377498734839068, 'optim.beta2': 0.9607102038013766, 'optim.eps': 2.2082402145063478e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.07989565975433491, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9628197872587725, 'model.dropout': 0.4006542194215884, 'model.attn_dropout': 0.21650727044450901, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8965241780426045, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.3140772025378886, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.786513279537455, 'loss.cls.alpha': 0.19402506646765336, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4306 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.1189736987237655e-05
  Dropout: 0.3314264644903837
================================================================================

[I 2025-11-05 05:00:18,038] Trial 4306 pruned. Pruned at step 13 with metric 0.6527

================================================================================
TRIAL 4307 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 3.204768166298272e-05
  Dropout: 0.39900053416741094
================================================================================

[I 2025-11-05 05:21:51,060] Trial 4307 pruned. Pruned at step 18 with metric 0.6382

================================================================================
TRIAL 4308 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.149711982399795e-05
  Dropout: 0.2411837000299888
================================================================================

[I 2025-11-05 05:28:41,410] Trial 4299 finished with value: 0.6798780487804879 and parameters: {'seed': 4632, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.2286971413161645e-06, 'optim.weight_decay': 0.0024915099126819698, 'optim.beta1': 0.8742171119255032, 'optim.beta2': 0.9885272572484454, 'optim.eps': 4.2327512456570134e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1548415028234999, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.7645502714477749, 'model.dropout': 0.35243279151510737, 'model.attn_dropout': 0.13552809959356044, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9658494309390111, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.07429014044846433, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.692713650616425, 'loss.cls.alpha': 0.10002086432483868, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 4309 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 1.6998069427238432e-05
  Dropout: 0.485733322327406
================================================================================

[I 2025-11-05 05:43:23,930] Trial 4309 pruned. Pruned at step 7 with metric 0.6053
[I 2025-11-05 05:43:24,546] Trial 4310 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4311 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.9184658350388024e-05
  Dropout: 0.3889944142610683
================================================================================

[I 2025-11-05 05:54:23,929] Trial 4308 finished with value: 0.45910290237467016 and parameters: {'seed': 60933, 'model.name': 'bert-large-uncased', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.149711982399795e-05, 'optim.weight_decay': 0.0005091185449279476, 'optim.beta1': 0.8525894114891848, 'optim.beta2': 0.9579631602383964, 'optim.eps': 7.330927817584542e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.05826810278455238, 'train.clip_grad': 1.2087305210593926, 'model.dropout': 0.2411837000299888, 'model.attn_dropout': 0.1665085142413929, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9598033725201272, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3575885368064618, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.054509303473376236, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4312 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.8227651166887704e-05
  Dropout: 0.19221768547108992
================================================================================

[I 2025-11-05 06:09:15,264] Trial 4311 pruned. Pruned at step 17 with metric 0.5847

================================================================================
TRIAL 4313 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.6293326646583166e-05
  Dropout: 0.43544795614151643
================================================================================

[I 2025-11-05 06:16:11,120] Trial 4312 finished with value: 0.6453648112343543 and parameters: {'seed': 47377, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.8227651166887704e-05, 'optim.weight_decay': 6.622654884066087e-05, 'optim.beta1': 0.9470068178468959, 'optim.beta2': 0.9668088756662204, 'optim.eps': 1.2173861923330978e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03475852558017619, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1124701254678773, 'model.dropout': 0.19221768547108992, 'model.attn_dropout': 0.2522271540721658, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8364533511604174, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.20949966446466034, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.901539443860112, 'loss.cls.alpha': 0.19262422631780463, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 06:16:11,681] Trial 4314 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-05 06:16:12,218] Trial 4315 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-05 06:16:12,761] Trial 4316 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4317 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.2134116087591923e-05
  Dropout: 0.15363707699038803
================================================================================

[I 2025-11-05 06:34:55,873] Trial 4313 pruned. Pruned at step 32 with metric 0.6002

================================================================================
TRIAL 4318 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 6.16705738242146e-06
  Dropout: 0.4350528447389225
================================================================================

[I 2025-11-05 06:38:09,523] Trial 4317 pruned. Pruned at step 17 with metric 0.6540
[I 2025-11-05 06:38:10,140] Trial 4319 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4320 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.9429432276118027e-05
  Dropout: 0.03569393214625194
================================================================================

[I 2025-11-05 06:49:11,937] Trial 4320 pruned. Pruned at step 24 with metric 0.6079
[I 2025-11-05 06:49:12,505] Trial 4321 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4322 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.5248153719131032e-05
  Dropout: 0.33172961461893485
================================================================================

[I 2025-11-05 07:04:43,655] Trial 4318 finished with value: 0.6750132108450876 and parameters: {'seed': 63965, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.16705738242146e-06, 'optim.weight_decay': 0.0021141796590301645, 'optim.beta1': 0.8755727436606824, 'optim.beta2': 0.9510178320758124, 'optim.eps': 3.7120147069296213e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.10983395791295476, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.6264156275588056, 'model.dropout': 0.4350528447389225, 'model.attn_dropout': 0.2897877807397009, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8341811386575797, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.2226450642049393, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.03725567822009947, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 4323 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.895452711722337e-05
  Dropout: 0.008550960440769612
================================================================================

[I 2025-11-05 07:13:11,822] Trial 4322 finished with value: 0.43213296398891965 and parameters: {'seed': 5016, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.5248153719131032e-05, 'optim.weight_decay': 0.06207115855674686, 'optim.beta1': 0.9071901416738036, 'optim.beta2': 0.9700902387979998, 'optim.eps': 1.4233456718958914e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06209418990584563, 'sched.poly_power': 0.8437585763844732, 'train.clip_grad': 0.8242673690392237, 'model.dropout': 0.33172961461893485, 'model.attn_dropout': 0.2613224515390091, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8927986991646699, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4812799580695711, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.586418709939597, 'loss.cls.alpha': 0.10549606572486565, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 07:13:12,427] Trial 4324 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-05 07:13:13,001] Trial 4325 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4326 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.9535717433941416e-05
  Dropout: 0.013966955796961292
================================================================================

[I 2025-11-05 07:17:51,185] Trial 4326 pruned. Pruned at step 10 with metric 0.5824
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4327 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.155756516130253e-05
  Dropout: 0.45886747586511906
================================================================================

[I 2025-11-05 07:27:10,586] Trial 4323 finished with value: 0.7488536525604677 and parameters: {'seed': 65242, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.895452711722337e-05, 'optim.weight_decay': 1.9080197066840557e-06, 'optim.beta1': 0.9336791163867995, 'optim.beta2': 0.971423902414286, 'optim.eps': 6.900133717481952e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.020637185702321034, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4446398435967074, 'model.dropout': 0.008550960440769612, 'model.attn_dropout': 0.20858766762071954, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8071978698218024, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.29858506810391394, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.319722395948691, 'loss.cls.alpha': 0.24812054655096316, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 07:27:11,155] Trial 4328 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-05 07:27:11,694] Trial 4329 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 46 (patience=20)

================================================================================
TRIAL 4330 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 9.954374966636794e-06
  Dropout: 0.1289834607729104
================================================================================

[I 2025-11-05 07:48:04,266] Trial 4330 finished with value: 0.7049242424242425 and parameters: {'seed': 58097, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 9.954374966636794e-06, 'optim.weight_decay': 1.1329928253299836e-05, 'optim.beta1': 0.9313388661584013, 'optim.beta2': 0.9846477455735085, 'optim.eps': 5.0179131406675575e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03753314698960791, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4069126663527172, 'model.dropout': 0.1289834607729104, 'model.attn_dropout': 0.11634290832979872, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8199500861811587, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.26726073383001314, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.012484394024858, 'loss.cls.alpha': 0.2331009808717725, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 45 (patience=20)

================================================================================
TRIAL 4331 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.109189864249232e-05
  Dropout: 0.1882470408694
================================================================================

[I 2025-11-05 07:54:31,088] Trial 4327 finished with value: 0.7143710191082803 and parameters: {'seed': 29529, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.155756516130253e-05, 'optim.weight_decay': 0.12220477814138564, 'optim.beta1': 0.8417212512463003, 'optim.beta2': 0.954157529305891, 'optim.eps': 3.043874129933337e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0032917088035021172, 'sched.poly_power': 0.8962170101717523, 'train.clip_grad': 1.180613358340092, 'model.dropout': 0.45886747586511906, 'model.attn_dropout': 0.1646842482571034, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9928030871103974, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3152883860368805, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.979054993257221, 'loss.cls.alpha': 0.19659920670484435, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 07:54:31,672] Trial 4332 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 74 (patience=20)

================================================================================
TRIAL 4333 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.0856271744085512e-05
  Dropout: 0.08529643430166993
================================================================================

[I 2025-11-05 08:02:23,669] Trial 4331 pruned. Pruned at step 9 with metric 0.6042
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4334 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 5.6396358665375286e-05
  Dropout: 0.16690940830611756
================================================================================

[I 2025-11-05 08:04:08,529] Trial 4333 pruned. Pruned at step 7 with metric 0.6164
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4335 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 4.579481575772142e-05
  Dropout: 0.08704109494305923
================================================================================

[I 2025-11-05 08:12:37,445] Trial 4335 pruned. Pruned at step 27 with metric 0.6356
[I 2025-11-05 08:12:38,016] Trial 4336 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-05 08:12:38,562] Trial 4337 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-05 08:12:39,119] Trial 4338 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4339 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.8975642320023213e-05
  Dropout: 0.15375102407542285
================================================================================

[I 2025-11-05 08:14:03,928] Trial 4334 pruned. Pruned at step 27 with metric 0.6076
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4340 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.386258188989387e-05
  Dropout: 0.469887561707022
================================================================================

[I 2025-11-05 08:20:14,070] Trial 4340 pruned. Pruned at step 10 with metric 0.5688
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4341 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.53780876063001e-05
  Dropout: 0.4752667736208962
================================================================================

[I 2025-11-05 08:28:52,270] Trial 4341 pruned. OOM: microsoft/deberta-v3-large bs=8 len=320
[I 2025-11-05 08:28:52,995] Trial 4342 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4341 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 64 with grad_accum=8)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 38.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4343 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.1668800785292215e-05
  Dropout: 0.4838096189635221
================================================================================

[I 2025-11-05 08:32:34,592] Trial 4343 pruned. Pruned at step 8 with metric 0.6197
[I 2025-11-05 08:32:35,152] Trial 4344 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-05 08:32:35,708] Trial 4345 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-05 08:32:36,216] The parameter `tok.doc_stride` in Trial#4346 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4346 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3161499313302672e-05
  Dropout: 0.133378573752794
================================================================================

[I 2025-11-05 08:37:26,230] Trial 4346 pruned. Pruned at step 11 with metric 0.5750
[I 2025-11-05 08:37:26,811] Trial 4347 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4348 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.6202045306866855e-05
  Dropout: 0.4777402281223697
================================================================================

[I 2025-11-05 09:01:41,841] Trial 4348 pruned. Pruned at step 10 with metric 0.6290
[W 2025-11-05 09:01:42,412] The parameter `tok.doc_stride` in Trial#4349 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4349 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2205176796196895e-05
  Dropout: 0.1399679035316525
================================================================================

[I 2025-11-05 09:13:32,391] Trial 4349 pruned. Pruned at step 11 with metric 0.5941
[I 2025-11-05 09:13:32,950] Trial 4350 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4351 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.4820430083225967e-05
  Dropout: 0.024368045919630366
================================================================================

[I 2025-11-05 09:43:17,858] Trial 4351 finished with value: 0.43989071038251365 and parameters: {'seed': 20621, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.4820430083225967e-05, 'optim.weight_decay': 2.1690671119088413e-06, 'optim.beta1': 0.9200017043376775, 'optim.beta2': 0.97675287441255, 'optim.eps': 9.074040263065885e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.05511258535486968, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.265629965586416, 'model.dropout': 0.024368045919630366, 'model.attn_dropout': 0.24145277504997228, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8015267609109986, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.15593896711507715, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.3926245249273537, 'loss.cls.alpha': 0.2337132798216149, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-05 09:43:18,397] The parameter `tok.doc_stride` in Trial#4352 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4352 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.246407328645636e-05
  Dropout: 0.38131214352556486
================================================================================

[I 2025-11-05 09:51:05,964] Trial 4352 pruned. Pruned at step 7 with metric 0.5539
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4353 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 8.76322682833243e-06
  Dropout: 0.10221432623842018
================================================================================

[I 2025-11-05 09:55:02,207] Trial 4339 finished with value: 0.7521547521547521 and parameters: {'seed': 63851, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.8975642320023213e-05, 'optim.weight_decay': 1.2040242236623321e-05, 'optim.beta1': 0.9255744793814957, 'optim.beta2': 0.9810802409735001, 'optim.eps': 4.15670353830357e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0378497501032263, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.3426783268654043, 'model.dropout': 0.15375102407542285, 'model.attn_dropout': 0.17373089063230665, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8256827731032867, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.33733666302328413, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.35976178678916, 'loss.cls.alpha': 0.3092875718163168, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 68 (patience=20)

================================================================================
TRIAL 4354 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.562454814354792e-06
  Dropout: 0.19052468440567438
================================================================================

[I 2025-11-05 10:04:25,075] Trial 4354 pruned. Pruned at step 27 with metric 0.6042

================================================================================
TRIAL 4355 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.6490534659766658e-05
  Dropout: 0.08650591835576031
================================================================================

[I 2025-11-05 10:04:50,420] Trial 4353 pruned. Pruned at step 10 with metric 0.5962
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4356 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 9.740967553693088e-06
  Dropout: 0.1757497366379001
================================================================================

[I 2025-11-05 10:13:22,924] Trial 4355 finished with value: 0.7090322580645161 and parameters: {'seed': 45312, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.6490534659766658e-05, 'optim.weight_decay': 1.4221427070594716e-06, 'optim.beta1': 0.9043038463245721, 'optim.beta2': 0.9786697559099005, 'optim.eps': 9.715811072072637e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0767528440558205, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.3170267390770944, 'model.dropout': 0.08650591835576031, 'model.attn_dropout': 0.16970424264153827, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8249261264755696, 'head.pooling': 'mean', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.3139678807967841, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.304672944767029, 'loss.cls.alpha': 0.2657036420180896, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 4357 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.931964911187449e-06
  Dropout: 0.20935488887295872
================================================================================

[I 2025-11-05 10:45:42,535] Trial 4357 finished with value: 0.6440972222222222 and parameters: {'seed': 60271, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.931964911187449e-06, 'optim.weight_decay': 0.0009010085485096644, 'optim.beta1': 0.9300265564640179, 'optim.beta2': 0.9758720855898947, 'optim.eps': 4.767697205729542e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.06629813892634401, 'train.clip_grad': 1.1045410369603115, 'model.dropout': 0.20935488887295872, 'model.attn_dropout': 0.23037371709606655, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8025197270598053, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.2365178658446146, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.388335203309756, 'loss.cls.alpha': 0.2703064993361995, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 4358 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 4.8834711152173716e-05
  Dropout: 0.046631235557979586
================================================================================

[I 2025-11-05 10:49:23,504] Trial 4358 pruned. Pruned at step 12 with metric 0.6038
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4359 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.7430572093071366e-05
  Dropout: 0.10819318616487911
================================================================================

[I 2025-11-05 10:57:11,541] Trial 4359 finished with value: 0.6762848119437266 and parameters: {'seed': 61205, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.7430572093071366e-05, 'optim.weight_decay': 0.0001140700831907977, 'optim.beta1': 0.9063728224687686, 'optim.beta2': 0.9872802728909358, 'optim.eps': 3.84998811093679e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.06145484940501851, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4005110222053148, 'model.dropout': 0.10819318616487911, 'model.attn_dropout': 0.27624657943476516, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.843675856911263, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.39596788015358814, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.300733507497839, 'loss.cls.alpha': 0.31227860287964204, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4360 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 3.865636152189252e-05
  Dropout: 0.3425667668054543
================================================================================

[I 2025-11-05 10:57:23,639] Trial 4356 finished with value: 0.6450020264544416 and parameters: {'seed': 57644, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.740967553693088e-06, 'optim.weight_decay': 0.0003228620979853905, 'optim.beta1': 0.9319054998317556, 'optim.beta2': 0.9753638165506384, 'optim.eps': 3.4326717243371996e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.08724278467426512, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2360121338741599, 'model.dropout': 0.1757497366379001, 'model.attn_dropout': 0.2589612357293788, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8295767468049214, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.3035788562082755, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.29026650762737, 'loss.cls.alpha': 0.26945636267355527, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4361 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.1311327493464832e-05
  Dropout: 0.04698664823411374
================================================================================

[I 2025-11-05 11:19:14,289] Trial 4360 finished with value: 0.4489247311827957 and parameters: {'seed': 15960, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 3.865636152189252e-05, 'optim.weight_decay': 0.00032610378796327336, 'optim.beta1': 0.8950598885463653, 'optim.beta2': 0.9721628397148391, 'optim.eps': 1.192313703747747e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1716215430719967, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9954562644203114, 'model.dropout': 0.3425667668054543, 'model.attn_dropout': 0.06430207661905986, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9251612872276175, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.4486265970115866, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.02965046127034191, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 11:19:14,857] Trial 4362 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4363 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.3008863894401788e-05
  Dropout: 0.45308538111090835
================================================================================

[I 2025-11-05 11:36:28,826] Trial 4361 pruned. Pruned at step 9 with metric 0.5338
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4364 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.258438316520123e-05
  Dropout: 0.021183126534392076
================================================================================

[I 2025-11-05 11:42:31,833] Trial 4363 pruned. Pruned at step 17 with metric 0.6093

================================================================================
TRIAL 4365 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.656028709384991e-06
  Dropout: 0.19434007588750216
================================================================================

[I 2025-11-05 11:46:44,518] Trial 4365 pruned. Pruned at step 21 with metric 0.5945
[W 2025-11-05 11:46:45,052] The parameter `tok.doc_stride` in Trial#4366 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-05 11:46:45,108] Trial 4366 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-05 11:46:45,609] The parameter `tok.doc_stride` in Trial#4367 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4367 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.5148384432934748e-05
  Dropout: 0.02479261909384959
================================================================================

[I 2025-11-05 12:11:30,405] Trial 4367 finished with value: 0.7356398069341586 and parameters: {'seed': 57090, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.5148384432934748e-05, 'optim.weight_decay': 2.792092932636461e-06, 'optim.beta1': 0.9305063348129465, 'optim.beta2': 0.9754055647064861, 'optim.eps': 6.0741374592089284e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.02454499535143257, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.390762243348593, 'model.dropout': 0.02479261909384959, 'model.attn_dropout': 0.2148300591503232, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8035834637918198, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.26369737763408, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.994029476559544, 'loss.cls.alpha': 0.2975760946838646, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 49 (patience=20)

================================================================================
TRIAL 4368 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.301212787441955e-06
  Dropout: 0.47935547418894375
================================================================================

[I 2025-11-05 12:27:04,762] Trial 4368 pruned. Pruned at step 27 with metric 0.6298
[W 2025-11-05 12:27:05,286] The parameter `tok.doc_stride` in Trial#4369 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4369 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.2157791711168149e-05
  Dropout: 0.13223322636010287
================================================================================

[I 2025-11-05 12:29:50,556] Trial 4364 pruned. Pruned at step 27 with metric 0.6359
[I 2025-11-05 12:29:51,226] Trial 4370 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4371 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 9.260173446972935e-06
  Dropout: 0.3541536033668879
================================================================================

[I 2025-11-05 12:31:01,114] Trial 4369 pruned. Pruned at step 9 with metric 0.6250
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4372 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 3.531932284341615e-05
  Dropout: 0.07293126869225693
================================================================================

[I 2025-11-05 12:31:09,849] Trial 4372 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352
[I 2025-11-05 12:31:10,104] Trial 4371 pruned. OOM: xlm-roberta-base bs=12 len=288
[W 2025-11-05 12:31:10,811] The parameter `tok.doc_stride` in Trial#4374 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-05 12:31:11,068] Trial 4374 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4371 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 42.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4372 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 24 with grad_accum=3)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 82.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4373 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.3972171811545595e-05
  Dropout: 0.14231655432552487
================================================================================


================================================================================
TRIAL 4375 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 9.398748197063127e-06
  Dropout: 0.06255451584833985
================================================================================

[I 2025-11-05 12:45:22,974] Trial 4375 pruned. Pruned at step 15 with metric 0.6482
[I 2025-11-05 12:45:23,805] Trial 4376 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4377 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.3015029474976001e-05
  Dropout: 0.43584276703395913
================================================================================

[I 2025-11-05 12:45:31,771] Trial 4377 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-05 12:45:34,259] Trial 4373 pruned. OOM: roberta-large bs=12 len=128

[OOM] Trial 4377 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 54.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4378 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 4.582821753480939e-05
  Dropout: 0.46322616837856656
================================================================================


[OOM] Trial 4373 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 34.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

[I 2025-11-05 12:45:35,280] Trial 4379 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4380 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.439496470875768e-06
  Dropout: 0.07999841393341693
================================================================================

[I 2025-11-05 12:56:33,474] Trial 4378 finished with value: 0.4444444444444444 and parameters: {'seed': 65503, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 4.582821753480939e-05, 'optim.weight_decay': 2.3058045367522076e-06, 'optim.beta1': 0.8328024412703193, 'optim.beta2': 0.9855700037819498, 'optim.eps': 1.9277641385422557e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.18741331982880077, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.6415622763597193, 'model.dropout': 0.46322616837856656, 'model.attn_dropout': 0.12409929245753398, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9321144573910665, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.4473644342596744, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.604888865912838, 'loss.cls.alpha': 0.2639343119648496, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 12:56:34,061] Trial 4381 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4382 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 2.0289102527128045e-05
  Dropout: 0.1930840875388087
================================================================================

[I 2025-11-05 13:07:31,748] Trial 4382 pruned. Pruned at step 14 with metric 0.5866
[I 2025-11-05 13:07:32,314] Trial 4383 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4384 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.666110294816963e-05
  Dropout: 0.055248398783481506
================================================================================

[I 2025-11-05 13:19:57,396] Trial 4384 pruned. Pruned at step 9 with metric 0.6002
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4385 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.0272627178979224e-05
  Dropout: 0.1183336581059688
================================================================================

[I 2025-11-05 13:27:19,425] Trial 4385 pruned. Pruned at step 13 with metric 0.5349
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4386 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 9.311136714611524e-06
  Dropout: 0.06798731882485957
================================================================================

[I 2025-11-05 13:27:27,089] Trial 4380 pruned. OOM: microsoft/deberta-v3-base bs=8 len=320
[I 2025-11-05 13:27:27,553] Trial 4386 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384
[I 2025-11-05 13:27:28,390] Trial 4387 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-05 13:27:29,174] The parameter `tok.doc_stride` in Trial#4389 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 4380 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4386 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4388 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 4.672379070861813e-05
  Dropout: 0.051407058470256776
================================================================================


================================================================================
TRIAL 4389 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.4971720084677844e-05
  Dropout: 0.35616084828282674
================================================================================

[I 2025-11-05 13:30:05,192] Trial 4389 pruned. Pruned at step 12 with metric 0.5341
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4390 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.4880329757110034e-05
  Dropout: 0.029428776472119052
================================================================================

[I 2025-11-05 13:30:13,034] Trial 4388 pruned. OOM: microsoft/deberta-v3-base bs=8 len=320
[W 2025-11-05 13:30:13,702] The parameter `tok.doc_stride` in Trial#4391 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-05 13:30:13,755] Trial 4391 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[I 2025-11-05 13:30:14,425] Trial 4390 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384
[I 2025-11-05 13:30:15,108] Trial 4392 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

[OOM] Trial 4388 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 48.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4390 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 16 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 48.38 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4393 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.3763880599900686e-05
  Dropout: 0.3426452433608859
================================================================================


================================================================================
TRIAL 4394 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.1406293261392503e-05
  Dropout: 0.12333599684720228
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-05 13:32:10,546] Trial 4394 pruned. Pruned at step 9 with metric 0.6014
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4395 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.1982144336252556e-05
  Dropout: 0.039104685027553474
================================================================================

[I 2025-11-05 13:36:25,302] Trial 4393 pruned. Pruned at step 8 with metric 0.5750
[I 2025-11-05 13:36:25,885] Trial 4396 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[W 2025-11-05 13:36:26,390] The parameter `tok.doc_stride` in Trial#4397 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4397 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.275785163875304e-06
  Dropout: 0.4892470204604471
================================================================================

[I 2025-11-05 13:41:38,621] Trial 4397 pruned. Pruned at step 9 with metric 0.5962
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4398 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.0920819535640663e-05
  Dropout: 0.027471619397799376
================================================================================

[I 2025-11-05 13:48:53,023] Trial 4398 pruned. Pruned at step 13 with metric 0.5758
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4399 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.5213041330205019e-05
  Dropout: 0.3884380738739006
================================================================================

[I 2025-11-05 13:55:07,662] Trial 4399 pruned. Pruned at step 9 with metric 0.6199
[I 2025-11-05 13:55:08,237] Trial 4400 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-05 13:55:08,778] Trial 4401 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4402 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.8010674351313547e-05
  Dropout: 0.07175565388208976
================================================================================

[I 2025-11-05 14:06:38,164] Trial 4395 finished with value: 0.739553924336533 and parameters: {'seed': 22009, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.1982144336252556e-05, 'optim.weight_decay': 3.310980049238079e-06, 'optim.beta1': 0.9272095434682139, 'optim.beta2': 0.9833137395635171, 'optim.eps': 1.6365012086501724e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.08780750822251208, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2526921248074578, 'model.dropout': 0.039104685027553474, 'model.attn_dropout': 0.20091543166527048, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8249547928970531, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.34324991643912534, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.127244108282919, 'loss.cls.alpha': 0.1729397935509541, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 39 (patience=20)

================================================================================
TRIAL 4403 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.4499226936990117e-05
  Dropout: 0.21590933927086461
================================================================================

[I 2025-11-05 14:10:02,501] Trial 4402 pruned. Pruned at step 8 with metric 0.5886
[W 2025-11-05 14:10:03,133] The parameter `tok.doc_stride` in Trial#4404 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-05 14:10:03,189] Trial 4404 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4405 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 8.601208898251392e-06
  Dropout: 0.4114161877635519
================================================================================

[I 2025-11-05 14:20:47,197] Trial 4405 pruned. Pruned at step 9 with metric 0.5957
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4406 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 8.900416292854287e-06
  Dropout: 0.04976023468942106
================================================================================

[I 2025-11-05 14:21:52,511] Trial 4403 pruned. Pruned at step 11 with metric 0.6388

================================================================================
TRIAL 4407 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 3.42137403206043e-05
  Dropout: 0.1510959117824779
================================================================================

[I 2025-11-05 14:56:03,305] Trial 4407 finished with value: 0.45478723404255317 and parameters: {'seed': 51293, 'model.name': 'bert-large-uncased', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.42137403206043e-05, 'optim.weight_decay': 0.1277087687599219, 'optim.beta1': 0.8560255377155962, 'optim.beta2': 0.9808724089165688, 'optim.eps': 8.549399552247762e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.149987722351748, 'train.clip_grad': 0.49454515889438105, 'model.dropout': 0.1510959117824779, 'model.attn_dropout': 0.18075197388198894, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8109667439253019, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.001484984966294782, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.18817862102338956, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4408 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.275244156104828e-05
  Dropout: 0.37719472234231377
================================================================================

[I 2025-11-05 14:58:36,086] Trial 4408 pruned. Pruned at step 8 with metric 0.6085
[I 2025-11-05 14:58:36,655] Trial 4409 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-05 14:58:37,227] Trial 4410 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4411 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.7936907862039175e-05
  Dropout: 0.24197169083439962
================================================================================

[I 2025-11-05 15:01:40,121] Trial 4411 pruned. Pruned at step 13 with metric 0.6245
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4412 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.905091135571303e-05
  Dropout: 0.04068305656620155
================================================================================

[I 2025-11-05 15:45:58,118] Trial 4412 pruned. Pruned at step 17 with metric 0.5545
[I 2025-11-05 15:45:58,752] Trial 4413 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4414 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.7322706838440678e-05
  Dropout: 0.42134944597529933
================================================================================

[I 2025-11-05 15:47:51,259] Trial 4406 pruned. Pruned at step 27 with metric 0.5874
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4415 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.675179745301196e-05
  Dropout: 0.11693737875536182
================================================================================

[I 2025-11-05 16:11:03,616] Trial 4414 finished with value: 0.4192634560906516 and parameters: {'seed': 24512, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.7322706838440678e-05, 'optim.weight_decay': 0.0029449002312507203, 'optim.beta1': 0.8338040772047917, 'optim.beta2': 0.963130489517176, 'optim.eps': 7.96466966226677e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.056391830752913735, 'train.clip_grad': 1.4802267044547746, 'model.dropout': 0.42134944597529933, 'model.attn_dropout': 0.24014239298275222, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9580897849905452, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.44424329853637967, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.748353006782409, 'loss.cls.alpha': 0.33207958831104006, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4416 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 8.343061989934026e-06
  Dropout: 0.42975210428162364
================================================================================

[I 2025-11-05 16:17:15,866] Trial 4416 pruned. Pruned at step 10 with metric 0.5769
[I 2025-11-05 16:17:16,624] Trial 4417 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4418 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.4811475894445864e-05
  Dropout: 0.39651676552204124
================================================================================

[I 2025-11-05 16:23:32,541] Trial 4418 pruned. Pruned at step 6 with metric 0.5807
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4419 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 3.9372179430133446e-05
  Dropout: 0.03978375045520605
================================================================================

[I 2025-11-05 16:36:56,667] Trial 4415 finished with value: 0.6738636363636363 and parameters: {'seed': 23273, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.675179745301196e-05, 'optim.weight_decay': 0.000490689929779471, 'optim.beta1': 0.8984662104379604, 'optim.beta2': 0.9756099431468114, 'optim.eps': 2.024496041526305e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.031777897485815526, 'train.clip_grad': 1.4245631772468708, 'model.dropout': 0.11693737875536182, 'model.attn_dropout': 0.09129410910310629, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8566598998525778, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.23194499734888374, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.994559311993407, 'loss.cls.alpha': 0.10598403191678793, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 16:36:57,276] Trial 4420 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-05 16:36:57,805] The parameter `tok.doc_stride` in Trial#4421 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 33 (patience=20)

================================================================================
TRIAL 4421 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.65711574190986e-05
  Dropout: 0.03849684001644493
================================================================================

[I 2025-11-05 16:39:52,136] Trial 4421 pruned. Pruned at step 14 with metric 0.5667

================================================================================
TRIAL 4422 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.834841505620318e-05
  Dropout: 0.08571972305915958
================================================================================

[I 2025-11-05 16:40:50,261] Trial 4419 pruned. Pruned at step 12 with metric 0.6082
[W 2025-11-05 16:40:50,892] The parameter `tok.doc_stride` in Trial#4423 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4423 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 5.325309173077544e-05
  Dropout: 0.13447141619268105
================================================================================

[I 2025-11-05 16:44:09,251] Trial 4423 pruned. Pruned at step 11 with metric 0.6124
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4424 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.719948693096203e-05
  Dropout: 0.4115683707884974
================================================================================

[I 2025-11-05 16:45:27,272] Trial 4422 pruned. Pruned at step 8 with metric 0.5657
[W 2025-11-05 16:45:27,831] The parameter `tok.doc_stride` in Trial#4425 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4425 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 5.093455233774945e-06
  Dropout: 0.3387143371876993
================================================================================

[I 2025-11-05 16:48:22,429] Trial 4424 pruned. Pruned at step 10 with metric 0.6124

================================================================================
TRIAL 4426 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.676654301984376e-05
  Dropout: 0.21402997226102893
================================================================================

[I 2025-11-05 16:54:07,570] Trial 4425 pruned. Pruned at step 12 with metric 0.5652

================================================================================
TRIAL 4427 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.081976935909756e-06
  Dropout: 0.13157652009204107
================================================================================

[I 2025-11-05 16:54:27,746] Trial 4426 pruned. Pruned at step 10 with metric 0.5349
[W 2025-11-05 16:54:28,301] The parameter `tok.doc_stride` in Trial#4428 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4428 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.98492603759139e-05
  Dropout: 0.4492909886357544
================================================================================

[I 2025-11-05 17:00:59,097] Trial 4427 pruned. Pruned at step 11 with metric 0.5807
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4429 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.3687016146471396e-05
  Dropout: 0.1318408520794142
================================================================================

[I 2025-11-05 17:05:40,817] Trial 4428 pruned. Pruned at step 27 with metric 0.5769
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4430 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.183225836435084e-06
  Dropout: 0.3266077661747812
================================================================================

[I 2025-11-05 17:20:34,462] Trial 4430 pruned. Pruned at step 12 with metric 0.6279
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4431 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.0063744196268308e-05
  Dropout: 0.33884460486478163
================================================================================

[I 2025-11-05 17:41:35,453] Trial 4431 pruned. Pruned at step 27 with metric 0.5962

================================================================================
TRIAL 4432 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.072909847779009e-06
  Dropout: 0.2815395776177661
================================================================================

[I 2025-11-05 17:44:52,490] Trial 4432 pruned. Pruned at step 10 with metric 0.5827
[W 2025-11-05 17:44:53,034] The parameter `tok.doc_stride` in Trial#4433 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-05 17:44:53,089] Trial 4433 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4434 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 4.132706978713962e-05
  Dropout: 0.029664337755728912
================================================================================

[I 2025-11-05 18:00:34,299] Trial 4434 finished with value: 0.43989071038251365 and parameters: {'seed': 4217, 'model.name': 'bert-large-uncased', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 4.132706978713962e-05, 'optim.weight_decay': 0.00018271749004523417, 'optim.beta1': 0.9384187670033843, 'optim.beta2': 0.9833705165821095, 'optim.eps': 7.019628535766222e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.015458622686835476, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2728679121225455, 'model.dropout': 0.029664337755728912, 'model.attn_dropout': 0.2043421685030915, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8140013335794637, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.42312679393251457, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8715036332533534, 'loss.cls.alpha': 0.11059411349928928, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 18:00:34,874] Trial 4435 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-05 18:00:35,429] Trial 4436 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-05 18:00:35,977] Trial 4437 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4438 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.5017498158664938e-05
  Dropout: 0.3196990651026645
================================================================================

[I 2025-11-05 18:11:42,126] Trial 4438 finished with value: 0.7150139017608896 and parameters: {'seed': 3452, 'model.name': 'bert-base-uncased', 'tok.max_length': 288, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.5017498158664938e-05, 'optim.weight_decay': 0.11455407589356639, 'optim.beta1': 0.8221682061421421, 'optim.beta2': 0.9666190395067108, 'optim.eps': 1.364158655978385e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.01153216178173445, 'sched.poly_power': 0.7433645326088262, 'train.clip_grad': 1.4029186580032602, 'model.dropout': 0.3196990651026645, 'model.attn_dropout': 0.11608773498102494, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9971644626609, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.1796675509010066, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.355451260335143, 'loss.cls.alpha': 0.19437447767490368, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4439 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.4781297718384266e-05
  Dropout: 0.033257368468212034
================================================================================

[I 2025-11-05 18:16:23,177] Trial 4439 pruned. Pruned at step 13 with metric 0.5862

================================================================================
TRIAL 4440 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.1781378526707484e-05
  Dropout: 0.4707570906173722
================================================================================

[I 2025-11-05 18:20:29,765] Trial 4440 pruned. Pruned at step 14 with metric 0.5451
[I 2025-11-05 18:20:30,345] Trial 4441 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-05 18:20:30,904] Trial 4442 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4443 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 9.615051885407009e-06
  Dropout: 0.4045651124993026
================================================================================

[I 2025-11-05 18:44:37,974] Trial 4429 finished with value: 0.739553924336533 and parameters: {'seed': 58974, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.3687016146471396e-05, 'optim.weight_decay': 2.827629651981399e-05, 'optim.beta1': 0.9401080803162868, 'optim.beta2': 0.9575819878458203, 'optim.eps': 1.3272509073287681e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.03857257077706427, 'train.clip_grad': 1.1150685368924855, 'model.dropout': 0.1318408520794142, 'model.attn_dropout': 0.28141826673659215, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8086307030060387, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3106484854051302, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.143909441009795, 'loss.cls.alpha': 0.3254017885024791, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 43 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 200 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 4444 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.4483517685638272e-05
  Dropout: 0.46653528918989434
================================================================================

[I 2025-11-05 18:58:16,124] Trial 4443 finished with value: 0.7254175858808698 and parameters: {'seed': 51968, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 9.615051885407009e-06, 'optim.weight_decay': 0.09294355891645667, 'optim.beta1': 0.9058937405224572, 'optim.beta2': 0.9599775822055435, 'optim.eps': 3.218507848526671e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.16318853754612667, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.19357042944196, 'model.dropout': 0.4045651124993026, 'model.attn_dropout': 0.07450894377891706, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8679947086810472, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.3574153346756878, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.445007810600797, 'loss.cls.alpha': 0.10528350030225325, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 18:58:16,708] Trial 4445 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 4446 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.54212527570488e-06
  Dropout: 0.4596734592804398
================================================================================

[I 2025-11-05 19:22:18,630] Trial 4446 finished with value: 0.7304995617879053 and parameters: {'seed': 61262, 'model.name': 'bert-base-uncased', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.54212527570488e-06, 'optim.weight_decay': 0.12714723903494804, 'optim.beta1': 0.8935821588435526, 'optim.beta2': 0.9517587455101221, 'optim.eps': 5.5572526433215946e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.14011639096979672, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.1883925484626847, 'model.dropout': 0.4596734592804398, 'model.attn_dropout': 0.2657088383481859, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8280105569695412, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.4791377793676339, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.832553457929691, 'loss.cls.alpha': 0.2644950741296197, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 19:22:19,202] Trial 4447 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 56 (patience=20)

================================================================================
TRIAL 4448 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 4.282915198918582e-05
  Dropout: 0.479309301523653
================================================================================

[I 2025-11-05 19:48:39,097] Trial 4444 finished with value: 0.6496445173639596 and parameters: {'seed': 17431, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.4483517685638272e-05, 'optim.weight_decay': 0.008326477019920927, 'optim.beta1': 0.9418178835987782, 'optim.beta2': 0.9524395570543417, 'optim.eps': 4.1890605958895274e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.04122947514934057, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8122100974898329, 'model.dropout': 0.46653528918989434, 'model.attn_dropout': 0.22319385887901888, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8219673938407932, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4556083582786966, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.412015114908673, 'loss.cls.alpha': 0.17109702457695497, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-05 19:48:39,655] The parameter `tok.doc_stride` in Trial#4449 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 4449 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.620101937922575e-05
  Dropout: 0.11392635308711084
================================================================================

[I 2025-11-05 19:52:09,708] Trial 4449 pruned. Pruned at step 9 with metric 0.6314
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4450 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.761569274755283e-05
  Dropout: 0.1295801851227969
================================================================================

[I 2025-11-05 20:10:00,505] Trial 4448 finished with value: 0.4474393530997305 and parameters: {'seed': 33536, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 4.282915198918582e-05, 'optim.weight_decay': 0.0038545425529505828, 'optim.beta1': 0.9167309469727436, 'optim.beta2': 0.9907239416945398, 'optim.eps': 1.2030375416037455e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.1746474606700559, 'train.clip_grad': 1.0606470577299163, 'model.dropout': 0.479309301523653, 'model.attn_dropout': 0.2504545157781693, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.909001368634232, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.4241781017827112, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.951598182412813, 'loss.cls.alpha': 0.2293295359288427, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4451 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.9682555680290182e-05
  Dropout: 0.24594263776089353
================================================================================

[I 2025-11-05 20:13:57,696] Trial 4451 pruned. Pruned at step 12 with metric 0.6155
[I 2025-11-05 20:13:58,286] Trial 4452 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4453 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.2169887232054637e-05
  Dropout: 0.020446178119944772
================================================================================

[I 2025-11-05 20:18:39,730] Trial 4453 pruned. Pruned at step 17 with metric 0.6002
[I 2025-11-05 20:18:40,301] Trial 4454 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4455 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 2.7154830146854685e-05
  Dropout: 0.2082846468765304
================================================================================

[I 2025-11-05 20:30:04,742] Trial 4455 finished with value: 0.6928930968360498 and parameters: {'seed': 12882, 'model.name': 'xlm-roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.7154830146854685e-05, 'optim.weight_decay': 1.2158677045834084e-05, 'optim.beta1': 0.9002083794573266, 'optim.beta2': 0.9808322846447519, 'optim.eps': 1.2611825031377934e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.016994259540197187, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8376261321556775, 'model.dropout': 0.2082846468765304, 'model.attn_dropout': 0.22333762334310758, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8001646517145247, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.29256730151489074, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.14612195918156, 'loss.cls.alpha': 0.18209078068379017, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-05 20:30:05,307] Trial 4456 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-05 20:30:05,861] Trial 4457 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-05 20:30:06,419] The parameter `tok.doc_stride` in Trial#4458 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 31 (patience=20)

================================================================================
TRIAL 4458 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3037880035763406e-05
  Dropout: 0.11094519303628742
================================================================================

[I 2025-11-05 20:39:41,278] Trial 4458 finished with value: 0.7759562841530054 and parameters: {'seed': 61649, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.3037880035763406e-05, 'optim.weight_decay': 0.0001814837774332653, 'optim.beta1': 0.9092674143414208, 'optim.beta2': 0.9787911359981294, 'optim.eps': 3.1531839050896515e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0015803430765194415, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.0679903315148802, 'model.dropout': 0.11094519303628742, 'model.attn_dropout': 0.15574574801211324, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8121537689890708, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.44781061461723826, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.195321612635597, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 4459 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.133569000903485e-05
  Dropout: 0.15030401924425402
================================================================================

[I 2025-11-05 20:51:06,445] Trial 4450 finished with value: 0.6825343388095879 and parameters: {'seed': 53452, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.761569274755283e-05, 'optim.weight_decay': 1.1436708071869879e-06, 'optim.beta1': 0.9134053188000602, 'optim.beta2': 0.9792362159540886, 'optim.eps': 1.0046323057955715e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.04638925220905747, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.4529121811916157, 'model.dropout': 0.1295801851227969, 'model.attn_dropout': 0.16893312074432565, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8066754048568259, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.4283366859324216, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.176453496082802, 'loss.cls.alpha': 0.2681357919138856, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4460 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.2877996490210222e-05
  Dropout: 0.1904074684290475
================================================================================

[I 2025-11-05 20:56:38,409] Trial 4460 pruned. Pruned at step 7 with metric 0.5545

================================================================================
TRIAL 4461 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.4486911194811316e-05
  Dropout: 0.026680429667726453
================================================================================

[I 2025-11-05 21:01:01,768] Trial 4461 pruned. Pruned at step 9 with metric 0.5801
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4462 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.502015898223526e-05
  Dropout: 0.0803852491871047
================================================================================

[I 2025-11-05 21:05:31,348] Trial 4459 pruned. Pruned at step 20 with metric 0.5923

================================================================================
TRIAL 4463 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4292261806025887e-05
  Dropout: 0.044532839313542266
================================================================================

[I 2025-11-05 21:07:40,693] Trial 4463 pruned. Pruned at step 8 with metric 0.6009
[I 2025-11-05 21:07:41,264] Trial 4464 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4465 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.123893580898617e-06
  Dropout: 0.49679132032206663
================================================================================

[I 2025-11-05 21:16:00,215] Trial 4462 finished with value: 0.43370165745856354 and parameters: {'seed': 31169, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.502015898223526e-05, 'optim.weight_decay': 3.1825215500460875e-06, 'optim.beta1': 0.9137029963671329, 'optim.beta2': 0.9623363400627829, 'optim.eps': 8.669113015262393e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0026606519827044005, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.3531433497746388, 'model.dropout': 0.0803852491871047, 'model.attn_dropout': 0.20660863545675584, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8392391513622933, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.29066539188865453, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8540915232069994, 'loss.cls.alpha': 0.1870662224180564, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4466 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.0076264213059145e-05
  Dropout: 0.14393137829780356
================================================================================

[I 2025-11-05 21:28:08,967] Trial 4465 pruned. Pruned at step 17 with metric 0.5986
[W 2025-11-05 21:28:09,604] The parameter `tok.doc_stride` in Trial#4467 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4467 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.330911956937648e-06
  Dropout: 0.2541350703992592
================================================================================

[I 2025-11-05 21:32:33,542] Trial 4467 pruned. Pruned at step 27 with metric 0.5957
[W 2025-11-05 21:32:34,073] The parameter `tok.doc_stride` in Trial#4468 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4468 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.9391135164463858e-05
  Dropout: 0.42511050482218427
================================================================================

[I 2025-11-05 21:32:57,675] Trial 4466 pruned. Pruned at step 14 with metric 0.5842
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4469 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.4372169545409642e-05
  Dropout: 0.008107425897748974
================================================================================

[I 2025-11-05 21:38:18,008] Trial 4468 pruned. Pruned at step 9 with metric 0.5371

================================================================================
TRIAL 4470 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6521881510126934e-05
  Dropout: 0.2614769396275175
================================================================================

[I 2025-11-05 21:44:11,523] Trial 4470 pruned. Pruned at step 12 with metric 0.5443

================================================================================
TRIAL 4471 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.5294426679468236e-05
  Dropout: 0.14540386957778714
================================================================================

[I 2025-11-05 21:47:42,839] Trial 4471 pruned. Pruned at step 9 with metric 0.6189
[I 2025-11-05 21:47:43,409] Trial 4472 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-05 21:47:43,963] Trial 4473 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[I 2025-11-05 21:47:44,516] Trial 4474 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4475 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1174027447247969e-05
  Dropout: 0.33242361249806684
================================================================================

[I 2025-11-05 21:57:51,102] Trial 4469 pruned. Pruned at step 8 with metric 0.5823
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4476 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.923279962766263e-06
  Dropout: 0.04607216365557533
================================================================================

[I 2025-11-05 22:11:50,768] Trial 4475 pruned. Pruned at step 27 with metric 0.6562

================================================================================
TRIAL 4477 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 3.612276394829886e-05
  Dropout: 0.42605278721861867
================================================================================

[I 2025-11-05 22:18:15,179] Trial 4477 pruned. Pruned at step 10 with metric 0.5666
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4478 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.682596386912421e-05
  Dropout: 0.15151774013138725
================================================================================

[I 2025-11-05 22:50:06,493] Trial 4478 finished with value: 0.4533333333333333 and parameters: {'seed': 55423, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.682596386912421e-05, 'optim.weight_decay': 4.380137403513888e-05, 'optim.beta1': 0.8969007066863488, 'optim.beta2': 0.9703655302160737, 'optim.eps': 7.015585402135305e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.014400663778569424, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.338462312814617, 'model.dropout': 0.15151774013138725, 'model.attn_dropout': 0.13940632201725614, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8178725537116377, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.37157333124567615, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19960951643724592, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4479 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.851874699577992e-05
  Dropout: 0.48731789633146577
================================================================================

[I 2025-11-05 23:01:15,387] Trial 4476 finished with value: 0.7389865036923859 and parameters: {'seed': 20569, 'model.name': 'roberta-large', 'tok.max_length': 256, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.923279962766263e-06, 'optim.weight_decay': 0.0008447698022645266, 'optim.beta1': 0.9275316101650004, 'optim.beta2': 0.9869514749934694, 'optim.eps': 4.0490430641859515e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0834948377465017, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.261224066044937, 'model.dropout': 0.04607216365557533, 'model.attn_dropout': 0.1161242997326691, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8005326782077925, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.4354432120812805, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1802472337989437, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 4480 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 8.9711429194451e-06
  Dropout: 0.2681277746043611
================================================================================

[I 2025-11-05 23:33:58,303] Trial 4479 finished with value: 0.4368131868131868 and parameters: {'seed': 57285, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 2.851874699577992e-05, 'optim.weight_decay': 0.03391088188728063, 'optim.beta1': 0.9086266212857244, 'optim.beta2': 0.9500938751679053, 'optim.eps': 4.5322785050890367e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1250481785763634, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.8650704884482339, 'model.dropout': 0.48731789633146577, 'model.attn_dropout': 0.14705258171836239, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8319207319107828, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3976784063646728, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.659530725574134, 'loss.cls.alpha': 0.289846837860145, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4481 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 5.717936319814444e-05
  Dropout: 0.11960283218329427
================================================================================

[I 2025-11-05 23:37:46,178] Trial 4480 pruned. Pruned at step 9 with metric 0.5349
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4482 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.515912772584751e-06
  Dropout: 0.06346289367563264
================================================================================

[I 2025-11-05 23:39:42,574] Trial 4481 pruned. Pruned at step 16 with metric 0.6469
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4483 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.0351188751724738e-05
  Dropout: 0.051314981185506314
================================================================================

[I 2025-11-05 23:43:53,143] Trial 4482 pruned. Pruned at step 11 with metric 0.6207

================================================================================
TRIAL 4484 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.216952084414415e-05
  Dropout: 0.3194158114809902
================================================================================

[I 2025-11-05 23:54:07,248] Trial 4484 pruned. Pruned at step 6 with metric 0.5693
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4485 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.2462854744250501e-05
  Dropout: 0.3668018063713855
================================================================================

[I 2025-11-06 00:08:13,678] Trial 4485 pruned. Pruned at step 7 with metric 0.5837
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4486 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.0619641592067093e-05
  Dropout: 0.0959842846773468
================================================================================

[I 2025-11-06 00:11:57,856] Trial 4483 pruned. Pruned at step 27 with metric 0.6139
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4487 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.874646983276767e-05
  Dropout: 0.1175437352809512
================================================================================

[I 2025-11-06 00:17:49,922] Trial 4487 pruned. Pruned at step 9 with metric 0.5927

================================================================================
TRIAL 4488 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.3992006627452208e-05
  Dropout: 0.4137417644884743
================================================================================

[I 2025-11-06 00:22:49,500] Trial 4488 pruned. Pruned at step 9 with metric 0.5545
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4489 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 4.582637688817585e-05
  Dropout: 0.43747664907788586
================================================================================

[I 2025-11-06 00:43:07,531] Trial 4486 finished with value: 0.7054272134409196 and parameters: {'seed': 58001, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.0619641592067093e-05, 'optim.weight_decay': 0.0003002462442809257, 'optim.beta1': 0.9198890763213937, 'optim.beta2': 0.9604794868525468, 'optim.eps': 4.361281647390673e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.04329973282695842, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.087838215222329, 'model.dropout': 0.0959842846773468, 'model.attn_dropout': 0.19148564977615734, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8545981871536236, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.3986443171110464, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18075063146525017, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 00:43:08,119] Trial 4490 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4491 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.2608446806189973e-05
  Dropout: 0.4043968343471103
================================================================================

[I 2025-11-06 00:43:15,753] Trial 4491 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352
[I 2025-11-06 00:43:16,494] Trial 4492 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

[OOM] Trial 4491 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4493 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.437678370784222e-06
  Dropout: 0.046447564443554586
================================================================================

[I 2025-11-06 00:50:41,349] Trial 4489 finished with value: 0.43370165745856354 and parameters: {'seed': 15711, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 4.582637688817585e-05, 'optim.weight_decay': 0.00040804865873558495, 'optim.beta1': 0.8660758925740806, 'optim.beta2': 0.9615505455465744, 'optim.eps': 5.891345552193083e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0008749055322393867, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.3485089393182381, 'model.dropout': 0.43747664907788586, 'model.attn_dropout': 0.1717493271955862, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9260183079611358, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.2968680206837069, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.607614902326105, 'loss.cls.alpha': 0.10058627168719318, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4494 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.1429668991614252e-05
  Dropout: 0.12791976584991746
================================================================================

[I 2025-11-06 00:56:34,235] Trial 4494 pruned. Pruned at step 9 with metric 0.5417
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4495 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.2843286682231856e-05
  Dropout: 0.3933980725237832
================================================================================

[I 2025-11-06 01:00:33,583] Trial 4495 pruned. Pruned at step 7 with metric 0.6242
[I 2025-11-06 01:00:34,161] Trial 4496 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-06 01:00:34,692] The parameter `tok.doc_stride` in Trial#4497 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4497 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0392945894183397e-05
  Dropout: 0.010632228921716152
================================================================================

[I 2025-11-06 01:01:19,461] Trial 4493 pruned. Pruned at step 18 with metric 0.6197
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4498 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 2.055133396679821e-05
  Dropout: 0.33378264169504707
================================================================================

[I 2025-11-06 01:03:34,254] Trial 4497 pruned. Pruned at step 6 with metric 0.5672
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4499 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.436624973295457e-05
  Dropout: 0.24482610558716247
================================================================================

[I 2025-11-06 01:29:40,560] Trial 4499 finished with value: 0.7096604196172722 and parameters: {'seed': 64887, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.436624973295457e-05, 'optim.weight_decay': 0.00019270817159218101, 'optim.beta1': 0.9085991273676359, 'optim.beta2': 0.9580787216731561, 'optim.eps': 2.215618454994485e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.03903634522563762, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.9254070624056081, 'model.dropout': 0.24482610558716247, 'model.attn_dropout': 0.2388767124225299, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8610995134605594, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4565669322074446, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.560371393659688, 'loss.cls.alpha': 0.21160633129182685, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4500 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.5336554418690034e-05
  Dropout: 0.060110859530218955
================================================================================

[I 2025-11-06 01:31:38,431] Trial 4500 pruned. Pruned at step 9 with metric 0.6069
[I 2025-11-06 01:31:39,006] Trial 4501 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4502 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.125965863634737e-05
  Dropout: 0.1379761491531956
================================================================================

[I 2025-11-06 01:35:52,498] Trial 4502 pruned. Pruned at step 9 with metric 0.6037

================================================================================
TRIAL 4503 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.4201086545181945e-05
  Dropout: 0.1458348680085119
================================================================================

[I 2025-11-06 01:43:01,011] Trial 4503 pruned. Pruned at step 6 with metric 0.4880
[W 2025-11-06 01:43:01,575] The parameter `tok.doc_stride` in Trial#4504 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4504 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3957495658112637e-05
  Dropout: 0.10210734293430894
================================================================================

[I 2025-11-06 01:45:27,840] Trial 4504 pruned. Pruned at step 11 with metric 0.6083
[W 2025-11-06 01:45:28,390] The parameter `tok.doc_stride` in Trial#4505 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4505 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.2355043055450689e-05
  Dropout: 0.255293897136212
================================================================================

[I 2025-11-06 01:46:19,538] Trial 4498 finished with value: 0.657129639514608 and parameters: {'seed': 20068, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.055133396679821e-05, 'optim.weight_decay': 1.9013226479460204e-06, 'optim.beta1': 0.8851085238278362, 'optim.beta2': 0.9907410902203956, 'optim.eps': 3.2808763984775137e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.19682601716300546, 'train.clip_grad': 0.8627203871853302, 'model.dropout': 0.33378264169504707, 'model.attn_dropout': 0.11977952946863288, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8622136970343407, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4071535534243859, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.960088674306361, 'loss.cls.alpha': 0.22586079893506184, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 01:46:20,116] Trial 4506 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-06 01:46:20,667] Trial 4507 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 4508 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 7.596894113907634e-05
  Dropout: 0.10446619869533183
================================================================================

[I 2025-11-06 02:02:41,005] Trial 4508 finished with value: 0.4605263157894737 and parameters: {'seed': 52525, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 7.596894113907634e-05, 'optim.weight_decay': 0.006698316651262525, 'optim.beta1': 0.9190739278944223, 'optim.beta2': 0.9779231386825276, 'optim.eps': 3.5023333108622287e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18620956720794254, 'train.clip_grad': 0.037108309711770315, 'model.dropout': 0.10446619869533183, 'model.attn_dropout': 0.1686070004145392, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.839721466920771, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.06615715276739578, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.17561677806023562, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 02:02:41,599] Trial 4509 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4510 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 4.565119804691656e-05
  Dropout: 0.22294418445747144
================================================================================

[I 2025-11-06 02:03:16,953] Trial 4505 finished with value: 0.6917293233082706 and parameters: {'seed': 39650, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.2355043055450689e-05, 'optim.weight_decay': 1.5845842016420697e-05, 'optim.beta1': 0.9275252871092212, 'optim.beta2': 0.9619083380846359, 'optim.eps': 7.739280180867062e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.06048398717074062, 'train.clip_grad': 1.4990335526822618, 'model.dropout': 0.255293897136212, 'model.attn_dropout': 0.28190781080780974, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8028726625282454, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.4063377519387018, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.21905172332902, 'loss.cls.alpha': 0.3547673095609646, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 44 (patience=20)

================================================================================
TRIAL 4511 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.1485247581473026e-06
  Dropout: 0.3007210842088371
================================================================================

[I 2025-11-06 02:33:20,895] Trial 4510 pruned. Pruned at step 27 with metric 0.5962
[I 2025-11-06 02:33:21,559] Trial 4512 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4513 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 7.588958874426755e-06
  Dropout: 0.025332387151001043
================================================================================

[I 2025-11-06 02:45:47,519] Trial 4511 finished with value: 0.7288083850615651 and parameters: {'seed': 62524, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 288, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 5.1485247581473026e-06, 'optim.weight_decay': 1.7883610060712781e-06, 'optim.beta1': 0.8550807670623548, 'optim.beta2': 0.9677317494364771, 'optim.eps': 4.932359880768789e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.18717398518037856, 'train.clip_grad': 1.1652330235740975, 'model.dropout': 0.3007210842088371, 'model.attn_dropout': 0.041708657631560134, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9052243851393462, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.40145866525508184, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.999201359892607, 'loss.cls.alpha': 0.7972874682072753, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 4514 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.5653470187048018e-05
  Dropout: 0.18689163282652438
================================================================================

[I 2025-11-06 02:49:42,007] Trial 4513 finished with value: 0.6877855619859884 and parameters: {'seed': 12339, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.588958874426755e-06, 'optim.weight_decay': 2.5869791958965068e-05, 'optim.beta1': 0.8842253555178942, 'optim.beta2': 0.9517227190059948, 'optim.eps': 1.4258355906584386e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.03362033464283298, 'train.clip_grad': 1.3906140519354429, 'model.dropout': 0.025332387151001043, 'model.attn_dropout': 0.17035322397647595, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8011733126119632, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3212033825806282, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.3294167809816475, 'loss.cls.alpha': 0.724779851801636, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 02:49:42,600] Trial 4515 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-06 02:49:43,171] Trial 4516 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-06 02:49:43,687] The parameter `tok.doc_stride` in Trial#4517 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4517 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.846889533455651e-05
  Dropout: 0.030569246499802544
================================================================================

[I 2025-11-06 03:18:38,849] Trial 4517 pruned. Pruned at step 27 with metric 0.6069
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4518 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.0602983497902046e-05
  Dropout: 0.3762350949528946
================================================================================

[I 2025-11-06 03:25:07,820] Trial 4514 finished with value: 0.7472879684418146 and parameters: {'seed': 65332, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.5653470187048018e-05, 'optim.weight_decay': 8.06302319953272e-05, 'optim.beta1': 0.8779011923240121, 'optim.beta2': 0.9706217827110146, 'optim.eps': 3.299807428838481e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.024395476171869736, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9761947442486143, 'model.dropout': 0.18689163282652438, 'model.attn_dropout': 0.1736293739929461, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8084296950118844, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.4797706288823016, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1779873033129658, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 4519 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3249549733176283e-05
  Dropout: 0.14000638625013295
================================================================================

[I 2025-11-06 03:32:41,337] Trial 4518 pruned. Pruned at step 27 with metric 0.6450
[W 2025-11-06 03:32:41,899] The parameter `tok.doc_stride` in Trial#4520 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4520 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.084466486437931e-06
  Dropout: 0.02623882777969641
================================================================================

[I 2025-11-06 03:35:40,302] Trial 4519 finished with value: 0.6833487797343218 and parameters: {'seed': 63906, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.3249549733176283e-05, 'optim.weight_decay': 6.557721270079898e-05, 'optim.beta1': 0.8965931360876425, 'optim.beta2': 0.9825763475458228, 'optim.eps': 5.206243095824553e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.0014012335115605595, 'train.clip_grad': 0.9822073795210681, 'model.dropout': 0.14000638625013295, 'model.attn_dropout': 0.22501868076671372, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8855631071020302, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.35228509258005014, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.286773888985119, 'loss.cls.alpha': 0.3761418799270967, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 03:35:40,896] Trial 4521 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[I 2025-11-06 03:35:41,464] Trial 4522 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 4523 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.6086191802583505e-05
  Dropout: 0.1512347100776526
================================================================================

[I 2025-11-06 03:40:35,613] Trial 4523 pruned. Pruned at step 9 with metric 0.5859
[I 2025-11-06 03:40:36,197] Trial 4524 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4525 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 4.2839255873694094e-05
  Dropout: 0.05411121208536416
================================================================================

[I 2025-11-06 03:45:48,387] Trial 4520 pruned. Pruned at step 9 with metric 0.5997
[I 2025-11-06 03:45:48,982] Trial 4526 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4527 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.84980802371621e-06
  Dropout: 0.3571095661220744
================================================================================

[I 2025-11-06 03:51:09,662] Trial 4525 finished with value: 0.4429347826086957 and parameters: {'seed': 57807, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 4.2839255873694094e-05, 'optim.weight_decay': 4.937272438085804e-06, 'optim.beta1': 0.9381202370470867, 'optim.beta2': 0.9668113989174583, 'optim.eps': 3.4873416771131534e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.008604274893985346, 'train.clip_grad': 1.449658668431739, 'model.dropout': 0.05411121208536416, 'model.attn_dropout': 0.19478731440455216, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8055847742909777, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.3457232213180851, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.99715738852474, 'loss.cls.alpha': 0.30110716135373494, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4528 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 8.851667552193176e-06
  Dropout: 0.41094990371178575
================================================================================

[I 2025-11-06 03:51:16,862] Trial 4528 pruned. OOM: microsoft/deberta-v3-large bs=16 len=352
[I 2025-11-06 03:51:17,046] Trial 4527 pruned. OOM: microsoft/deberta-v3-base bs=8 len=384
[I 2025-11-06 03:51:17,847] Trial 4530 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

[OOM] Trial 4527 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 36.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4528 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 36.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4529 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 6.669549646522927e-06
  Dropout: 0.4505274617397047
================================================================================


================================================================================
TRIAL 4531 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.703154905697522e-05
  Dropout: 0.33356409382676844
================================================================================

[I 2025-11-06 04:02:30,655] Trial 4531 finished with value: 0.7304404996712689 and parameters: {'seed': 57719, 'model.name': 'bert-base-uncased', 'tok.max_length': 288, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 4.703154905697522e-05, 'optim.weight_decay': 5.1273209222531866e-05, 'optim.beta1': 0.8826240802161193, 'optim.beta2': 0.9670088201651639, 'optim.eps': 1.3912019653669395e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03160572145667683, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8689547355953408, 'model.dropout': 0.33356409382676844, 'model.attn_dropout': 0.17183804154394453, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8118004464722908, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.4184822563924012, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.17760540049383636, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4532 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 1.9368707077414063e-05
  Dropout: 0.09164921946800175
================================================================================

[I 2025-11-06 04:13:56,107] Trial 4532 pruned. Pruned at step 9 with metric 0.6359
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4533 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.9568040938469004e-05
  Dropout: 0.1986467102201667
================================================================================

[I 2025-11-06 04:32:05,576] Trial 4533 pruned. Pruned at step 15 with metric 0.6215

================================================================================
TRIAL 4534 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.1313038835862607e-05
  Dropout: 0.32969466995877905
================================================================================

[I 2025-11-06 04:34:35,138] Trial 4529 pruned. Pruned at step 11 with metric 0.6042
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4535 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.3001943040586874e-05
  Dropout: 0.03138736954915684
================================================================================

[I 2025-11-06 04:35:47,268] Trial 4534 pruned. Pruned at step 12 with metric 0.6111
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4536 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.619068301722916e-05
  Dropout: 0.2317494078202886
================================================================================

[I 2025-11-06 04:45:36,380] Trial 4535 pruned. Pruned at step 7 with metric 0.6072
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4537 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 9.615139111269821e-06
  Dropout: 0.3733575490326583
================================================================================

[I 2025-11-06 04:49:09,442] Trial 4536 pruned. Pruned at step 9 with metric 0.6396
[I 2025-11-06 04:49:10,111] Trial 4538 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-06 04:49:10,669] Trial 4539 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4540 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.4344623337445773e-05
  Dropout: 0.14541802491970335
================================================================================

[I 2025-11-06 04:56:31,700] Trial 4540 pruned. Pruned at step 10 with metric 0.5827
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4541 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 3.809672795783503e-05
  Dropout: 0.028194948722567198
================================================================================

[I 2025-11-06 05:06:05,017] Trial 4541 pruned. Pruned at step 9 with metric 0.6484
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4542 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.870641351472325e-06
  Dropout: 0.3388593298947875
================================================================================

[I 2025-11-06 05:15:28,234] Trial 4537 pruned. Pruned at step 12 with metric 0.6417
[I 2025-11-06 05:15:28,928] Trial 4543 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-06 05:15:29,530] Trial 4544 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4545 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.5528556959142882e-05
  Dropout: 0.17342712144132913
================================================================================

[I 2025-11-06 05:25:45,204] Trial 4545 finished with value: 0.661570485213896 and parameters: {'seed': 45233, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.5528556959142882e-05, 'optim.weight_decay': 3.429378450034854e-05, 'optim.beta1': 0.8631340423052217, 'optim.beta2': 0.9780810669975996, 'optim.eps': 5.8625162396374665e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.03778420624380907, 'train.clip_grad': 1.439967749695465, 'model.dropout': 0.17342712144132913, 'model.attn_dropout': 0.2455323256523535, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8061128635089658, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.46207713417580554, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.17931039330793666, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 22 (patience=20)

================================================================================
TRIAL 4546 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.359923681066995e-05
  Dropout: 0.2206977286417211
================================================================================

[I 2025-11-06 05:37:51,914] Trial 4542 finished with value: 0.7054272134409196 and parameters: {'seed': 56301, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 288, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.870641351472325e-06, 'optim.weight_decay': 0.0032437228469476965, 'optim.beta1': 0.8488075701536134, 'optim.beta2': 0.9783926262635134, 'optim.eps': 2.6229006536402167e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18943308567605757, 'train.clip_grad': 0.9303865932046463, 'model.dropout': 0.3388593298947875, 'model.attn_dropout': 0.038171615454867226, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8531798038104478, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.34825137839551357, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.17574275937929917, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-06 05:37:52,460] The parameter `tok.doc_stride` in Trial#4547 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4547 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0053950489820011e-05
  Dropout: 0.20708370985963306
================================================================================

[I 2025-11-06 05:43:00,818] Trial 4547 pruned. Pruned at step 9 with metric 0.5884
[I 2025-11-06 05:43:01,396] Trial 4548 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-06 05:43:01,948] Trial 4549 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-06 05:43:02,473] The parameter `tok.doc_stride` in Trial#4550 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4550 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.193980054540537e-05
  Dropout: 0.15885590129344
================================================================================

[I 2025-11-06 05:55:48,898] Trial 4550 finished with value: 0.6770975532920298 and parameters: {'seed': 43102, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.193980054540537e-05, 'optim.weight_decay': 0.0017697431693108796, 'optim.beta1': 0.8193481693663394, 'optim.beta2': 0.9654588291913654, 'optim.eps': 3.909257839981416e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.019352229618066694, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8706551325577068, 'model.dropout': 0.15885590129344, 'model.attn_dropout': 0.1462154619469013, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8037567241243092, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.470491659951408, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.17461356928285537, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 05:55:49,470] Trial 4551 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-06 05:55:50,027] Trial 4552 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-06 05:55:50,585] Trial 4553 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4554 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.5976173332579342e-05
  Dropout: 0.0740822893331231
================================================================================

[I 2025-11-06 06:09:00,090] Trial 4554 pruned. Pruned at step 10 with metric 0.5957
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4555 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.890499988931932e-06
  Dropout: 0.33303410854795706
================================================================================

[I 2025-11-06 06:22:44,050] Trial 4546 pruned. Pruned at step 27 with metric 0.5957

================================================================================
TRIAL 4556 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.474182363727483e-05
  Dropout: 0.031804514615360255
================================================================================

[I 2025-11-06 06:41:50,287] Trial 4555 finished with value: 0.7092656791679799 and parameters: {'seed': 64188, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.890499988931932e-06, 'optim.weight_decay': 0.1331106047029026, 'optim.beta1': 0.8786397366343784, 'optim.beta2': 0.9764863634957672, 'optim.eps': 7.960264870331271e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.04198342119202527, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.246831752996131, 'model.dropout': 0.33303410854795706, 'model.attn_dropout': 0.2924659738946959, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9317647353799348, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.31008844323401236, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.3935178285788805, 'loss.cls.alpha': 0.8948566814347925, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-06 06:41:50,836] The parameter `tok.doc_stride` in Trial#4557 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4557 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.6056801965214684e-05
  Dropout: 0.2335428839766504
================================================================================

[I 2025-11-06 06:43:11,554] Trial 4556 finished with value: 0.6722046722046722 and parameters: {'seed': 60165, 'model.name': 'xlm-roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.474182363727483e-05, 'optim.weight_decay': 5.461171005527826e-05, 'optim.beta1': 0.949280738096751, 'optim.beta2': 0.9651598157603243, 'optim.eps': 4.482624799811408e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0726099692514844, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.3858088222151752, 'model.dropout': 0.031804514615360255, 'model.attn_dropout': 0.2807583650759302, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8089786170409807, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.23094587060358804, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.451554028117237, 'loss.cls.alpha': 0.3569624080178981, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 4558 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.3260103199411163e-05
  Dropout: 0.17851329154265044
================================================================================

[I 2025-11-06 06:48:03,503] Trial 4557 pruned. Pruned at step 10 with metric 0.5878
[I 2025-11-06 06:48:04,116] Trial 4559 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4560 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.21507971294077e-05
  Dropout: 0.06169514415193832
================================================================================

[I 2025-11-06 06:49:37,969] Trial 4560 pruned. OOM: microsoft/deberta-v3-base bs=8 len=384
[W 2025-11-06 06:49:38,625] The parameter `tok.doc_stride` in Trial#4561 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-06 06:49:39,265] Trial 4558 pruned. OOM: microsoft/deberta-v3-large bs=8 len=288
[W 2025-11-06 06:49:39,947] The parameter `tok.doc_stride` in Trial#4562 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 4560 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 86.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4558 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 46.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4561 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.2085166138309715e-06
  Dropout: 0.3004829213905517
================================================================================


================================================================================
TRIAL 4562 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.2713173958803434e-05
  Dropout: 0.46629139964926997
================================================================================

[I 2025-11-06 06:53:07,195] Trial 4561 pruned. Pruned at step 11 with metric 0.4993

================================================================================
TRIAL 4563 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.7069494816932333e-05
  Dropout: 0.06772895828800818
================================================================================

[I 2025-11-06 06:58:28,678] Trial 4562 pruned. Pruned at step 19 with metric 0.4979

================================================================================
TRIAL 4564 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.221114272217831e-05
  Dropout: 0.3976129546887291
================================================================================

[I 2025-11-06 07:00:55,141] Trial 4563 pruned. Pruned at step 11 with metric 0.5684
[I 2025-11-06 07:00:55,739] Trial 4565 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-06 07:00:56,303] Trial 4566 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-06 07:00:56,865] Trial 4567 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4568 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.1774265295338766e-05
  Dropout: 0.4274949689899181
================================================================================

[I 2025-11-06 07:23:01,372] Trial 4564 finished with value: 0.6672077922077921 and parameters: {'seed': 55227, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.221114272217831e-05, 'optim.weight_decay': 8.623126104183267e-06, 'optim.beta1': 0.8884558675020088, 'optim.beta2': 0.975410043482308, 'optim.eps': 1.905667393969489e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.19555950045791767, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.0854116357525958, 'model.dropout': 0.3976129546887291, 'model.attn_dropout': 0.041284168336717934, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9630026945791488, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.47335157545808293, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.754196000387923, 'loss.cls.alpha': 0.7303314759069325, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 4569 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.189772048159852e-06
  Dropout: 0.44905118812040146
================================================================================

[I 2025-11-06 07:25:40,833] Trial 4568 finished with value: 0.6446580523164198 and parameters: {'seed': 62974, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 2.1774265295338766e-05, 'optim.weight_decay': 0.022009269403460752, 'optim.beta1': 0.8868828751148358, 'optim.beta2': 0.9637463557845658, 'optim.eps': 9.043136647849517e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.05807901321066377, 'sched.poly_power': 0.9802534872507096, 'train.clip_grad': 1.157640941253755, 'model.dropout': 0.4274949689899181, 'model.attn_dropout': 0.25699425816189897, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9647947802563137, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.496402274615672, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.151115086817428, 'loss.cls.alpha': 0.6963129426414216, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 50 (patience=20)

================================================================================
TRIAL 4570 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.1176701153832508e-05
  Dropout: 0.48168844680201583
================================================================================

[I 2025-11-06 07:32:41,608] Trial 4570 pruned. Pruned at step 9 with metric 0.6257
[I 2025-11-06 07:32:42,206] Trial 4571 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4572 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.6087798819839082e-05
  Dropout: 0.1398553668374338
================================================================================

[I 2025-11-06 07:42:12,521] Trial 4572 pruned. Pruned at step 27 with metric 0.5997
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4573 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.713078508047521e-05
  Dropout: 0.39137918858748205
================================================================================

[I 2025-11-06 07:58:00,835] Trial 4573 pruned. Pruned at step 12 with metric 0.4414

================================================================================
TRIAL 4574 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.2622046058108043e-05
  Dropout: 0.16430476361000168
================================================================================

[I 2025-11-06 08:05:14,957] Trial 4574 pruned. Pruned at step 9 with metric 0.5927

================================================================================
TRIAL 4575 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.6469479154356015e-05
  Dropout: 0.02242865216096679
================================================================================

[I 2025-11-06 08:11:22,608] Trial 4575 pruned. Pruned at step 13 with metric 0.6555

================================================================================
TRIAL 4576 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 4.942046391151922e-05
  Dropout: 0.41646274588379895
================================================================================

[I 2025-11-06 08:12:34,728] Trial 4569 pruned. Pruned at step 16 with metric 0.5282
[I 2025-11-06 08:12:35,404] Trial 4577 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4578 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.0494757654733068e-05
  Dropout: 0.029308895841783842
================================================================================

[I 2025-11-06 08:13:32,140] Trial 4576 pruned. Pruned at step 11 with metric 0.6199

================================================================================
TRIAL 4579 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.637443474519481e-06
  Dropout: 0.08697754262283143
================================================================================

[I 2025-11-06 08:16:55,179] Trial 4579 pruned. Pruned at step 9 with metric 0.5676
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4580 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.7326245676098436e-05
  Dropout: 0.0426397545954119
================================================================================

[I 2025-11-06 08:28:10,893] Trial 4580 pruned. Pruned at step 8 with metric 0.5923
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4581 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.0107025903696225e-06
  Dropout: 0.468407264440017
================================================================================

[I 2025-11-06 08:37:43,035] Trial 4578 pruned. Pruned at step 9 with metric 0.6027

================================================================================
TRIAL 4582 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.179759297621492e-05
  Dropout: 0.48829465692762747
================================================================================

[I 2025-11-06 08:41:28,820] Trial 4581 pruned. Pruned at step 9 with metric 0.6155
[W 2025-11-06 08:41:29,500] The parameter `tok.doc_stride` in Trial#4583 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4583 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.1053389603736931e-05
  Dropout: 0.005119492284938798
================================================================================

[I 2025-11-06 08:47:11,338] Trial 4583 finished with value: 0.6562146562146562 and parameters: {'seed': 16164, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.1053389603736931e-05, 'optim.weight_decay': 0.01136121650063459, 'optim.beta1': 0.830556652185931, 'optim.beta2': 0.9663790537781954, 'optim.eps': 3.170872575394148e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18674531040702755, 'sched.poly_power': 0.8900370338212169, 'train.clip_grad': 0.8311807978193817, 'model.dropout': 0.005119492284938798, 'model.attn_dropout': 0.222368382959617, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9279586169712625, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.303774168376381, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.9684392965043513, 'loss.cls.alpha': 0.3684718214554633, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 08:47:11,906] Trial 4584 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4585 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.479363982557572e-05
  Dropout: 0.2814094076415291
================================================================================

[I 2025-11-06 09:10:44,394] Trial 4582 finished with value: 0.5732845248349124 and parameters: {'seed': 64273, 'model.name': 'bert-base-uncased', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.179759297621492e-05, 'optim.weight_decay': 0.05772426567719415, 'optim.beta1': 0.8530738116121787, 'optim.beta2': 0.9599187875349884, 'optim.eps': 3.076814184539165e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04257823956453098, 'sched.poly_power': 0.7739962845598239, 'train.clip_grad': 1.0933263529015238, 'model.dropout': 0.48829465692762747, 'model.attn_dropout': 0.1472088860444382, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9627630065972446, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.17505331584031525, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16649009378621304, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 4586 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.026145924686842e-05
  Dropout: 0.05573619745767113
================================================================================

[I 2025-11-06 09:27:43,849] Trial 4585 pruned. Pruned at step 27 with metric 0.5872
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4587 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 3.6985779840528774e-05
  Dropout: 0.1284990844595646
================================================================================

[I 2025-11-06 09:35:10,926] Trial 4587 pruned. Pruned at step 9 with metric 0.5927
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4588 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.1435126032928344e-05
  Dropout: 0.0012339894116348837
================================================================================

[I 2025-11-06 09:42:42,122] Trial 4588 pruned. Pruned at step 8 with metric 0.6111

================================================================================
TRIAL 4589 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.11339169366417e-05
  Dropout: 0.1252590958654199
================================================================================

[I 2025-11-06 09:47:09,219] Trial 4589 pruned. Pruned at step 10 with metric 0.6082
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4590 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.4650726724019866e-05
  Dropout: 0.15319522195479626
================================================================================

[I 2025-11-06 09:49:09,882] Trial 4586 pruned. Pruned at step 27 with metric 0.6230
[W 2025-11-06 09:49:10,700] The parameter `tok.doc_stride` in Trial#4591 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4591 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0790053855905389e-05
  Dropout: 0.4004346004847673
================================================================================

[I 2025-11-06 09:55:28,034] Trial 4590 pruned. Pruned at step 7 with metric 0.6159
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4592 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 8.925504449675606e-06
  Dropout: 0.16305030156883155
================================================================================

[I 2025-11-06 09:59:09,365] Trial 4591 pruned. Pruned at step 10 with metric 0.6069
[W 2025-11-06 09:59:09,928] The parameter `tok.doc_stride` in Trial#4593 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4593 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4721759870923979e-05
  Dropout: 0.3899282134815467
================================================================================

[I 2025-11-06 10:02:54,671] Trial 4593 pruned. Pruned at step 9 with metric 0.5949
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4594 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.9563965950874438e-05
  Dropout: 0.1364533919555021
================================================================================

[I 2025-11-06 10:06:08,246] Trial 4594 pruned. Pruned at step 11 with metric 0.6538
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4595 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 4.8017045711897766e-05
  Dropout: 0.2666393408373571
================================================================================

[I 2025-11-06 10:09:25,017] Trial 4595 pruned. Pruned at step 9 with metric 0.6067
[I 2025-11-06 10:09:25,596] Trial 4596 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-06 10:09:26,147] Trial 4597 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-06 10:09:26,674] The parameter `tok.doc_stride` in Trial#4598 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4598 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6927064720097868e-05
  Dropout: 0.028375874056951578
================================================================================

[I 2025-11-06 10:11:01,622] Trial 4592 pruned. Pruned at step 9 with metric 0.5603
[W 2025-11-06 10:11:02,266] The parameter `tok.doc_stride` in Trial#4599 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4599 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.8846539162681903e-05
  Dropout: 0.4438800122033866
================================================================================

[I 2025-11-06 10:14:04,089] Trial 4598 pruned. Pruned at step 9 with metric 0.4833

================================================================================
TRIAL 4600 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.539413108106357e-05
  Dropout: 0.14527691134691825
================================================================================

[I 2025-11-06 10:17:27,639] Trial 4599 finished with value: 0.6870865981479597 and parameters: {'seed': 56159, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.8846539162681903e-05, 'optim.weight_decay': 0.007766616671624249, 'optim.beta1': 0.8130303918491066, 'optim.beta2': 0.9568317142344406, 'optim.eps': 2.672086379598651e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.018278001899567676, 'train.clip_grad': 1.297372887744637, 'model.dropout': 0.4438800122033866, 'model.attn_dropout': 0.07870965556720413, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9447385516642566, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.3080997962078895, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.16360608760524975, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4601 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.131276864700052e-05
  Dropout: 0.07231411425935692
================================================================================

[I 2025-11-06 10:23:02,380] Trial 4601 pruned. Pruned at step 12 with metric 0.6135

================================================================================
TRIAL 4602 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.8884308987148846e-05
  Dropout: 0.14194522103670265
================================================================================

[I 2025-11-06 10:31:45,949] Trial 4600 pruned. Pruned at step 9 with metric 0.6207

================================================================================
TRIAL 4603 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.60719389710859e-05
  Dropout: 0.40569159042475306
================================================================================

[I 2025-11-06 10:33:49,801] Trial 4602 pruned. Pruned at step 13 with metric 0.6494
[I 2025-11-06 10:33:50,637] Trial 4604 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-06 10:33:51,165] The parameter `tok.doc_stride` in Trial#4605 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4605 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.9654431816093077e-05
  Dropout: 0.2953217057007717
================================================================================

[I 2025-11-06 10:36:49,812] Trial 4603 pruned. Pruned at step 9 with metric 0.6036
[W 2025-11-06 10:36:50,380] The parameter `tok.doc_stride` in Trial#4606 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-06 10:36:50,436] Trial 4606 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4607 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.4634400002370715e-05
  Dropout: 0.2643279271477281
================================================================================

[I 2025-11-06 10:45:54,796] Trial 4605 finished with value: 0.4383561643835616 and parameters: {'seed': 52201, 'model.name': 'xlm-roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.9654431816093077e-05, 'optim.weight_decay': 0.00017943554331517964, 'optim.beta1': 0.9042137048344763, 'optim.beta2': 0.9690661485952696, 'optim.eps': 1.1016456032015905e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.05342810484653074, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.6841445275659759, 'model.dropout': 0.2953217057007717, 'model.attn_dropout': 0.14686256370928685, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8194393637151753, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.42690859626927413, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.79773528921469, 'loss.cls.alpha': 0.2784187855754491, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4608 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.040188885874458e-05
  Dropout: 0.3795512024631771
================================================================================

[I 2025-11-06 11:01:52,998] Trial 4608 pruned. Pruned at step 7 with metric 0.6164

================================================================================
TRIAL 4609 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.8589714272627288e-05
  Dropout: 0.046594343257441745
================================================================================

[I 2025-11-06 11:05:12,264] Trial 4609 pruned. Pruned at step 9 with metric 0.5382
[I 2025-11-06 11:05:12,859] Trial 4610 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-06 11:05:13,435] Trial 4611 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4612 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.3181210090794592e-05
  Dropout: 0.33592480736178715
================================================================================

[I 2025-11-06 11:11:31,654] Trial 4607 pruned. Pruned at step 13 with metric 0.5629

================================================================================
TRIAL 4613 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.4816607257932748e-05
  Dropout: 0.4007582911221046
================================================================================

[I 2025-11-06 11:17:09,724] Trial 4612 pruned. Pruned at step 13 with metric 0.6143

================================================================================
TRIAL 4614 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.6102341029468914e-05
  Dropout: 0.10219574166677331
================================================================================

[I 2025-11-06 11:18:50,833] Trial 4614 pruned. Pruned at step 8 with metric 0.5827
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4615 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.3287770670149966e-05
  Dropout: 0.3330859320525276
================================================================================

[I 2025-11-06 11:38:20,455] Trial 4613 finished with value: 0.6903209166943001 and parameters: {'seed': 37271, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.4816607257932748e-05, 'optim.weight_decay': 2.7968467668146665e-05, 'optim.beta1': 0.8462073619051077, 'optim.beta2': 0.9673736953564098, 'optim.eps': 3.591945349218213e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.003865272876200013, 'train.clip_grad': 1.2385101858880827, 'model.dropout': 0.4007582911221046, 'model.attn_dropout': 0.1587203516407221, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9778621822352697, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.403595285578337, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.2838042939180982, 'loss.cls.alpha': 0.3115064926977179, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 11:38:21,052] Trial 4616 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4617 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.074618970621043e-06
  Dropout: 0.48258571047645377
================================================================================

[I 2025-11-06 11:38:47,369] Trial 4615 finished with value: 0.4429347826086957 and parameters: {'seed': 45708, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.3287770670149966e-05, 'optim.weight_decay': 0.0016722314742727586, 'optim.beta1': 0.8378618889572973, 'optim.beta2': 0.9500080218235515, 'optim.eps': 5.696252466166641e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.010385007531198802, 'sched.poly_power': 0.6278343241575648, 'train.clip_grad': 1.3862508783140495, 'model.dropout': 0.3330859320525276, 'model.attn_dropout': 0.10673971612447573, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9713048983040057, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.41281225135502936, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.234860721083393, 'loss.cls.alpha': 0.24379077441975844, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 11:38:47,979] Trial 4618 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4619 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.8877863151057863e-05
  Dropout: 0.2996954904200496
================================================================================

[I 2025-11-06 11:52:42,301] Trial 4619 finished with value: 0.6337612696829947 and parameters: {'seed': 46069, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.8877863151057863e-05, 'optim.weight_decay': 1.3881024311762874e-06, 'optim.beta1': 0.8709592189739536, 'optim.beta2': 0.9715037847885699, 'optim.eps': 2.6085036122018696e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.12970829331834308, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.3114720109809097, 'model.dropout': 0.2996954904200496, 'model.attn_dropout': 0.11385753373184448, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9504224692018047, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.49418541475296157, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.898024544989912, 'loss.cls.alpha': 0.324607858959488, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4620 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.3802212774190414e-05
  Dropout: 0.10899436280361047
================================================================================

[I 2025-11-06 11:55:13,201] Trial 4620 pruned. Pruned at step 8 with metric 0.5984
[I 2025-11-06 11:55:13,791] Trial 4621 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4622 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.1970631725247783e-05
  Dropout: 0.022071095119163546
================================================================================

[I 2025-11-06 12:05:19,784] Trial 4622 finished with value: 0.6785727943707216 and parameters: {'seed': 57897, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.1970631725247783e-05, 'optim.weight_decay': 1.978442477890309e-06, 'optim.beta1': 0.8236006207837713, 'optim.beta2': 0.9771725003748015, 'optim.eps': 8.424673036372266e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.12958950005023853, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.1267699787792147, 'model.dropout': 0.022071095119163546, 'model.attn_dropout': 0.23869651442093182, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9509498475486541, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.021546499002304376, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16612748860492008, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 12:05:20,379] Trial 4623 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4624 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.43284261584094e-05
  Dropout: 0.14761722458584203
================================================================================

[I 2025-11-06 12:10:16,168] Trial 4624 pruned. Pruned at step 14 with metric 0.5957
[W 2025-11-06 12:10:16,727] The parameter `tok.doc_stride` in Trial#4625 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4625 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 3.295320733632491e-05
  Dropout: 0.02492085162889617
================================================================================

[I 2025-11-06 12:15:15,372] Trial 4625 pruned. Pruned at step 20 with metric 0.6002
[I 2025-11-06 12:15:15,953] Trial 4626 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-06 12:15:16,511] Trial 4627 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4628 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.9735398055335834e-05
  Dropout: 0.35662658319595
================================================================================

[I 2025-11-06 12:20:00,266] Trial 4628 pruned. Pruned at step 8 with metric 0.6072
[W 2025-11-06 12:20:00,829] The parameter `tok.doc_stride` in Trial#4629 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4629 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.909156087777433e-06
  Dropout: 0.3649609720569087
================================================================================

[I 2025-11-06 12:22:23,910] Trial 4629 pruned. Pruned at step 9 with metric 0.5496
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4630 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 4.566730790265899e-05
  Dropout: 0.17625304279453935
================================================================================

[I 2025-11-06 12:39:58,808] Trial 4617 finished with value: 0.7477440525020509 and parameters: {'seed': 58642, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.074618970621043e-06, 'optim.weight_decay': 0.0004132354445473472, 'optim.beta1': 0.8923255114900744, 'optim.beta2': 0.9568779434502844, 'optim.eps': 5.2404744025566994e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.018853906118308728, 'sched.poly_power': 1.1958966379079397, 'train.clip_grad': 1.4851877298731304, 'model.dropout': 0.48258571047645377, 'model.attn_dropout': 0.18189028902886856, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9471145365666378, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3529605895309936, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.509488543519111, 'loss.cls.alpha': 0.7360995498859697, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 12:39:59,426] Trial 4631 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4632 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.869188923461724e-05
  Dropout: 0.2919104201624677
================================================================================

[I 2025-11-06 12:50:17,881] Trial 4630 finished with value: 0.43213296398891965 and parameters: {'seed': 12614, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 4.566730790265899e-05, 'optim.weight_decay': 0.0009654510218445726, 'optim.beta1': 0.9011997875919726, 'optim.beta2': 0.9738005759233421, 'optim.eps': 1.3430057736839763e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.07373271868209456, 'sched.poly_power': 0.7561906340794121, 'train.clip_grad': 1.4898787554767445, 'model.dropout': 0.17625304279453935, 'model.attn_dropout': 0.2111728685011552, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8081281656516183, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.24830366330144277, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.997201960466673, 'loss.cls.alpha': 0.3760253662935238, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4633 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 9.174063980801564e-06
  Dropout: 0.3653777462300953
================================================================================

[I 2025-11-06 13:08:16,022] Trial 4632 pruned. Pruned at step 9 with metric 0.6637
[W 2025-11-06 13:08:16,703] The parameter `tok.doc_stride` in Trial#4634 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4634 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.237162944653561e-06
  Dropout: 0.3644079766949089
================================================================================

[I 2025-11-06 13:20:52,382] Trial 4634 pruned. Pruned at step 22 with metric 0.5866
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4635 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.3214181884411916e-05
  Dropout: 0.187528128388681
================================================================================

[I 2025-11-06 13:21:29,259] Trial 4633 pruned. Pruned at step 27 with metric 0.6165

================================================================================
TRIAL 4636 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.175696595212617e-05
  Dropout: 0.22027661726462744
================================================================================

[I 2025-11-06 13:38:05,208] Trial 4636 pruned. Pruned at step 27 with metric 0.6407
[I 2025-11-06 13:38:06,045] Trial 4637 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4638 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.61502923315594e-06
  Dropout: 0.21375295641724404
================================================================================

[I 2025-11-06 13:42:47,461] Trial 4638 pruned. OOM: bert-base-uncased bs=64 len=384
[I 2025-11-06 13:42:48,060] Trial 4639 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

[OOM] Trial 4638 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 384 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 78.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4640 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.540713231544847e-06
  Dropout: 0.4053085449366612
================================================================================

[I 2025-11-06 13:52:58,125] Trial 4635 pruned. Pruned at step 13 with metric 0.5748
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4641 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.1432205055987477e-05
  Dropout: 0.11492699113315458
================================================================================

[I 2025-11-06 13:53:43,272] Trial 4640 pruned. Pruned at step 9 with metric 0.6643
[I 2025-11-06 13:53:43,943] Trial 4642 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4643 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 7.092888230529817e-06
  Dropout: 0.2980171718744649
================================================================================

[I 2025-11-06 14:05:00,118] Trial 4641 pruned. Pruned at step 9 with metric 0.6450

================================================================================
TRIAL 4644 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.755526623346537e-06
  Dropout: 0.2073270229469119
================================================================================

[I 2025-11-06 14:25:39,552] Trial 4643 finished with value: 0.6914487731457174 and parameters: {'seed': 62123, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 7.092888230529817e-06, 'optim.weight_decay': 3.996911312994185e-05, 'optim.beta1': 0.846837181919289, 'optim.beta2': 0.9812314822285314, 'optim.eps': 2.8991711509814844e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.15528891876632428, 'sched.poly_power': 1.2596280723743951, 'train.clip_grad': 0.9638989172829291, 'model.dropout': 0.2980171718744649, 'model.attn_dropout': 0.08620562915782881, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9709945627345009, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.46728632470885767, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.934265010930471, 'loss.cls.alpha': 0.6887140225460389, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-06 14:25:40,112] The parameter `tok.doc_stride` in Trial#4645 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4645 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.022207127862901e-06
  Dropout: 0.49682925899917485
================================================================================

[I 2025-11-06 14:29:05,484] Trial 4645 pruned. Pruned at step 13 with metric 0.6630
[I 2025-11-06 14:29:06,093] Trial 4646 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-06 14:29:06,666] Trial 4647 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4648 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 9.274703351455546e-06
  Dropout: 0.20949041088547962
================================================================================

[I 2025-11-06 14:34:07,296] Trial 4644 finished with value: 0.7010416666666667 and parameters: {'seed': 56800, 'model.name': 'bert-base-uncased', 'tok.max_length': 320, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 7.755526623346537e-06, 'optim.weight_decay': 1.9839234892408248e-06, 'optim.beta1': 0.8526029190925571, 'optim.beta2': 0.9927397537380842, 'optim.eps': 2.1569153171022505e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.13295709605033637, 'train.clip_grad': 1.031925671322343, 'model.dropout': 0.2073270229469119, 'model.attn_dropout': 0.10830234645347804, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.980470918406096, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.4976669579131848, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.6762865387944266, 'loss.cls.alpha': 0.848305967617331, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4649 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 4.8960469988928186e-05
  Dropout: 0.08429876228584175
================================================================================

[I 2025-11-06 15:03:46,368] Trial 4649 finished with value: 0.43989071038251365 and parameters: {'seed': 36774, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 4.8960469988928186e-05, 'optim.weight_decay': 1.4338143588254556e-05, 'optim.beta1': 0.879025335037872, 'optim.beta2': 0.9721270720705676, 'optim.eps': 3.531738502972362e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0007833401195717626, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9066966308318598, 'model.dropout': 0.08429876228584175, 'model.attn_dropout': 0.09712850183275774, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8235024325789414, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3782598214304012, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16557470906641375, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-06 15:03:46,917] The parameter `tok.doc_stride` in Trial#4650 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4650 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.0615184515130221e-05
  Dropout: 0.391604271616661
================================================================================

[I 2025-11-06 15:04:00,465] Trial 4648 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4648 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 82.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4651 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 6.6441109883015066e-06
  Dropout: 0.2805799757206063
================================================================================

[I 2025-11-06 15:10:45,554] Trial 4651 pruned. Pruned at step 14 with metric 0.5957

================================================================================
TRIAL 4652 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1242439269957985e-05
  Dropout: 0.31650315953691804
================================================================================

[I 2025-11-06 15:18:30,159] Trial 4652 pruned. Pruned at step 8 with metric 0.5742
[I 2025-11-06 15:18:30,762] Trial 4653 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4654 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 6.009708528288795e-06
  Dropout: 0.4736863369338832
================================================================================

[I 2025-11-06 15:28:50,702] Trial 4650 finished with value: 0.7031853281853282 and parameters: {'seed': 22394, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 32, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.0615184515130221e-05, 'optim.weight_decay': 0.0022414199896983312, 'optim.beta1': 0.8755259017941135, 'optim.beta2': 0.9653599759893449, 'optim.eps': 2.6877720968973108e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04771708994353765, 'sched.poly_power': 1.2378875132183689, 'train.clip_grad': 1.4297367506730587, 'model.dropout': 0.391604271616661, 'model.attn_dropout': 0.21457931846182715, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.953878817330714, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.29547255659885296, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.18277509152253424, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 52 (patience=20)

================================================================================
TRIAL 4655 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.0764943236110196e-05
  Dropout: 0.16269860765107305
================================================================================

[I 2025-11-06 15:33:00,635] Trial 4654 pruned. Pruned at step 14 with metric 0.5997
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4656 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 8.027701061479546e-06
  Dropout: 0.4432077783377404
================================================================================

[I 2025-11-06 15:47:53,672] Trial 4656 pruned. Pruned at step 11 with metric 0.6556
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4657 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 7.288044761536716e-06
  Dropout: 0.3308469834622226
================================================================================

[I 2025-11-06 16:05:34,615] Trial 4657 pruned. Pruned at step 14 with metric 0.5997
[I 2025-11-06 16:05:35,274] Trial 4658 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-06 16:05:35,867] Trial 4659 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[W 2025-11-06 16:05:36,400] The parameter `tok.doc_stride` in Trial#4660 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4660 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.729981608811087e-05
  Dropout: 0.342018027112319
================================================================================

[I 2025-11-06 16:08:08,999] Trial 4660 pruned. Pruned at step 10 with metric 0.5783

================================================================================
TRIAL 4661 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 3.5052999928402376e-05
  Dropout: 0.12410128588201896
================================================================================

[I 2025-11-06 16:20:38,497] Trial 4661 pruned. Pruned at step 27 with metric 0.6447
[W 2025-11-06 16:20:39,051] The parameter `tok.doc_stride` in Trial#4662 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4662 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.23264545901659e-05
  Dropout: 0.06985872059449795
================================================================================

[I 2025-11-06 16:28:23,518] Trial 4662 finished with value: 0.6650326797385622 and parameters: {'seed': 41312, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.23264545901659e-05, 'optim.weight_decay': 4.4196767996805564e-05, 'optim.beta1': 0.9367868420806805, 'optim.beta2': 0.9882625796290909, 'optim.eps': 1.1836574410485414e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.010536901852826363, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.0263691190046034, 'model.dropout': 0.06985872059449795, 'model.attn_dropout': 0.12069761218773095, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8186479151643705, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3278824223033562, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1993956713325257, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 16:28:24,125] Trial 4663 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-06 16:28:24,715] Trial 4664 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 4665 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 5.911306630246711e-06
  Dropout: 0.4358697427318855
================================================================================

[I 2025-11-06 16:45:05,147] Trial 4665 finished with value: 0.7143710191082803 and parameters: {'seed': 62955, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.911306630246711e-06, 'optim.weight_decay': 0.0049712199856274284, 'optim.beta1': 0.8737323029664066, 'optim.beta2': 0.9736596039600907, 'optim.eps': 1.5907577316437143e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.12516542631166336, 'train.clip_grad': 0.13900097155158952, 'model.dropout': 0.4358697427318855, 'model.attn_dropout': 0.026806123139177088, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9438568886497016, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.3661929200368417, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.195301037510453, 'loss.cls.alpha': 0.7599526277666185, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 83 (patience=20)

================================================================================
TRIAL 4666 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.3777079615274052e-05
  Dropout: 0.1254538100127539
================================================================================

[I 2025-11-06 17:00:30,855] Trial 4666 finished with value: 0.4429347826086957 and parameters: {'seed': 37552, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.3777079615274052e-05, 'optim.weight_decay': 0.0002411533171448946, 'optim.beta1': 0.8688760564116703, 'optim.beta2': 0.9842068375855194, 'optim.eps': 1.0953474299424378e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.07362999918797093, 'train.clip_grad': 1.3554224662902874, 'model.dropout': 0.1254538100127539, 'model.attn_dropout': 0.13657075244053776, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8310096605157375, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.4296327570846875, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.19974126095424796, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4667 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.2575734057681016e-05
  Dropout: 0.4963596610636811
================================================================================

[I 2025-11-06 17:09:08,790] Trial 4667 pruned. Pruned at step 7 with metric 0.5349
[I 2025-11-06 17:09:09,456] Trial 4668 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-06 17:09:09,996] The parameter `tok.doc_stride` in Trial#4669 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4669 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0204208425920865e-05
  Dropout: 0.12118307731869875
================================================================================

[I 2025-11-06 17:13:59,370] Trial 4669 pruned. Pruned at step 20 with metric 0.6052
[I 2025-11-06 17:13:59,972] Trial 4670 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4671 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 6.278381871047392e-06
  Dropout: 0.37378475876498
================================================================================

[I 2025-11-06 17:14:07,071] Trial 4671 pruned. OOM: microsoft/deberta-v3-large bs=12 len=384
[I 2025-11-06 17:14:07,254] Trial 4655 pruned. OOM: microsoft/deberta-v3-base bs=8 len=320

[OOM] Trial 4671 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 88.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4655 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 62.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4672 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 7.0094351518051864e-06
  Dropout: 0.3467165575288455
================================================================================


================================================================================
TRIAL 4673 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.096462361888022e-05
  Dropout: 0.14657795477074245
================================================================================

[I 2025-11-06 17:17:53,548] Trial 4672 pruned. Pruned at step 11 with metric 0.5545

================================================================================
TRIAL 4674 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 7.238732023847873e-05
  Dropout: 0.07384404238423903
================================================================================

[I 2025-11-06 17:32:27,481] Trial 4673 pruned. Pruned at step 8 with metric 0.6197
[I 2025-11-06 17:32:28,153] Trial 4675 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-06 17:32:28,722] Trial 4676 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4677 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.3204464827835654e-05
  Dropout: 0.45586309804903874
================================================================================

[I 2025-11-06 17:53:21,601] Trial 4677 pruned. Pruned at step 27 with metric 0.5510

================================================================================
TRIAL 4678 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.2265133310726179e-05
  Dropout: 0.31099818510033544
================================================================================

[I 2025-11-06 17:59:00,719] Trial 4678 pruned. Pruned at step 18 with metric 0.6242
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4679 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 2.555045358736719e-05
  Dropout: 0.29672301393774325
================================================================================

[I 2025-11-06 17:59:08,538] Trial 4679 pruned. OOM: microsoft/deberta-v3-large bs=16 len=256
[I 2025-11-06 17:59:08,585] Trial 4674 pruned. OOM: bert-large-uncased bs=8 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4679 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 58.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4674 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 58.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4680 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 9.0834334277809e-06
  Dropout: 0.13457585174440453
================================================================================


================================================================================
TRIAL 4681 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 9.818764428241394e-06
  Dropout: 0.4674133899133007
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-06 18:09:46,253] Trial 4681 pruned. Pruned at step 9 with metric 0.6232
[I 2025-11-06 18:09:46,881] Trial 4682 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4683 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 7.423381914057597e-05
  Dropout: 0.0876755101017455
================================================================================

[I 2025-11-06 18:52:20,736] Trial 4680 pruned. Pruned at step 30 with metric 0.5521
[I 2025-11-06 18:52:21,360] Trial 4684 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4685 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.4389651107726244e-05
  Dropout: 0.11929517497230901
================================================================================

[I 2025-11-06 18:56:52,180] Trial 4685 pruned. Pruned at step 10 with metric 0.6517
[W 2025-11-06 18:56:52,739] The parameter `tok.doc_stride` in Trial#4686 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4686 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.307755462889705e-05
  Dropout: 0.2992892203247006
================================================================================

[I 2025-11-06 19:05:38,897] Trial 4683 finished with value: 0.4474393530997305 and parameters: {'seed': 61507, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 7.423381914057597e-05, 'optim.weight_decay': 3.890791789681147e-05, 'optim.beta1': 0.9024169382178546, 'optim.beta2': 0.9855528123539748, 'optim.eps': 4.861558667327055e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.030364904177293508, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2955472001438664, 'model.dropout': 0.0876755101017455, 'model.attn_dropout': 0.19981396825461156, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8261275259339513, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.48218376664266865, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16401093988892246, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4687 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.141730255116028e-05
  Dropout: 0.04839081411554534
================================================================================

[I 2025-11-06 19:16:49,662] Trial 4686 pruned. Pruned at step 17 with metric 0.5695
[I 2025-11-06 19:16:50,345] Trial 4688 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4689 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 8.706631681562144e-06
  Dropout: 0.36314024521335136
================================================================================

[I 2025-11-06 19:18:10,861] Trial 4687 pruned. Pruned at step 9 with metric 0.6285
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4690 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 5.412880370243144e-06
  Dropout: 0.4262102730746781
================================================================================

[I 2025-11-06 19:34:36,410] Trial 4690 pruned. Pruned at step 11 with metric 0.6274
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4691 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 4.5031993707995544e-05
  Dropout: 0.37509702793489547
================================================================================

[I 2025-11-06 19:35:52,056] Trial 4689 pruned. Pruned at step 27 with metric 0.6215

================================================================================
TRIAL 4692 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.3604713451418035e-05
  Dropout: 0.44625459276346796
================================================================================

[I 2025-11-06 19:52:32,786] Trial 4692 finished with value: 0.6780284043441938 and parameters: {'seed': 41094, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.3604713451418035e-05, 'optim.weight_decay': 4.217011921948688e-05, 'optim.beta1': 0.9373780486931036, 'optim.beta2': 0.9611462060838615, 'optim.eps': 6.660864374775571e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.044619804141552574, 'sched.cosine_cycles': 4, 'train.clip_grad': 1.4407860869474018, 'model.dropout': 0.44625459276346796, 'model.attn_dropout': 0.22475317379500936, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9255509528411093, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.3639848326382584, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.271946687125619, 'loss.cls.alpha': 0.6383976533990086, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 4693 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.190056208350669e-06
  Dropout: 0.03799923027431551
================================================================================

[I 2025-11-06 20:00:16,364] Trial 4693 pruned. Pruned at step 9 with metric 0.6514
[I 2025-11-06 20:00:17,141] Trial 4694 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4695 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.1132233762091563e-05
  Dropout: 0.43568150279572604
================================================================================

[I 2025-11-06 20:00:25,236] Trial 4695 pruned. OOM: roberta-base bs=64 len=384
[I 2025-11-06 20:00:25,288] Trial 4691 pruned. OOM: microsoft/deberta-v3-base bs=8 len=384
[I 2025-11-06 20:00:26,477] Trial 4697 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4695 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 84.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4691 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 8 (effective: 24 with grad_accum=3)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 44.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4696 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.20418491264784e-06
  Dropout: 0.212341348780285
================================================================================


================================================================================
TRIAL 4698 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.4388667013841928e-05
  Dropout: 0.013629822369163727
================================================================================

/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[I 2025-11-06 20:00:33,529] Trial 4698 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384
[W 2025-11-06 20:00:34,263] The parameter `tok.doc_stride` in Trial#4699 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 4698 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 56.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4699 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 0.00014999112589322568
  Dropout: 0.1868698098922732
================================================================================

[I 2025-11-06 20:06:21,287] Trial 4699 pruned. Pruned at step 19 with metric 0.5695
[W 2025-11-06 20:06:21,870] The parameter `tok.doc_stride` in Trial#4700 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-06 20:06:21,925] Trial 4700 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4701 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.5165206949238463e-05
  Dropout: 0.4927191045410339
================================================================================

[I 2025-11-06 20:06:31,344] Trial 4701 pruned. OOM: microsoft/deberta-v3-large bs=12 len=224
[I 2025-11-06 20:06:33,100] Trial 4696 pruned. OOM: roberta-large bs=8 len=224

[OOM] Trial 4701 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 44.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4702 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.1698320872688064e-05
  Dropout: 0.292077844706237
================================================================================


[OOM] Trial 4696 exceeded GPU memory:
  Model: roberta-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 44.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4703 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.161970946175576e-05
  Dropout: 0.42566721337462954
================================================================================

[I 2025-11-06 20:27:45,370] Trial 4703 pruned. Pruned at step 16 with metric 0.6294
[I 2025-11-06 20:27:46,072] Trial 4704 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4705 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.886608650945494e-05
  Dropout: 0.07604305291552577
================================================================================

[I 2025-11-06 20:33:56,017] Trial 4705 pruned. Pruned at step 27 with metric 0.6398
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4706 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.288579981299291e-06
  Dropout: 0.3766027928846767
================================================================================

[I 2025-11-06 20:49:13,495] Trial 4706 finished with value: 0.7010416666666667 and parameters: {'seed': 42948, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 9.288579981299291e-06, 'optim.weight_decay': 0.18703129892036827, 'optim.beta1': 0.8999042720907613, 'optim.beta2': 0.989806360229121, 'optim.eps': 5.46877534827934e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.11187454515694618, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.30167556597104683, 'model.dropout': 0.3766027928846767, 'model.attn_dropout': 0.19744025470930285, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8429501218162019, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.12812532864894513, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.443388003306759, 'loss.cls.alpha': 0.629577250016432, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 20:49:14,269] Trial 4707 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 37 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 250 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 250 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 4708 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 5.920256663638199e-05
  Dropout: 0.23425121482054584
================================================================================

[I 2025-11-06 20:51:12,950] Trial 4708 pruned. Pruned at step 10 with metric 0.6388
[I 2025-11-06 20:51:13,744] Trial 4709 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
[GPU RESET] Performing periodic GPU reset after 250 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 250 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 4710 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 3.344285815018689e-05
  Dropout: 0.16526493290772953
================================================================================

[I 2025-11-06 21:22:45,856] Trial 4710 finished with value: 0.44141689373297005 and parameters: {'seed': 58885, 'model.name': 'bert-large-uncased', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.344285815018689e-05, 'optim.weight_decay': 1.581614004018655e-06, 'optim.beta1': 0.8925460078511042, 'optim.beta2': 0.9838725639481353, 'optim.eps': 4.256142344200871e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.022556978847969256, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.407723699154144, 'model.dropout': 0.16526493290772953, 'model.attn_dropout': 0.10488502562852586, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8992727981816583, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.31076798247918125, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.983419329669046, 'loss.cls.alpha': 0.8580114991345773, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4711 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.154098479699962e-05
  Dropout: 0.08207464982374699
================================================================================

[I 2025-11-06 21:26:13,990] Trial 4711 pruned. Pruned at step 13 with metric 0.6231
[I 2025-11-06 21:26:14,593] Trial 4712 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4713 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.96937046930919e-06
  Dropout: 0.4991142911158012
================================================================================

[I 2025-11-06 21:35:08,523] Trial 4713 pruned. Pruned at step 15 with metric 0.5827

================================================================================
TRIAL 4714 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.370603483777087e-06
  Dropout: 0.45648691995931673
================================================================================

[I 2025-11-06 21:37:57,657] Trial 4702 pruned. Pruned at step 27 with metric 0.5804
[I 2025-11-06 21:37:58,354] Trial 4715 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4716 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.5691138858160592e-05
  Dropout: 0.4068203480818497
================================================================================

[I 2025-11-06 21:42:11,482] Trial 4714 pruned. Pruned at step 18 with metric 0.6294
[I 2025-11-06 21:42:12,105] Trial 4717 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4718 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 4.0098119267055136e-05
  Dropout: 0.003081819637434885
================================================================================

[I 2025-11-06 21:42:23,336] Trial 4716 pruned. OOM: roberta-base bs=32 len=256
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4716 exceeded GPU memory:
  Model: roberta-base
  Batch size: 32 (effective: 128 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4719 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 8.613870699570995e-06
  Dropout: 0.11520306152270167
================================================================================

[I 2025-11-06 21:42:28,962] Trial 4719 pruned. OOM: roberta-base bs=48 len=128
[I 2025-11-06 21:42:30,220] Trial 4718 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4719 exceeded GPU memory:
  Model: roberta-base
  Batch size: 48 (effective: 192 with grad_accum=4)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 86.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4718 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 86.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4720 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 8.064565807726417e-06
  Dropout: 0.42424258037473883
================================================================================


================================================================================
TRIAL 4721 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.2565918322176695e-06
  Dropout: 0.042318746766733195
================================================================================

[I 2025-11-06 21:42:39,035] Trial 4721 pruned. OOM: roberta-base bs=16 len=320
[I 2025-11-06 21:42:40,285] Trial 4720 pruned. OOM: microsoft/deberta-v3-large bs=12 len=256

[OOM] Trial 4721 exceeded GPU memory:
  Model: roberta-base
  Batch size: 16 (effective: 96 with grad_accum=6)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 34.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4720 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 34.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4722 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.412107021481358e-05
  Dropout: 0.1674960207481252
================================================================================


================================================================================
TRIAL 4723 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.386880895902579e-05
  Dropout: 0.19028767907934446
================================================================================

[I 2025-11-06 21:53:34,401] Trial 4723 finished with value: 0.6472140426020115 and parameters: {'seed': 44192, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.386880895902579e-05, 'optim.weight_decay': 2.1258787325458967e-05, 'optim.beta1': 0.8830684742767405, 'optim.beta2': 0.9809682839248374, 'optim.eps': 3.237006644347711e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.033119126212847017, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.1303370315504735, 'model.dropout': 0.19028767907934446, 'model.attn_dropout': 0.15915296836858767, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8256692217347316, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.3539126473989811, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19969654550714522, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4724 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 4.552968297226681e-05
  Dropout: 0.14323946747650518
================================================================================

[I 2025-11-06 21:56:55,353] Trial 4722 pruned. Pruned at step 7 with metric 0.5127
[W 2025-11-06 21:56:56,028] The parameter `tok.doc_stride` in Trial#4725 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4725 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.7766395480313152e-05
  Dropout: 0.07648897940472449
================================================================================

[I 2025-11-06 22:24:12,337] Trial 4724 finished with value: 0.4444444444444444 and parameters: {'seed': 7560, 'model.name': 'bert-large-uncased', 'tok.max_length': 320, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 4.552968297226681e-05, 'optim.weight_decay': 0.0014958237897816046, 'optim.beta1': 0.8500838052766317, 'optim.beta2': 0.9793458950890737, 'optim.eps': 1.84083973714403e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.11629003392215795, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.7733156735808273, 'model.dropout': 0.14323946747650518, 'model.attn_dropout': 0.2087946249166081, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8586119426222203, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.21019192474516907, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16602487002099425, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-06 22:24:12,948] Trial 4726 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4727 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.147403925862432e-06
  Dropout: 0.4090909609822292
================================================================================

[I 2025-11-06 22:25:11,903] Trial 4725 finished with value: 0.42896935933147634 and parameters: {'seed': 24185, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.7766395480313152e-05, 'optim.weight_decay': 7.874781227897107e-06, 'optim.beta1': 0.9071466443180102, 'optim.beta2': 0.9849584169759676, 'optim.eps': 2.5904609408152355e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.018140208643225512, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.997141860821158, 'model.dropout': 0.07648897940472449, 'model.attn_dropout': 0.10780642818514799, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8174444745606997, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.4669541665719618, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18159684150252137, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4728 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.3957386022546995e-05
  Dropout: 0.015905154368596405
================================================================================

[I 2025-11-06 22:36:41,097] Trial 4728 finished with value: 0.4444444444444444 and parameters: {'seed': 64491, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.3957386022546995e-05, 'optim.weight_decay': 0.002862588852596037, 'optim.beta1': 0.8321537584977576, 'optim.beta2': 0.9609157679145126, 'optim.eps': 1.0736143561101312e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1115737422635444, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.19144725835883902, 'model.dropout': 0.015905154368596405, 'model.attn_dropout': 0.12447255314240392, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8660384268313284, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.09191002691920235, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.329395515872437, 'loss.cls.alpha': 0.7289354112119213, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4729 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.6386760541049536e-05
  Dropout: 0.1354928737329
================================================================================

[I 2025-11-06 22:41:14,609] Trial 4729 pruned. Pruned at step 10 with metric 0.5712
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4730 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.6393662505238318e-05
  Dropout: 0.4055402808098433
================================================================================

[I 2025-11-06 22:55:12,944] Trial 4730 pruned. Pruned at step 10 with metric 0.6199
[I 2025-11-06 22:55:13,608] Trial 4731 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4732 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.1811261760403034e-05
  Dropout: 0.4217196252688979
================================================================================

[I 2025-11-06 23:04:04,362] Trial 4732 pruned. Pruned at step 14 with metric 0.6174

================================================================================
TRIAL 4733 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.236777091829616e-06
  Dropout: 0.44379814742612783
================================================================================

[I 2025-11-06 23:21:16,262] Trial 4733 finished with value: 0.6416812324702248 and parameters: {'seed': 51284, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.236777091829616e-06, 'optim.weight_decay': 0.00014619728199519234, 'optim.beta1': 0.8867130795663971, 'optim.beta2': 0.9870406717578607, 'optim.eps': 1.6983925120783845e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.06095344340322452, 'train.clip_grad': 0.21646828602678292, 'model.dropout': 0.44379814742612783, 'model.attn_dropout': 0.03493805403243868, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9389476274047739, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.2031402655996935, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.17920984068347906, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 80 (patience=20)

================================================================================
TRIAL 4734 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.2161595566550165e-05
  Dropout: 0.44338827235861056
================================================================================

[I 2025-11-06 23:24:17,810] Trial 4734 pruned. Pruned at step 9 with metric 0.5884
[I 2025-11-06 23:24:18,405] Trial 4735 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-06 23:24:18,987] Trial 4736 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4737 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.6603394567228526e-05
  Dropout: 0.2518857194689638
================================================================================

[I 2025-11-06 23:27:05,471] Trial 4727 finished with value: 0.6899159663865546 and parameters: {'seed': 39572, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.147403925862432e-06, 'optim.weight_decay': 0.0006087325849129222, 'optim.beta1': 0.8793735371395441, 'optim.beta2': 0.9989473834220893, 'optim.eps': 1.3237989212973098e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.180894040531559, 'train.clip_grad': 0.5202024650827259, 'model.dropout': 0.4090909609822292, 'model.attn_dropout': 0.14123140892182978, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9387509543865415, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.10026153173863966, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.426871027114687, 'loss.cls.alpha': 0.7262231325549321, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-06 23:27:06,035] The parameter `tok.doc_stride` in Trial#4738 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 4738 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.3816067842321982e-05
  Dropout: 0.07321373002361937
================================================================================

[I 2025-11-06 23:36:18,435] Trial 4738 pruned. Pruned at step 9 with metric 0.5124
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4739 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.3139283512161182e-05
  Dropout: 0.11788971121685018
================================================================================

[I 2025-11-06 23:43:10,615] Trial 4737 pruned. Pruned at step 15 with metric 0.5880
[I 2025-11-06 23:43:11,300] Trial 4740 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4741 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.055555807921879e-06
  Dropout: 0.3328693374578262
================================================================================

[I 2025-11-06 23:45:37,427] Trial 4739 pruned. Pruned at step 7 with metric 0.5666
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4742 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.7117291741186617e-05
  Dropout: 0.17936800788792376
================================================================================

[I 2025-11-06 23:45:44,857] Trial 4741 pruned. Pruned at step 13 with metric 0.6210
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4743 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.092904629507632e-06
  Dropout: 0.46284431682710736
================================================================================

[I 2025-11-07 00:31:02,783] Trial 4742 finished with value: 0.4444444444444444 and parameters: {'seed': 312, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.7117291741186617e-05, 'optim.weight_decay': 1.0448114217264831e-05, 'optim.beta1': 0.8291254333822385, 'optim.beta2': 0.9851558519108287, 'optim.eps': 1.250046229204171e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.18262815162791216, 'train.clip_grad': 1.1384442700134534, 'model.dropout': 0.17936800788792376, 'model.attn_dropout': 0.047790604945287765, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8681383662175121, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.3480608216307283, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1574551142628006, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-07 00:31:03,362] The parameter `tok.doc_stride` in Trial#4744 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4744 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0731669524961655e-05
  Dropout: 0.40145092068493743
================================================================================

[I 2025-11-07 00:35:17,434] Trial 4744 pruned. Pruned at step 9 with metric 0.5584
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4745 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.1306527009922244e-05
  Dropout: 0.2547303646346542
================================================================================

[I 2025-11-07 00:44:08,192] Trial 4745 pruned. Pruned at step 16 with metric 0.6517

================================================================================
TRIAL 4746 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.7462352787655224e-05
  Dropout: 0.26155015782441504
================================================================================

[I 2025-11-07 00:58:26,029] Trial 4746 finished with value: 0.6846035498517768 and parameters: {'seed': 58810, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 3.7462352787655224e-05, 'optim.weight_decay': 0.00017644306969313752, 'optim.beta1': 0.903937373036052, 'optim.beta2': 0.9872000718355857, 'optim.eps': 4.9920999570316834e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.06113839256095203, 'train.clip_grad': 1.0493726836544832, 'model.dropout': 0.26155015782441504, 'model.attn_dropout': 0.1677866397403215, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8143258388651221, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.4811873189306649, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.6484191866933737, 'loss.cls.alpha': 0.7687627736622089, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 00:58:26,628] Trial 4747 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 53 (patience=20)

================================================================================
TRIAL 4748 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.1445225848889837e-05
  Dropout: 0.27748430804394286
================================================================================

[I 2025-11-07 01:30:06,720] Trial 4743 finished with value: 0.6491542288557214 and parameters: {'seed': 63764, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.092904629507632e-06, 'optim.weight_decay': 0.001766353627043357, 'optim.beta1': 0.8830436804646025, 'optim.beta2': 0.9605639246438281, 'optim.eps': 1.7507465450651874e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.032769877372633746, 'sched.poly_power': 1.1328305058679418, 'train.clip_grad': 1.4440400882371012, 'model.dropout': 0.46284431682710736, 'model.attn_dropout': 0.21160975607723387, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9560950973804011, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.44650500296217244, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.195945808066431, 'loss.cls.alpha': 0.6671671889303099, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 63 (patience=20)

================================================================================
TRIAL 4749 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.5453493414163337e-05
  Dropout: 0.14592441364706107
================================================================================

[I 2025-11-07 01:40:17,467] Trial 4749 finished with value: 0.6691906653426019 and parameters: {'seed': 61772, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.5453493414163337e-05, 'optim.weight_decay': 0.12589333928382374, 'optim.beta1': 0.9216619842765569, 'optim.beta2': 0.9687580954033546, 'optim.eps': 8.442424631540563e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.16511461104835962, 'train.clip_grad': 0.44614002094312655, 'model.dropout': 0.14592441364706107, 'model.attn_dropout': 0.26092731627302684, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8044733406005509, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.11746387205705278, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.986447863129501, 'loss.cls.alpha': 0.35443729899532017, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-07 01:40:18,035] The parameter `tok.doc_stride` in Trial#4750 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 4750 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.2336803968717451e-05
  Dropout: 0.4847393402116603
================================================================================

[I 2025-11-07 01:48:40,322] Trial 4750 pruned. Pruned at step 30 with metric 0.6530
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4751 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.676234461708641e-06
  Dropout: 0.33952974352790444
================================================================================

[I 2025-11-07 02:35:05,489] Trial 4751 finished with value: 0.6770975532920298 and parameters: {'seed': 65267, 'model.name': 'roberta-large', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.676234461708641e-06, 'optim.weight_decay': 1.9044945890046305e-05, 'optim.beta1': 0.8515417882986319, 'optim.beta2': 0.9746525204038383, 'optim.eps': 7.726783927275277e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18368351213473838, 'sched.poly_power': 1.3802579602822012, 'train.clip_grad': 1.145741024761114, 'model.dropout': 0.33952974352790444, 'model.attn_dropout': 0.08845189563259853, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9056605287390253, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4608730593240677, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.15948857815067807, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4752 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 6.414359232642098e-06
  Dropout: 0.3431254802900532
================================================================================

[I 2025-11-07 02:48:01,516] Trial 4752 pruned. Pruned at step 15 with metric 0.5732
[I 2025-11-07 02:48:02,320] Trial 4753 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-07 02:48:02,911] Trial 4754 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4755 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.4015516857821648e-05
  Dropout: 0.007360473850481138
================================================================================

[I 2025-11-07 02:56:59,403] Trial 4755 finished with value: 0.7014890318944056 and parameters: {'seed': 63833, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.4015516857821648e-05, 'optim.weight_decay': 2.8933441538848897e-06, 'optim.beta1': 0.8690911954109756, 'optim.beta2': 0.9631389239439925, 'optim.eps': 4.275531901290505e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.020344914998548475, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.1431471504185247, 'model.dropout': 0.007360473850481138, 'model.attn_dropout': 0.13639777789598373, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8606479022750607, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.19051899096477315, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.98029752014195, 'loss.cls.alpha': 0.5655627611571334, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 4756 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.2671986849475927e-05
  Dropout: 0.07150898645727735
================================================================================

[I 2025-11-07 03:16:29,405] Trial 4748 finished with value: 0.7354838709677419 and parameters: {'seed': 54951, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.1445225848889837e-05, 'optim.weight_decay': 3.444292570067465e-05, 'optim.beta1': 0.8867576009044087, 'optim.beta2': 0.9735847913473301, 'optim.eps': 3.7556652543209665e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.17139424692966726, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2225350202554648, 'model.dropout': 0.27748430804394286, 'model.attn_dropout': 0.2637863523021037, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9301362103011024, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.3984505369587721, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.057349788162143, 'loss.cls.alpha': 0.504249443979358, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 50 (patience=20)

================================================================================
TRIAL 4757 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.4763825253871696e-05
  Dropout: 0.4981648310550578
================================================================================

[I 2025-11-07 03:17:30,969] Trial 4756 finished with value: 0.46194225721784776 and parameters: {'seed': 863, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.2671986849475927e-05, 'optim.weight_decay': 0.00010303311627822067, 'optim.beta1': 0.8235229759805461, 'optim.beta2': 0.9721629128620382, 'optim.eps': 5.781155096605239e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03631961747220786, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.0154205135407057, 'model.dropout': 0.07150898645727735, 'model.attn_dropout': 0.22627042668356787, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.815078311554699, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.22861250154740403, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19902021385869087, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4758 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.34245658136359e-05
  Dropout: 0.3340695532506426
================================================================================

[I 2025-11-07 03:23:32,819] Trial 4758 pruned. Pruned at step 12 with metric 0.6257

================================================================================
TRIAL 4759 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4828318285693233e-05
  Dropout: 0.20067991931446338
================================================================================

[I 2025-11-07 03:28:07,551] Trial 4759 pruned. Pruned at step 9 with metric 0.6107
[I 2025-11-07 03:28:08,150] Trial 4760 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-07 03:28:08,719] Trial 4761 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4762 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.0037557440265389e-05
  Dropout: 0.04114912905075434
================================================================================

[I 2025-11-07 03:45:01,032] Trial 4762 finished with value: 0.4444444444444444 and parameters: {'seed': 64863, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.0037557440265389e-05, 'optim.weight_decay': 3.28589244158006e-06, 'optim.beta1': 0.9497966250038395, 'optim.beta2': 0.9856977197525095, 'optim.eps': 8.151214793537061e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.016253439487019555, 'sched.poly_power': 1.5737245009502046, 'train.clip_grad': 1.4248556910807526, 'model.dropout': 0.04114912905075434, 'model.attn_dropout': 0.22299208839426354, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8226263338291153, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.28400986104733533, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.4774431122152345, 'loss.cls.alpha': 0.7642106351401228, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 03:45:01,631] Trial 4763 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4764 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.110075169205588e-05
  Dropout: 0.17485677293159962
================================================================================

[I 2025-11-07 03:54:13,568] Trial 4764 pruned. Pruned at step 14 with metric 0.6083
[I 2025-11-07 03:54:14,365] Trial 4765 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-07 03:54:14,940] Trial 4766 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4767 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.107892583133445e-05
  Dropout: 0.03264223026053287
================================================================================

[I 2025-11-07 03:56:52,507] Trial 4757 pruned. Pruned at step 12 with metric 0.6208

================================================================================
TRIAL 4768 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.3939057739513113e-05
  Dropout: 0.03488649725427265
================================================================================

[I 2025-11-07 04:00:55,314] Trial 4767 pruned. Pruned at step 9 with metric 0.6014
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4769 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.6690093184489362e-05
  Dropout: 0.4023838588245164
================================================================================

[I 2025-11-07 04:12:53,404] Trial 4769 pruned. Pruned at step 16 with metric 0.6242
[I 2025-11-07 04:12:54,003] Trial 4770 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4771 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.4368406214828546e-06
  Dropout: 0.09880786266448817
================================================================================

[I 2025-11-07 04:19:15,745] Trial 4771 pruned. Pruned at step 9 with metric 0.6564
[I 2025-11-07 04:19:16,352] Trial 4772 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-07 04:19:16,931] Trial 4773 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-07 04:19:17,468] The parameter `tok.doc_stride` in Trial#4774 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4774 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 8.604198054098034e-06
  Dropout: 0.4410754518995684
================================================================================

[I 2025-11-07 04:33:48,276] Trial 4768 pruned. Pruned at step 13 with metric 0.6027
[I 2025-11-07 04:33:48,889] Trial 4775 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4776 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.4315843684116016e-05
  Dropout: 0.4282382077764927
================================================================================

[I 2025-11-07 04:40:02,203] Trial 4774 pruned. Pruned at step 26 with metric 0.5884
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4777 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.7674100243153526e-05
  Dropout: 0.21826272909387584
================================================================================

[I 2025-11-07 05:04:05,618] Trial 4777 finished with value: 0.450402144772118 and parameters: {'seed': 10655, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.7674100243153526e-05, 'optim.weight_decay': 0.0009632691620818759, 'optim.beta1': 0.9244750529365452, 'optim.beta2': 0.9847477361366138, 'optim.eps': 1.623097885948183e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.06952376607334221, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4863377677196017, 'model.dropout': 0.21826272909387584, 'model.attn_dropout': 0.1477791341093851, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8190900411954831, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.33446660020984975, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7978009891035502, 'loss.cls.alpha': 0.41114719328507726, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4778 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 7.980264669195609e-06
  Dropout: 0.19287986113651379
================================================================================

[I 2025-11-07 06:03:50,756] Trial 4776 finished with value: 0.7437499999999999 and parameters: {'seed': 63237, 'model.name': 'roberta-large', 'tok.max_length': 352, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.4315843684116016e-05, 'optim.weight_decay': 0.060835139158766066, 'optim.beta1': 0.8646167602296374, 'optim.beta2': 0.9906114521699608, 'optim.eps': 3.129733261054422e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.15087958398489776, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.4287366525843911, 'model.dropout': 0.4282382077764927, 'model.attn_dropout': 0.1502395902573199, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8147479358723742, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.03218689293892431, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1596083952696287, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 4779 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.790715654440193e-06
  Dropout: 0.493816932253857
================================================================================

[I 2025-11-07 06:09:24,622] Trial 4779 pruned. Pruned at step 8 with metric 0.5045
[I 2025-11-07 06:09:25,260] Trial 4780 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-07 06:09:25,852] Trial 4781 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-07 06:09:26,458] Trial 4782 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-07 06:09:27,046] Trial 4783 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-07 06:09:27,624] Trial 4784 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4785 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.992368386798276e-05
  Dropout: 0.03872487956514256
================================================================================

[I 2025-11-07 06:19:53,100] Trial 4785 pruned. Pruned at step 12 with metric 0.5900
[I 2025-11-07 06:19:53,710] Trial 4786 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4787 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.475623199430811e-05
  Dropout: 0.3769116445379244
================================================================================

[I 2025-11-07 06:38:48,234] Trial 4778 finished with value: 0.7361647361647361 and parameters: {'seed': 34161, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 7.980264669195609e-06, 'optim.weight_decay': 2.757553886319414e-06, 'optim.beta1': 0.9481547011921265, 'optim.beta2': 0.9872681500667578, 'optim.eps': 2.46845499393828e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.12267734018500448, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.477165761045539, 'model.dropout': 0.19287986113651379, 'model.attn_dropout': 0.24295396179775686, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8383675525331036, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.41434926640317543, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.4288795884816743, 'loss.cls.alpha': 0.7133873602548316, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-07 06:38:48,817] The parameter `tok.doc_stride` in Trial#4788 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 44 (patience=20)

================================================================================
TRIAL 4788 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.2845546851713402e-05
  Dropout: 0.14119180708387674
================================================================================

[I 2025-11-07 06:39:29,011] Trial 4787 finished with value: 0.6779160382101559 and parameters: {'seed': 39148, 'model.name': 'roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.475623199430811e-05, 'optim.weight_decay': 0.06588070033090238, 'optim.beta1': 0.8269595759411618, 'optim.beta2': 0.9666010042891517, 'optim.eps': 1.1902685021784186e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.04620995329316972, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.8261277858634282, 'model.dropout': 0.3769116445379244, 'model.attn_dropout': 0.14687795569811482, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9745791663226556, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.1488331380793992, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.599854558678702, 'loss.cls.alpha': 0.6978861753503356, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 48 (patience=20)

================================================================================
TRIAL 4789 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 7.888483029794837e-06
  Dropout: 0.42373466893488104
================================================================================

[I 2025-11-07 06:51:13,700] Trial 4788 pruned. Pruned at step 10 with metric 0.5962
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4790 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.691178137017388e-05
  Dropout: 0.05859563661836304
================================================================================

[I 2025-11-07 07:00:21,729] Trial 4790 pruned. Pruned at step 9 with metric 0.6183
[W 2025-11-07 07:00:22,300] The parameter `tok.doc_stride` in Trial#4791 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4791 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.485272053377327e-06
  Dropout: 0.49182503557290175
================================================================================

[I 2025-11-07 07:00:26,974] Trial 4789 pruned. Pruned at step 35 with metric 0.6167
[I 2025-11-07 07:00:27,596] Trial 4792 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4793 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.468851171378757e-05
  Dropout: 0.20700766895569767
================================================================================

[I 2025-11-07 07:10:42,015] Trial 4791 pruned. Pruned at step 17 with metric 0.5886

================================================================================
TRIAL 4794 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.0904544379102608e-05
  Dropout: 0.09601556770004344
================================================================================

[I 2025-11-07 07:17:12,753] Trial 4794 pruned. Pruned at step 13 with metric 0.5595
[I 2025-11-07 07:17:13,353] Trial 4795 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4796 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 9.062741764987585e-06
  Dropout: 0.2059592679611248
================================================================================

[I 2025-11-07 07:28:03,441] Trial 4796 pruned. OOM: microsoft/deberta-v3-base bs=8 len=384
[I 2025-11-07 07:28:04,287] Trial 4797 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-07 07:28:04,844] Trial 4793 pruned. OOM: microsoft/deberta-v3-large bs=8 len=320
[W 2025-11-07 07:28:05,350] The parameter `tok.doc_stride` in Trial#4798 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 4796 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 92.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4793 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 92.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4798 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.165176447128495e-06
  Dropout: 0.10538511246408974
================================================================================


================================================================================
TRIAL 4799 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 6.422391639610274e-06
  Dropout: 0.4777873559828539
================================================================================

[I 2025-11-07 07:38:16,861] Trial 4798 pruned. Pruned at step 32 with metric 0.5656
[W 2025-11-07 07:38:17,452] The parameter `tok.doc_stride` in Trial#4800 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4800 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0093775303385555e-05
  Dropout: 0.39695135139454363
================================================================================

[I 2025-11-07 07:46:23,820] Trial 4800 pruned. Pruned at step 13 with metric 0.5629

================================================================================
TRIAL 4801 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.1017854187976355e-05
  Dropout: 0.3954780590137789
================================================================================

[I 2025-11-07 07:50:41,822] Trial 4801 pruned. Pruned at step 8 with metric 0.5657
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4802 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.752299785084331e-06
  Dropout: 0.4750730208549142
================================================================================

[I 2025-11-07 07:55:47,877] Trial 4802 pruned. Pruned at step 11 with metric 0.6215
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4803 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.9184158328663992e-05
  Dropout: 0.3103495772224893
================================================================================

[I 2025-11-07 08:09:30,495] Trial 4803 pruned. Pruned at step 12 with metric 0.6338

================================================================================
TRIAL 4804 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 5.8866088082538755e-05
  Dropout: 0.044809475568473034
================================================================================

[I 2025-11-07 08:38:21,835] Trial 4804 finished with value: 0.45478723404255317 and parameters: {'seed': 2844, 'model.name': 'bert-large-uncased', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.8866088082538755e-05, 'optim.weight_decay': 3.085577206286254e-05, 'optim.beta1': 0.8706961929155058, 'optim.beta2': 0.9821418338805329, 'optim.eps': 3.5304010210338265e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03465907041856921, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.7825722790626661, 'model.dropout': 0.044809475568473034, 'model.attn_dropout': 0.2457802279934745, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8050684268528754, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3622357128076518, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1990583358776506, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4805 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 5.411857534136411e-05
  Dropout: 0.268991962917883
================================================================================

[I 2025-11-07 08:41:27,860] Trial 4805 pruned. Pruned at step 9 with metric 0.6210
[I 2025-11-07 08:41:28,501] Trial 4806 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[I 2025-11-07 08:41:29,120] Trial 4807 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-07 08:41:29,680] The parameter `tok.doc_stride` in Trial#4808 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4808 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.1801180089392083e-05
  Dropout: 0.05810872467339616
================================================================================

[I 2025-11-07 08:50:40,728] Trial 4799 finished with value: 0.7247462919594068 and parameters: {'seed': 60124, 'model.name': 'xlm-roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.422391639610274e-06, 'optim.weight_decay': 1.8977965432909864e-05, 'optim.beta1': 0.8833047273972818, 'optim.beta2': 0.9627483794067946, 'optim.eps': 8.05610950115396e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.010568610016813263, 'train.clip_grad': 1.2877599796954158, 'model.dropout': 0.4777873559828539, 'model.attn_dropout': 0.1505331052650808, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9094198703924813, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.2301803216784189, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.8011703569315625, 'loss.cls.alpha': 0.3526398095396223, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 08:50:41,335] Trial 4809 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 45 (patience=20)

================================================================================
TRIAL 4810 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 3.9318875618261646e-05
  Dropout: 0.05962966128309899
================================================================================

[I 2025-11-07 08:54:32,787] Trial 4810 pruned. Pruned at step 11 with metric 0.6121

================================================================================
TRIAL 4811 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.495636458484711e-06
  Dropout: 0.2164002017749343
================================================================================

[I 2025-11-07 08:58:02,872] Trial 4811 pruned. Pruned at step 8 with metric 0.6121

================================================================================
TRIAL 4812 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 8.765083431413462e-06
  Dropout: 0.3272547057294833
================================================================================

[I 2025-11-07 09:03:04,637] Trial 4808 pruned. Pruned at step 14 with metric 0.5864
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4813 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.63276858063902e-05
  Dropout: 0.19645857400795758
================================================================================

[I 2025-11-07 09:13:46,767] Trial 4812 pruned. Pruned at step 8 with metric 0.6165

================================================================================
TRIAL 4814 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.7967737335852797e-05
  Dropout: 0.12813026716039216
================================================================================

[I 2025-11-07 10:10:22,723] Trial 4813 pruned. Pruned at step 27 with metric 0.5864

================================================================================
TRIAL 4815 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 9.920789436241656e-06
  Dropout: 0.2799818015346849
================================================================================

[I 2025-11-07 10:18:09,442] Trial 4814 pruned. Pruned at step 13 with metric 0.5991

================================================================================
TRIAL 4816 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.0592401402604467e-05
  Dropout: 0.3432128180325344
================================================================================

[I 2025-11-07 10:18:48,294] Trial 4815 pruned. Pruned at step 10 with metric 0.6053

================================================================================
TRIAL 4817 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.9749752021320598e-05
  Dropout: 0.36511089982280526
================================================================================

[I 2025-11-07 10:29:45,915] Trial 4816 pruned. Pruned at step 12 with metric 0.6121
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4818 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.2227220608053278e-05
  Dropout: 0.31547147623031835
================================================================================

[I 2025-11-07 10:34:44,671] Trial 4817 finished with value: 0.4383561643835616 and parameters: {'seed': 61226, 'model.name': 'xlm-roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.9749752021320598e-05, 'optim.weight_decay': 0.16054345868641753, 'optim.beta1': 0.8291866268306692, 'optim.beta2': 0.9730910979087628, 'optim.eps': 6.133792645994974e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.021726976729625747, 'sched.poly_power': 1.1245841804643586, 'train.clip_grad': 1.1934592498429277, 'model.dropout': 0.36511089982280526, 'model.attn_dropout': 0.10701116179642826, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9929670464268149, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.22866850197676314, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8442512000293565, 'loss.cls.alpha': 0.47307402948145044, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-07 10:34:45,247] The parameter `tok.doc_stride` in Trial#4819 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4819 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.0310821332907381e-05
  Dropout: 0.44490077953265594
================================================================================

[I 2025-11-07 10:47:07,181] Trial 4818 pruned. Pruned at step 11 with metric 0.5949
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4820 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 9.416890621167945e-06
  Dropout: 0.14022639631019437
================================================================================

[I 2025-11-07 10:56:44,596] Trial 4819 pruned. Pruned at step 17 with metric 0.5666
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4821 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.6110601637882448e-05
  Dropout: 0.23453974722697718
================================================================================

[I 2025-11-07 11:00:08,601] Trial 4821 pruned. Pruned at step 9 with metric 0.5936
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4822 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.769855646704192e-05
  Dropout: 0.08245563361203263
================================================================================

[I 2025-11-07 11:06:07,717] Trial 4820 pruned. Pruned at step 7 with metric 0.5866
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4823 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 8.235264838507205e-06
  Dropout: 0.2656286239289951
================================================================================

[I 2025-11-07 11:16:34,489] Trial 4822 pruned. Pruned at step 12 with metric 0.6382
[I 2025-11-07 11:16:35,185] Trial 4824 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4825 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.0383149037547855e-05
  Dropout: 0.13502163280170137
================================================================================

[I 2025-11-07 11:28:31,857] Trial 4823 pruned. Pruned at step 8 with metric 0.6159
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4826 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.611456971532918e-06
  Dropout: 0.3521271806365893
================================================================================

[I 2025-11-07 11:29:10,724] Trial 4825 finished with value: 0.7099315461394318 and parameters: {'seed': 48196, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.0383149037547855e-05, 'optim.weight_decay': 0.00017404365090536582, 'optim.beta1': 0.9040159047139237, 'optim.beta2': 0.9803057987604491, 'optim.eps': 1.0023102840857332e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.061166962553191005, 'sched.poly_power': 1.1423751813869265, 'train.clip_grad': 1.1068319381808804, 'model.dropout': 0.13502163280170137, 'model.attn_dropout': 0.18064050367897017, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8525927002741341, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3924162808552201, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.15578852555904588, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 4827 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.1646083034044382e-05
  Dropout: 0.286093731707437
================================================================================

[I 2025-11-07 11:37:40,486] Trial 4827 pruned. Pruned at step 10 with metric 0.5962
[I 2025-11-07 11:37:41,126] Trial 4828 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-07 11:37:41,722] Trial 4829 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4830 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 5.548776083032072e-05
  Dropout: 0.13168823803022794
================================================================================

[I 2025-11-07 11:56:00,928] Trial 4826 pruned. Pruned at step 13 with metric 0.6250

================================================================================
TRIAL 4831 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 2.2531929300558677e-05
  Dropout: 0.2282972340861691
================================================================================

[I 2025-11-07 11:58:01,987] Trial 4830 finished with value: 0.44594594594594594 and parameters: {'seed': 26870, 'model.name': 'bert-large-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.548776083032072e-05, 'optim.weight_decay': 0.0262375544238127, 'optim.beta1': 0.8203650262059767, 'optim.beta2': 0.9622409825728393, 'optim.eps': 7.176886753056846e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18453489947902682, 'train.clip_grad': 0.6915319676131318, 'model.dropout': 0.13168823803022794, 'model.attn_dropout': 0.2792104199064926, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8702540769681605, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.18525514339071336, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.160438571090248, 'loss.cls.alpha': 0.4738643097193011, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-07 11:58:02,577] The parameter `tok.doc_stride` in Trial#4832 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4832 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 8.74620198527383e-06
  Dropout: 0.16359918206536728
================================================================================

[I 2025-11-07 12:03:06,502] Trial 4831 pruned. Pruned at step 12 with metric 0.5941
[I 2025-11-07 12:03:07,390] Trial 4833 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4834 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.922877370422596e-05
  Dropout: 0.07798588183418169
================================================================================

[I 2025-11-07 12:11:59,958] Trial 4832 finished with value: 0.6483951135846546 and parameters: {'seed': 57096, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.74620198527383e-06, 'optim.weight_decay': 6.2670584959856545e-06, 'optim.beta1': 0.9372157301821337, 'optim.beta2': 0.9778640218602076, 'optim.eps': 3.149540554165998e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.021199818888354274, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.327866952976231, 'model.dropout': 0.16359918206536728, 'model.attn_dropout': 0.1879007681483109, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8070436945087957, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.34858110407739756, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.15841129969714213, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 12:12:00,589] Trial 4835 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-07 12:12:01,174] Trial 4836 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-07 12:12:01,751] Trial 4837 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 4838 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 9.91445740271819e-06
  Dropout: 0.24304897280105198
================================================================================

[I 2025-11-07 12:24:30,690] Trial 4834 pruned. Pruned at step 10 with metric 0.6077

================================================================================
TRIAL 4839 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.248828094458555e-05
  Dropout: 0.01346299091591277
================================================================================

[I 2025-11-07 12:31:18,827] Trial 4839 finished with value: 0.6272727272727272 and parameters: {'seed': 63965, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.248828094458555e-05, 'optim.weight_decay': 1.145011414795076e-06, 'optim.beta1': 0.9310668081286586, 'optim.beta2': 0.9763570379993595, 'optim.eps': 4.675333322839274e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.07209036650587505, 'train.clip_grad': 1.1793796287838483, 'model.dropout': 0.01346299091591277, 'model.attn_dropout': 0.24892815953115227, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8617193206795792, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.4152219961478225, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.4878849426962075, 'loss.cls.alpha': 0.2779395184979559, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 12:31:19,432] Trial 4840 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4841 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.1147359317398226e-05
  Dropout: 0.19177448428350102
================================================================================

EarlyStopping triggered at epoch 40 (patience=20)
[I 2025-11-07 12:31:22,432] Trial 4838 finished with value: 0.722258064516129 and parameters: {'seed': 4244, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.91445740271819e-06, 'optim.weight_decay': 3.295712546523277e-05, 'optim.beta1': 0.8448804247942517, 'optim.beta2': 0.9889330588111712, 'optim.eps': 2.8361941466680693e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13442549687591088, 'sched.cosine_cycles': 4, 'train.clip_grad': 0.7193128099922527, 'model.dropout': 0.24304897280105198, 'model.attn_dropout': 0.2001784901827239, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8749291814390198, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.2078791748344206, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18456525664412346, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.

================================================================================
TRIAL 4842 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.731707795505759e-05
  Dropout: 0.26421477493060025
================================================================================

[I 2025-11-07 12:33:26,235] Trial 4842 pruned. Pruned at step 9 with metric 0.6027
[W 2025-11-07 12:33:26,823] The parameter `tok.doc_stride` in Trial#4843 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4843 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.1530347763652347e-05
  Dropout: 0.27992450297836824
================================================================================

[I 2025-11-07 12:41:38,534] Trial 4843 pruned. Pruned at step 27 with metric 0.6070
[I 2025-11-07 12:41:39,176] Trial 4844 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4845 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 8.006381079001718e-06
  Dropout: 0.3319856452111319
================================================================================

[I 2025-11-07 12:48:45,941] Trial 4841 pruned. Pruned at step 9 with metric 0.6159
[I 2025-11-07 12:48:46,567] Trial 4846 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4847 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.3398307026049808e-05
  Dropout: 0.0960582969713692
================================================================================

[I 2025-11-07 12:49:27,307] Trial 4845 pruned. Pruned at step 11 with metric 0.5593
[I 2025-11-07 12:49:27,928] Trial 4848 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-07 12:49:28,472] The parameter `tok.doc_stride` in Trial#4849 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4849 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.230764703700693e-06
  Dropout: 0.40275061437749404
================================================================================

[I 2025-11-07 13:03:51,630] Trial 4847 finished with value: 0.7281083248511766 and parameters: {'seed': 47079, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.3398307026049808e-05, 'optim.weight_decay': 2.205715693227191e-06, 'optim.beta1': 0.937525041758151, 'optim.beta2': 0.9729973531144746, 'optim.eps': 3.0935537374599637e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0317439751156644, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.3537132201047115, 'model.dropout': 0.0960582969713692, 'model.attn_dropout': 0.23936843184138334, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8467377635337868, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.39833213477819895, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1723946181678036, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-07 13:03:52,225] The parameter `tok.doc_stride` in Trial#4850 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 4850 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.9939365982122004e-06
  Dropout: 0.2870948121285135
================================================================================

[I 2025-11-07 13:07:04,367] Trial 4849 pruned. Pruned at step 27 with metric 0.6494

================================================================================
TRIAL 4851 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.5278892379013226e-05
  Dropout: 0.283321918713755
================================================================================

[I 2025-11-07 13:12:56,664] Trial 4851 pruned. Pruned at step 27 with metric 0.5381
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4852 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.884586697562019e-05
  Dropout: 0.42791537458897805
================================================================================

[I 2025-11-07 13:50:46,377] Trial 4852 finished with value: 0.6513605442176871 and parameters: {'seed': 62494, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.884586697562019e-05, 'optim.weight_decay': 0.05959925229842254, 'optim.beta1': 0.8288426666681197, 'optim.beta2': 0.9592273773532607, 'optim.eps': 8.107142276446548e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.013733650597686026, 'sched.poly_power': 1.093855462201743, 'train.clip_grad': 1.1944614566957523, 'model.dropout': 0.42791537458897805, 'model.attn_dropout': 0.14910431670833876, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9300371632346022, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.06992308026557957, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.15380550264366483, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 13:50:46,981] Trial 4853 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 4854 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.8347660394209755e-05
  Dropout: 0.12470751298217625
================================================================================

[I 2025-11-07 13:54:46,892] Trial 4854 pruned. Pruned at step 15 with metric 0.6184

================================================================================
TRIAL 4855 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.7766921707781078e-05
  Dropout: 0.1878890915554799
================================================================================

[I 2025-11-07 14:10:08,430] Trial 4855 pruned. Pruned at step 10 with metric 0.6125
[I 2025-11-07 14:10:09,046] Trial 4856 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[W 2025-11-07 14:10:09,590] The parameter `tok.doc_stride` in Trial#4857 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4857 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.5510331224323602e-05
  Dropout: 0.34169749163433877
================================================================================

[I 2025-11-07 14:13:49,927] Trial 4857 pruned. Pruned at step 11 with metric 0.6077
[W 2025-11-07 14:13:50,535] The parameter `tok.doc_stride` in Trial#4858 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4858 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.0350406648525487e-05
  Dropout: 0.39592538609581485
================================================================================

[I 2025-11-07 14:16:33,046] Trial 4858 pruned. Pruned at step 9 with metric 0.5695
[W 2025-11-07 14:16:33,615] The parameter `tok.doc_stride` in Trial#4859 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4859 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.298067411057296e-06
  Dropout: 0.37233246572487405
================================================================================

[I 2025-11-07 14:17:38,409] Trial 4850 pruned. Pruned at step 27 with metric 0.5864
[I 2025-11-07 14:17:39,118] Trial 4860 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4861 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.209932472368581e-05
  Dropout: 0.4906397210144642
================================================================================

[I 2025-11-07 14:21:31,166] Trial 4859 pruned. Pruned at step 9 with metric 0.5208

================================================================================
TRIAL 4862 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 5.52536697164127e-05
  Dropout: 0.024611141971369092
================================================================================

[I 2025-11-07 14:23:38,357] Trial 4862 pruned. Pruned at step 9 with metric 0.5444
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4863 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 7.258402826684749e-06
  Dropout: 0.38626580600866073
================================================================================

[I 2025-11-07 14:38:29,795] Trial 4863 pruned. Pruned at step 10 with metric 0.6561
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4864 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.5093081554742808e-05
  Dropout: 0.05389502563997991
================================================================================

[I 2025-11-07 15:21:42,220] Trial 4864 finished with value: 0.7144183886696076 and parameters: {'seed': 25120, 'model.name': 'roberta-large', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.5093081554742808e-05, 'optim.weight_decay': 1.1370186704114223e-05, 'optim.beta1': 0.9491760827483459, 'optim.beta2': 0.9567451159935741, 'optim.eps': 3.927017760244165e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.07940595240541498, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.200825683027258, 'model.dropout': 0.05389502563997991, 'model.attn_dropout': 0.09585182737080886, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8360475717798155, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.3943449586774446, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.083633380313114, 'loss.cls.alpha': 0.5380851281608106, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 15:21:42,822] Trial 4865 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 4866 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.861548083492988e-06
  Dropout: 0.04395547221575469
================================================================================

[I 2025-11-07 15:29:28,207] Trial 4866 pruned. Pruned at step 27 with metric 0.6583

================================================================================
TRIAL 4867 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.237891507351013e-05
  Dropout: 0.07187123146898716
================================================================================

[I 2025-11-07 15:32:56,100] Trial 4867 pruned. Pruned at step 9 with metric 0.6496
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4868 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 0.00012000497734023808
  Dropout: 0.15694816518828392
================================================================================

[I 2025-11-07 15:56:00,275] Trial 4868 finished with value: 0.4444444444444444 and parameters: {'seed': 5480, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 0.00012000497734023808, 'optim.weight_decay': 0.0001544131676735947, 'optim.beta1': 0.8360995004592622, 'optim.beta2': 0.9687843344136097, 'optim.eps': 4.4600420616512134e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.17264844053816483, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.131635991597107, 'model.dropout': 0.15694816518828392, 'model.attn_dropout': 0.10503027247237849, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8780414410615939, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.2747138356534816, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18467654025954142, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 15:56:00,893] Trial 4869 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4870 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.823765145414588e-05
  Dropout: 0.2125215456525413
================================================================================

[I 2025-11-07 16:10:44,011] Trial 4861 finished with value: 0.6554621848739496 and parameters: {'seed': 65533, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.209932472368581e-05, 'optim.weight_decay': 0.04822592242480751, 'optim.beta1': 0.8523205177110483, 'optim.beta2': 0.9973445669151807, 'optim.eps': 1.1329176100542597e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.15448903012569798, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.22169489682973575, 'model.dropout': 0.4906397210144642, 'model.attn_dropout': 0.07904213164557243, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8576590388623168, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.09766538292245602, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.036355589238365, 'loss.cls.alpha': 0.41765354560144, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 4871 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 6.359319330607122e-06
  Dropout: 0.4639013240007917
================================================================================

[I 2025-11-07 16:12:41,758] Trial 4870 pruned. Pruned at step 29 with metric 0.6384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4872 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 4.3128996862259675e-05
  Dropout: 0.3912797076390183
================================================================================

[I 2025-11-07 16:39:06,647] Trial 4872 finished with value: 0.450402144772118 and parameters: {'seed': 12102, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 4.3128996862259675e-05, 'optim.weight_decay': 0.04455765597600946, 'optim.beta1': 0.8268015932806102, 'optim.beta2': 0.9552885251905165, 'optim.eps': 9.35544730580734e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.03604156306052482, 'sched.poly_power': 1.009036293353442, 'train.clip_grad': 1.1329826815213884, 'model.dropout': 0.3912797076390183, 'model.attn_dropout': 0.1258078925794006, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9886776697048224, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.1798203429226416, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.904382829217019, 'loss.cls.alpha': 0.2716134359857406, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4873 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.721117177075905e-06
  Dropout: 0.33275315816742285
================================================================================

[I 2025-11-07 16:41:25,575] Trial 4873 pruned. Pruned at step 13 with metric 0.5866

================================================================================
TRIAL 4874 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.2599840338113586e-05
  Dropout: 0.1701340868813398
================================================================================

[I 2025-11-07 16:47:15,347] Trial 4871 pruned. Pruned at step 17 with metric 0.6383

================================================================================
TRIAL 4875 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.891737251320949e-05
  Dropout: 0.4196475711756039
================================================================================

[I 2025-11-07 16:48:04,973] Trial 4874 pruned. Pruned at step 15 with metric 0.5733
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4876 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 5.6093016159912025e-06
  Dropout: 0.3678427541630565
================================================================================

[I 2025-11-07 16:48:13,693] Trial 4875 pruned. OOM: bert-base-uncased bs=64 len=128
[I 2025-11-07 16:48:15,013] Trial 4876 pruned. OOM: microsoft/deberta-v3-large bs=8 len=320
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4875 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 54.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4876 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 16 with grad_accum=2)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 54.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4877 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.807362716810169e-06
  Dropout: 0.4027861795906701
================================================================================


================================================================================
TRIAL 4878 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.6859742375756828e-05
  Dropout: 0.024150857372788676
================================================================================

[I 2025-11-07 16:52:09,826] Trial 4877 pruned. Pruned at step 11 with metric 0.5962
[I 2025-11-07 16:52:10,459] Trial 4879 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4880 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.2017637724024656e-05
  Dropout: 0.06872163155501822
================================================================================

[I 2025-11-07 16:55:35,501] Trial 4878 pruned. Pruned at step 10 with metric 0.5666
[I 2025-11-07 16:55:36,138] Trial 4881 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4882 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.958324869545649e-05
  Dropout: 0.12213877392666046
================================================================================

[I 2025-11-07 16:58:24,602] Trial 4882 pruned. Pruned at step 8 with metric 0.5619
[I 2025-11-07 16:58:25,207] Trial 4883 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-07 16:58:25,798] Trial 4884 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-07 16:58:26,389] Trial 4885 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4886 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.883117339744349e-05
  Dropout: 0.27218622165806006
================================================================================

[I 2025-11-07 17:32:05,862] Trial 4880 pruned. Pruned at step 36 with metric 0.5695

================================================================================
TRIAL 4887 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.4703907273198172e-05
  Dropout: 0.1746683637016491
================================================================================

[I 2025-11-07 17:40:17,834] Trial 4886 pruned. Pruned at step 14 with metric 0.4715
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4888 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.2265985435140091e-05
  Dropout: 0.47756133222814023
================================================================================

[I 2025-11-07 17:52:30,509] Trial 4887 finished with value: 0.7144278606965174 and parameters: {'seed': 57703, 'model.name': 'xlm-roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.4703907273198172e-05, 'optim.weight_decay': 1.2387987233301004e-05, 'optim.beta1': 0.8411525438413231, 'optim.beta2': 0.9783284213551047, 'optim.eps': 1.9900123994462172e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.09928653201316079, 'train.clip_grad': 1.3059860269227284, 'model.dropout': 0.1746683637016491, 'model.attn_dropout': 0.24715706896142728, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8459661895874349, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.12945229843302553, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.6302135309885877, 'loss.cls.alpha': 0.2113570974762456, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 17:52:31,118] Trial 4889 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 4890 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 7.917200133544074e-06
  Dropout: 0.4234691938571233
================================================================================

[I 2025-11-07 17:58:03,326] Trial 4888 pruned. Pruned at step 10 with metric 0.6200
[I 2025-11-07 17:58:04,050] Trial 4891 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4892 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 7.531636977599009e-06
  Dropout: 0.3515459891831317
================================================================================

[I 2025-11-07 18:17:08,216] Trial 4892 pruned. Pruned at step 9 with metric 0.6321
[I 2025-11-07 18:17:08,822] Trial 4893 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4894 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.556176486339876e-05
  Dropout: 0.08319869060762358
================================================================================

[I 2025-11-07 18:17:16,395] Trial 4890 pruned. OOM: microsoft/deberta-v3-base bs=8 len=352
[I 2025-11-07 18:17:17,411] Trial 4894 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384
[I 2025-11-07 18:17:17,826] Trial 4895 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-07 18:17:18,739] Trial 4896 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

[OOM] Trial 4890 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 36.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4894 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 64 with grad_accum=8)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 36.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4897 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.127863424391507e-05
  Dropout: 0.11045750335465065
================================================================================


================================================================================
TRIAL 4898 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 6.620406612562336e-06
  Dropout: 0.4692030463491967
================================================================================

[I 2025-11-07 18:17:26,748] Trial 4897 pruned. OOM: bert-base-uncased bs=64 len=224
[I 2025-11-07 18:17:28,060] Trial 4898 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352
[I 2025-11-07 18:17:28,814] Trial 4900 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-07 18:17:29,455] Trial 4901 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)

[OOM] Trial 4897 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 52.31 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 4898 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 92.31 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4899 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.6446155242699647e-05
  Dropout: 0.08942269768978306
================================================================================


================================================================================
TRIAL 4902 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.4530924842522783e-05
  Dropout: 0.33718433145086746
================================================================================

[I 2025-11-07 18:38:26,243] Trial 4899 finished with value: 0.6863100957261308 and parameters: {'seed': 38893, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.6446155242699647e-05, 'optim.weight_decay': 0.000779785773013129, 'optim.beta1': 0.8331237100031388, 'optim.beta2': 0.9681258900598527, 'optim.eps': 9.083333507531994e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1631394573735318, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.47825732042956953, 'model.dropout': 0.08942269768978306, 'model.attn_dropout': 0.23413132664187006, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8183078162706879, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.2871285681782514, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.987395622358489, 'loss.cls.alpha': 0.3439391593019775, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 4903 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 4.824858714544568e-05
  Dropout: 0.03460820480164286
================================================================================

[I 2025-11-07 18:44:58,340] Trial 4902 finished with value: 0.6491542288557214 and parameters: {'seed': 48273, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.4530924842522783e-05, 'optim.weight_decay': 8.433447251883233e-05, 'optim.beta1': 0.8152318818378622, 'optim.beta2': 0.9679326227071862, 'optim.eps': 9.864371682553605e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.11748835168952844, 'train.clip_grad': 1.3301897132288782, 'model.dropout': 0.33718433145086746, 'model.attn_dropout': 0.22669460366437758, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9861610849337225, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.37198230062569204, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1997586819473617, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4904 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 6.069245396402032e-06
  Dropout: 0.40420907630721925
================================================================================

[I 2025-11-07 19:26:57,381] Trial 4904 pruned. Pruned at step 27 with metric 0.5349
[I 2025-11-07 19:26:57,992] Trial 4905 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4906 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 6.824932892940965e-06
  Dropout: 0.4706672683903707
================================================================================

[I 2025-11-07 19:32:04,243] Trial 4906 pruned. Pruned at step 27 with metric 0.5997
[I 2025-11-07 19:32:04,860] Trial 4907 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-07 19:32:05,451] Trial 4908 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4909 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 8.350727978805817e-06
  Dropout: 0.4824040379923872
================================================================================

[I 2025-11-07 19:35:26,920] Trial 4909 pruned. Pruned at step 17 with metric 0.6095
[I 2025-11-07 19:35:27,536] Trial 4910 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4911 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.0372328574146905e-05
  Dropout: 0.2104989246504108
================================================================================

[I 2025-11-07 20:03:48,379] Trial 4911 finished with value: 0.6583333333333333 and parameters: {'seed': 63400, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 288, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.0372328574146905e-05, 'optim.weight_decay': 0.0007701080225314945, 'optim.beta1': 0.928200520960738, 'optim.beta2': 0.9765072952301994, 'optim.eps': 5.824279328800113e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0014906080324233579, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.297189572409066, 'model.dropout': 0.2104989246504108, 'model.attn_dropout': 0.13278753380740452, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9040652126932397, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.423425040355876, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19858096343743514, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 20:03:48,981] Trial 4912 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-07 20:03:49,529] The parameter `tok.doc_stride` in Trial#4913 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 4913 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.431204332265102e-06
  Dropout: 0.36874887708925475
================================================================================

[I 2025-11-07 20:07:41,240] Trial 4913 pruned. Pruned at step 9 with metric 0.5997
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4914 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 8.551106372783047e-06
  Dropout: 0.4105409542834757
================================================================================

[I 2025-11-07 20:17:01,542] Trial 4914 pruned. Pruned at step 27 with metric 0.6216
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4915 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 6.436364287727439e-06
  Dropout: 0.3539196332544673
================================================================================

[I 2025-11-07 20:21:12,941] Trial 4903 finished with value: 0.4444444444444444 and parameters: {'seed': 57930, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 4.824858714544568e-05, 'optim.weight_decay': 0.09150577885645278, 'optim.beta1': 0.9392630101586559, 'optim.beta2': 0.9683181227852433, 'optim.eps': 3.016851888045349e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0762529196706151, 'sched.poly_power': 1.8254186303079285, 'train.clip_grad': 1.3182557889221755, 'model.dropout': 0.03460820480164286, 'model.attn_dropout': 0.19577110108409246, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8206740560008956, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.29788299047488886, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7616747368835286, 'loss.cls.alpha': 0.4187445676486672, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4916 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 9.466620848540115e-06
  Dropout: 0.10602357192477169
================================================================================

[I 2025-11-07 20:23:00,071] Trial 4915 pruned. Pruned at step 9 with metric 0.6549
[W 2025-11-07 20:23:00,680] The parameter `tok.doc_stride` in Trial#4917 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4917 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.363845369181369e-05
  Dropout: 0.2717751546300394
================================================================================

[I 2025-11-07 20:31:39,751] Trial 4917 pruned. Pruned at step 16 with metric 0.6294

================================================================================
TRIAL 4918 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.12386186398228e-06
  Dropout: 0.4670205332366004
================================================================================

[I 2025-11-07 20:35:19,781] Trial 4918 pruned. Pruned at step 11 with metric 0.5450
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4919 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 4.159480251280443e-05
  Dropout: 0.05187000703876676
================================================================================

[I 2025-11-07 20:40:35,806] Trial 4916 pruned. Pruned at step 20 with metric 0.6151
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4920 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.1174429542302187e-05
  Dropout: 0.43354030187199516
================================================================================

[I 2025-11-07 20:52:12,933] Trial 4919 finished with value: 0.6425394114088467 and parameters: {'seed': 45831, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 4.159480251280443e-05, 'optim.weight_decay': 0.0003058168900998974, 'optim.beta1': 0.9300377038395122, 'optim.beta2': 0.9580983822844166, 'optim.eps': 1.4942845654684556e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06547774234733851, 'sched.poly_power': 1.2599044778845054, 'train.clip_grad': 1.3669045230363681, 'model.dropout': 0.05187000703876676, 'model.attn_dropout': 0.25869996827784814, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8943785888234301, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.24565776012106433, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1518581308591267, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 4921 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.23837614546126e-05
  Dropout: 0.1221445682700341
================================================================================

[I 2025-11-07 20:56:46,633] Trial 4920 finished with value: 0.7104619518412623 and parameters: {'seed': 31760, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.1174429542302187e-05, 'optim.weight_decay': 0.13090865335434387, 'optim.beta1': 0.868894777166103, 'optim.beta2': 0.9720354616121978, 'optim.eps': 8.572498747769546e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.01641504023723781, 'sched.poly_power': 1.0506954765797911, 'train.clip_grad': 1.4625400947498588, 'model.dropout': 0.43354030187199516, 'model.attn_dropout': 0.2833529189854817, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8560488094996311, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.35517793076972665, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.4306889128113385, 'loss.cls.alpha': 0.5829851814664319, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-07 20:56:47,233] The parameter `tok.doc_stride` in Trial#4922 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4922 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 5.746383541819172e-06
  Dropout: 0.09322347413185657
================================================================================

[I 2025-11-07 20:58:24,373] Trial 4921 pruned. Pruned at step 9 with metric 0.6250

================================================================================
TRIAL 4923 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 4.1604667638225565e-05
  Dropout: 0.10967325571239356
================================================================================

[I 2025-11-07 21:02:49,273] Trial 4923 pruned. Pruned at step 9 with metric 0.4915
[I 2025-11-07 21:02:49,931] Trial 4924 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4925 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.5067702655026172e-05
  Dropout: 0.4239613640627049
================================================================================

[I 2025-11-07 21:02:57,861] Trial 4925 pruned. OOM: roberta-large bs=16 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4925 exceeded GPU memory:
  Model: roberta-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 52.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4926 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.0407890966052591e-05
  Dropout: 0.28618102141303303
================================================================================

[I 2025-11-07 21:13:39,930] Trial 4926 pruned. Pruned at step 9 with metric 0.6608
[W 2025-11-07 21:13:40,516] The parameter `tok.doc_stride` in Trial#4927 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-07 21:13:40,575] Trial 4927 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-07 21:13:41,164] Trial 4928 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4929 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 6.730400742308553e-06
  Dropout: 0.4505794878772461
================================================================================

[I 2025-11-07 21:26:57,210] Trial 4929 finished with value: 0.6967455621301775 and parameters: {'seed': 51639, 'model.name': 'xlm-roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.730400742308553e-06, 'optim.weight_decay': 0.0009250232584641736, 'optim.beta1': 0.8607498027276614, 'optim.beta2': 0.9637576791436583, 'optim.eps': 2.56816429290988e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.037040572480720874, 'sched.cosine_cycles': 4, 'train.clip_grad': 1.1014824021212726, 'model.dropout': 0.4505794878772461, 'model.attn_dropout': 0.12642887054549518, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9482427740246594, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.2086606933653084, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.282709226222645, 'loss.cls.alpha': 0.40367315087674815, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 21:26:57,815] Trial 4930 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4931 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.3288013102725216e-05
  Dropout: 0.023752775241874705
================================================================================

[I 2025-11-07 21:31:02,458] Trial 4922 pruned. Pruned at step 6 with metric 0.6112

================================================================================
TRIAL 4932 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.799475306402579e-06
  Dropout: 0.4907357075511477
================================================================================

[I 2025-11-07 21:37:22,267] Trial 4932 pruned. Pruned at step 12 with metric 0.6242

================================================================================
TRIAL 4933 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.4951441229311239e-05
  Dropout: 0.0402495262819502
================================================================================

[I 2025-11-07 21:38:06,376] Trial 4931 finished with value: 0.6389506859473488 and parameters: {'seed': 33595, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.3288013102725216e-05, 'optim.weight_decay': 0.14025267074206302, 'optim.beta1': 0.8251436403689535, 'optim.beta2': 0.9872733344239996, 'optim.eps': 5.34643998052272e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.08116282377216917, 'train.clip_grad': 0.30458463962526083, 'model.dropout': 0.023752775241874705, 'model.attn_dropout': 0.24349046023476492, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8462913805490555, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.04292473229077913, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.17217050260020955, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-07 21:38:07,024] Trial 4934 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-07 21:38:07,613] Trial 4935 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 4936 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.989083768580765e-06
  Dropout: 0.12806116396773556
================================================================================

[I 2025-11-07 21:43:47,065] Trial 4936 pruned. Pruned at step 28 with metric 0.6167
[I 2025-11-07 21:43:47,693] Trial 4937 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-07 21:43:48,279] Trial 4938 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-07 21:43:48,864] Trial 4939 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4940 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.0317070445448116e-05
  Dropout: 0.12198160258336377
================================================================================

[I 2025-11-07 21:51:04,423] Trial 4940 pruned. Pruned at step 10 with metric 0.5884

================================================================================
TRIAL 4941 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.5860371365401456e-05
  Dropout: 0.39831345870312856
================================================================================

[I 2025-11-07 22:08:39,453] Trial 4941 pruned. Pruned at step 27 with metric 0.6095

================================================================================
TRIAL 4942 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 5.725650931327232e-06
  Dropout: 0.49126732341909685
================================================================================

[I 2025-11-07 22:20:34,964] Trial 4933 pruned. Pruned at step 27 with metric 0.5957
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4943 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.0968810650046215e-05
  Dropout: 0.13620011094693119
================================================================================

[I 2025-11-07 22:33:35,423] Trial 4942 pruned. Pruned at step 11 with metric 0.6199
[I 2025-11-07 22:33:36,065] Trial 4944 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4945 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.629456899362138e-06
  Dropout: 0.2461996426540595
================================================================================

[I 2025-11-07 22:37:36,156] Trial 4945 pruned. Pruned at step 13 with metric 0.6067

================================================================================
TRIAL 4946 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.439848841688624e-05
  Dropout: 0.3815104506985742
================================================================================

[I 2025-11-07 22:49:33,497] Trial 4946 pruned. Pruned at step 14 with metric 0.6482
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4947 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.3991743227858915e-05
  Dropout: 0.14665733506080886
================================================================================

[I 2025-11-07 23:01:31,945] Trial 4947 finished with value: 0.6991813407476057 and parameters: {'seed': 62386, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.3991743227858915e-05, 'optim.weight_decay': 0.00033857154650223606, 'optim.beta1': 0.9176107300576534, 'optim.beta2': 0.9642737484357701, 'optim.eps': 2.4057882905348813e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.029584355302861708, 'sched.poly_power': 1.517602620026702, 'train.clip_grad': 0.9799804278665871, 'model.dropout': 0.14665733506080886, 'model.attn_dropout': 0.10246611382494533, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8234064498623236, 'head.pooling': 'attn', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.46196854364943096, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16871252055862956, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 4948 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 8.687761630680353e-06
  Dropout: 0.027394660346545988
================================================================================

[I 2025-11-07 23:12:01,854] Trial 4948 pruned. Pruned at step 20 with metric 0.6208
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4949 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.234597561931554e-06
  Dropout: 0.367183907444871
================================================================================

[I 2025-11-07 23:17:28,342] Trial 4949 pruned. Pruned at step 9 with metric 0.6583

================================================================================
TRIAL 4950 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 8.54635168453197e-06
  Dropout: 0.23151814664960693
================================================================================

[I 2025-11-07 23:22:23,750] Trial 4950 finished with value: 0.6917293233082706 and parameters: {'seed': 53310, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 8.54635168453197e-06, 'optim.weight_decay': 7.533155033977154e-05, 'optim.beta1': 0.8604268797630035, 'optim.beta2': 0.982633187944591, 'optim.eps': 1.7521250757979086e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.15786627696203995, 'sched.poly_power': 1.0098071716918353, 'train.clip_grad': 1.1935003490493175, 'model.dropout': 0.23151814664960693, 'model.attn_dropout': 0.05044069972494469, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9383004184872957, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.3217974825933494, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.990922284900559, 'loss.cls.alpha': 0.36102307586024635, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 4951 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 7.50177286316445e-06
  Dropout: 0.44815442136091055
================================================================================

[I 2025-11-07 23:23:38,993] Trial 4943 finished with value: 0.46335078534031415 and parameters: {'seed': 55160, 'model.name': 'roberta-large', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.0968810650046215e-05, 'optim.weight_decay': 0.003978306335225516, 'optim.beta1': 0.8682227178645344, 'optim.beta2': 0.9678216537323121, 'optim.eps': 7.53437179677697e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.17172223487579774, 'train.clip_grad': 0.9398242740378444, 'model.dropout': 0.13620011094693119, 'model.attn_dropout': 0.1857666899275956, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9303975746477898, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.22009626794991893, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.15389918466505484, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4952 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.6244101079825867e-05
  Dropout: 0.141403493117622
================================================================================

[I 2025-11-07 23:27:39,774] Trial 4952 pruned. Pruned at step 12 with metric 0.5878
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4953 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 9.886698545348029e-06
  Dropout: 0.11215421096664707
================================================================================

[I 2025-11-07 23:33:07,126] Trial 4953 pruned. Pruned at step 11 with metric 0.5763
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4954 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.7016767102767006e-05
  Dropout: 0.26830013389879226
================================================================================

[I 2025-11-07 23:43:05,477] Trial 4951 finished with value: 0.6916792000325189 and parameters: {'seed': 49992, 'model.name': 'xlm-roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 7.50177286316445e-06, 'optim.weight_decay': 0.014636609559018774, 'optim.beta1': 0.9068152392191935, 'optim.beta2': 0.961885111386447, 'optim.eps': 1.4203508409798596e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04316815255138501, 'sched.poly_power': 0.6100240292537651, 'train.clip_grad': 1.4542819078384304, 'model.dropout': 0.44815442136091055, 'model.attn_dropout': 0.0630640177010957, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9907714551816661, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.36577365860915434, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7012913004980112, 'loss.cls.alpha': 0.2275543134457696, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 4955 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.2237552197721302e-05
  Dropout: 0.2918742295475467
================================================================================

[I 2025-11-07 23:55:53,500] Trial 4955 pruned. Pruned at step 13 with metric 0.5986
[I 2025-11-07 23:55:54,124] Trial 4956 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-11-07 23:55:54,755] Trial 4957 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-07 23:55:55,355] Trial 4958 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-07 23:55:55,983] Trial 4959 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4960 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.828203711493077e-05
  Dropout: 0.08577631196585024
================================================================================

[I 2025-11-08 00:00:59,144] Trial 4960 pruned. Pruned at step 12 with metric 0.5584
[I 2025-11-08 00:00:59,783] Trial 4961 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-08 00:01:00,378] Trial 4962 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4963 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 4.050625641513431e-05
  Dropout: 0.4503986024117498
================================================================================

[I 2025-11-08 00:11:40,831] Trial 4954 finished with value: 0.4429347826086957 and parameters: {'seed': 13202, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.7016767102767006e-05, 'optim.weight_decay': 4.480362011699331e-05, 'optim.beta1': 0.8685743189035835, 'optim.beta2': 0.9789438986800426, 'optim.eps': 1.8865059613961723e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.15017625452283237, 'train.clip_grad': 0.9420278869555281, 'model.dropout': 0.26830013389879226, 'model.attn_dropout': 0.025754173534472916, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8807614961107301, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4031508342813754, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.991577020265573, 'loss.cls.alpha': 0.7838040278886813, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4964 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.145022286752032e-05
  Dropout: 0.05627810473839464
================================================================================

[I 2025-11-08 00:32:41,275] Trial 4964 pruned. Pruned at step 8 with metric 0.5531
[W 2025-11-08 00:32:41,911] The parameter `tok.doc_stride` in Trial#4965 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-08 00:32:41,968] Trial 4965 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-08 00:32:42,525] The parameter `tok.doc_stride` in Trial#4966 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4966 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0735782711398506e-05
  Dropout: 0.39765469978622325
================================================================================

[I 2025-11-08 00:35:34,356] Trial 4963 finished with value: 0.6737471174770211 and parameters: {'seed': 48904, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 4.050625641513431e-05, 'optim.weight_decay': 0.08945378049854127, 'optim.beta1': 0.8574973493043667, 'optim.beta2': 0.9636839109419216, 'optim.eps': 8.682834678109305e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.013056031257333851, 'sched.poly_power': 0.6186825362138239, 'train.clip_grad': 1.4658454690282274, 'model.dropout': 0.4503986024117498, 'model.attn_dropout': 0.11006593915459187, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9691696408364624, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.05246273245510169, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.280630618191063, 'loss.cls.alpha': 0.6196250032013615, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 00:35:34,951] The parameter `tok.doc_stride` in Trial#4967 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 4967 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.210110257172889e-05
  Dropout: 0.1724753896297601
================================================================================

[I 2025-11-08 00:40:04,123] Trial 4967 pruned. Pruned at step 10 with metric 0.5884
[I 2025-11-08 00:40:04,746] Trial 4968 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4969 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.2892553073283315e-05
  Dropout: 0.12115411685116317
================================================================================

[I 2025-11-08 00:42:08,229] Trial 4969 pruned. Pruned at step 10 with metric 0.5554
[W 2025-11-08 00:42:08,812] The parameter `tok.doc_stride` in Trial#4970 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-08 00:42:08,868] Trial 4970 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-08 00:42:09,435] The parameter `tok.doc_stride` in Trial#4971 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-08 00:42:09,492] Trial 4971 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4972 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 7.561172477160314e-05
  Dropout: 0.20629643712750773
================================================================================

[I 2025-11-08 00:51:34,794] Trial 4972 finished with value: 0.4489247311827957 and parameters: {'seed': 15996, 'model.name': 'xlm-roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.561172477160314e-05, 'optim.weight_decay': 3.81982564143958e-05, 'optim.beta1': 0.9251821317654723, 'optim.beta2': 0.9832279222004223, 'optim.eps': 5.633381152169635e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.08735525494596205, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.0268005794034805, 'model.dropout': 0.20629643712750773, 'model.attn_dropout': 0.21684209845658184, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8555919758398792, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.42417466054760383, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.15477849660114423, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 00:51:35,585] Trial 4973 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-08 00:51:36,325] The parameter `tok.doc_stride` in Trial#4974 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 300 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 300 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 4974 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 3.204564170240702e-05
  Dropout: 0.08241821860908241
================================================================================

[I 2025-11-08 01:00:09,334] Trial 4966 pruned. Pruned at step 27 with metric 0.5944
[I 2025-11-08 01:00:10,153] Trial 4975 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
[GPU RESET] Performing periodic GPU reset after 300 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 300 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 4976 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 6.454002228497874e-05
  Dropout: 0.06866248700271213
================================================================================

[I 2025-11-08 01:01:17,215] Trial 4974 finished with value: 0.44141689373297005 and parameters: {'seed': 40372, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 3.204564170240702e-05, 'optim.weight_decay': 0.0003431929361760849, 'optim.beta1': 0.9471117174616646, 'optim.beta2': 0.9608776622514619, 'optim.eps': 4.7930533154543625e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.04460144519514212, 'train.clip_grad': 1.4577203128912422, 'model.dropout': 0.08241821860908241, 'model.attn_dropout': 0.22126793427511438, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8223702474327729, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.34064295990621957, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.144457367484369, 'loss.cls.alpha': 0.2716779777864573, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 01:01:17,883] Trial 4977 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-08 01:01:18,488] Trial 4978 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 4979 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.442519077361864e-05
  Dropout: 0.20574884592221362
================================================================================

[I 2025-11-08 01:01:27,634] Trial 4979 pruned. OOM: roberta-base bs=64 len=384
[I 2025-11-08 01:01:29,908] Trial 4976 pruned. OOM: roberta-large bs=8 len=192
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 4979 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 512 with grad_accum=8)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 44.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 4980 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.281000789818298e-05
  Dropout: 0.3706897956081436
================================================================================


[OOM] Trial 4976 exceeded GPU memory:
  Model: roberta-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 44.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

[I 2025-11-08 01:01:31,047] Trial 4981 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 4982 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.2306838218953285e-06
  Dropout: 0.35821932817751023
================================================================================

[I 2025-11-08 01:11:20,930] Trial 4980 pruned. Pruned at step 27 with metric 0.6722
[I 2025-11-08 01:11:21,560] Trial 4983 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4984 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.404471756560305e-05
  Dropout: 0.3876423487349987
================================================================================

[I 2025-11-08 01:16:06,759] Trial 4982 pruned. Pruned at step 18 with metric 0.6069
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 4985 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.771677148686174e-06
  Dropout: 0.020108849033572115
================================================================================

[I 2025-11-08 01:26:06,940] Trial 4984 pruned. Pruned at step 10 with metric 0.5880
[W 2025-11-08 01:26:07,626] The parameter `tok.doc_stride` in Trial#4986 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 4986 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1502852432121175e-05
  Dropout: 0.3341769609537495
================================================================================

[I 2025-11-08 01:37:50,218] Trial 4986 pruned. Pruned at step 21 with metric 0.5750
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4987 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 7.065659343588769e-06
  Dropout: 0.3701192314133004
================================================================================

[I 2025-11-08 01:39:18,408] Trial 4985 pruned. Pruned at step 10 with metric 0.5693

================================================================================
TRIAL 4988 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.79076412136712e-05
  Dropout: 0.2057483789633221
================================================================================

[I 2025-11-08 01:48:11,416] Trial 4987 pruned. Pruned at step 15 with metric 0.6250
[W 2025-11-08 01:48:12,026] The parameter `tok.doc_stride` in Trial#4989 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4989 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.1021732760695517e-05
  Dropout: 0.06433379309030512
================================================================================

[I 2025-11-08 01:48:19,299] Trial 4988 pruned. Pruned at step 10 with metric 0.5900

================================================================================
TRIAL 4990 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.647648973284e-06
  Dropout: 0.3670210441070471
================================================================================

[I 2025-11-08 01:50:33,803] Trial 4989 pruned. Pruned at step 10 with metric 0.6450

================================================================================
TRIAL 4991 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.1505713354357038e-05
  Dropout: 0.08466413011394477
================================================================================

[I 2025-11-08 01:54:26,539] Trial 4991 pruned. Pruned at step 10 with metric 0.6002

================================================================================
TRIAL 4992 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.151418689053404e-06
  Dropout: 0.18121763190959944
================================================================================

[I 2025-11-08 02:00:47,657] Trial 4992 pruned. Pruned at step 12 with metric 0.6216
[I 2025-11-08 02:00:48,282] Trial 4993 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 4994 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 8.76576543806344e-06
  Dropout: 0.06443203264361101
================================================================================

[I 2025-11-08 02:06:26,235] Trial 4994 pruned. Pruned at step 9 with metric 0.5788

================================================================================
TRIAL 4995 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0945053000861476e-05
  Dropout: 0.3536751597876305
================================================================================

[I 2025-11-08 02:18:32,439] Trial 4995 pruned. Pruned at step 18 with metric 0.6077
[I 2025-11-08 02:18:33,087] Trial 4996 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 02:18:33,689] Trial 4997 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-08 02:18:34,300] Trial 4998 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-08 02:18:34,904] Trial 4999 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-08 02:18:35,509] Trial 5000 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5001 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.316927663952983e-06
  Dropout: 0.1678907483574886
================================================================================

[I 2025-11-08 02:30:27,712] Trial 4990 finished with value: 0.6472140426020115 and parameters: {'seed': 27652, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 7.647648973284e-06, 'optim.weight_decay': 2.5273032046190124e-05, 'optim.beta1': 0.8877300621517845, 'optim.beta2': 0.9792582041162622, 'optim.eps': 4.300709968451548e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18616695942723704, 'train.clip_grad': 1.466492783952439, 'model.dropout': 0.3670210441070471, 'model.attn_dropout': 0.20681738221894552, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9609431155842558, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4405764259892651, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.5571868783867506, 'loss.cls.alpha': 0.7622130501345864, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 02:30:28,353] Trial 5002 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 02:30:28,958] Trial 5003 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-08 02:30:29,528] The parameter `tok.doc_stride` in Trial#5004 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 5004 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.497870536062212e-05
  Dropout: 0.1988076442237245
================================================================================

[I 2025-11-08 02:31:24,220] Trial 5001 pruned. Pruned at step 16 with metric 0.6315
[I 2025-11-08 02:31:25,091] Trial 5005 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5006 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.935789577226463e-06
  Dropout: 0.4596717878903626
================================================================================

[I 2025-11-08 02:34:23,799] Trial 5004 pruned. Pruned at step 7 with metric 0.6077
[I 2025-11-08 02:34:24,431] Trial 5007 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-08 02:34:25,030] Trial 5008 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5009 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 8.459349152917143e-05
  Dropout: 0.28752245207142446
================================================================================

[I 2025-11-08 02:38:37,387] Trial 5006 pruned. Pruned at step 27 with metric 0.6562
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5010 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.248808362694386e-05
  Dropout: 0.036316823766649464
================================================================================

[I 2025-11-08 03:02:32,269] Trial 5009 finished with value: 0.42896935933147634 and parameters: {'seed': 62798, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 8.459349152917143e-05, 'optim.weight_decay': 0.000336358862901788, 'optim.beta1': 0.8156940466530302, 'optim.beta2': 0.9838177291042928, 'optim.eps': 4.301016386614808e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13797104233389224, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.41249170105672195, 'model.dropout': 0.28752245207142446, 'model.attn_dropout': 0.06660918961100887, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9250644509938427, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.47094435247906796, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8813525019911927, 'loss.cls.alpha': 0.6090353111696645, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 03:02:32,862] The parameter `tok.doc_stride` in Trial#5011 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5011 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6604812580002077e-05
  Dropout: 0.00911729969344282
================================================================================

[I 2025-11-08 03:06:48,767] Trial 5011 pruned. Pruned at step 9 with metric 0.5064

================================================================================
TRIAL 5012 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.24571104011476e-06
  Dropout: 0.4649828789036715
================================================================================

[I 2025-11-08 03:09:09,583] Trial 5010 finished with value: 0.4474393530997305 and parameters: {'seed': 64112, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.248808362694386e-05, 'optim.weight_decay': 0.054170328514710285, 'optim.beta1': 0.9445805898781615, 'optim.beta2': 0.9883014311445215, 'optim.eps': 4.3370373435109556e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.08876967282764789, 'sched.poly_power': 0.6339016253831898, 'train.clip_grad': 1.171542653247509, 'model.dropout': 0.036316823766649464, 'model.attn_dropout': 0.2736884297473407, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8981257384444981, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.22625406595902137, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.06083039608624, 'loss.cls.alpha': 0.8535758021984383, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5013 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.8102625018739186e-05
  Dropout: 0.056425158479941256
================================================================================

[I 2025-11-08 03:13:49,074] Trial 5013 pruned. Pruned at step 7 with metric 0.5968

================================================================================
TRIAL 5014 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.6073412607101415e-05
  Dropout: 0.2428416873195025
================================================================================

[I 2025-11-08 03:33:41,696] Trial 5014 finished with value: 0.6537162162162162 and parameters: {'seed': 51870, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.6073412607101415e-05, 'optim.weight_decay': 0.1077882136607985, 'optim.beta1': 0.8121144096831718, 'optim.beta2': 0.9824563907996078, 'optim.eps': 7.060093800118475e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.12791168053515536, 'train.clip_grad': 0.7515470555807908, 'model.dropout': 0.2428416873195025, 'model.attn_dropout': 0.21279485008977078, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8691726716246315, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.06681852501116456, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.494125586928351, 'loss.cls.alpha': 0.7589069362257601, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 33 (patience=20)

================================================================================
TRIAL 5015 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.679680618961703e-06
  Dropout: 0.41971065903264054
================================================================================

[I 2025-11-08 03:33:59,122] Trial 5012 pruned. Pruned at step 14 with metric 0.6476

================================================================================
TRIAL 5016 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.7318814058182875e-06
  Dropout: 0.4193376601582682
================================================================================

[I 2025-11-08 03:41:47,676] Trial 5015 pruned. Pruned at step 10 with metric 0.6042
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5017 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 7.367157732615709e-05
  Dropout: 0.08902837181566522
================================================================================

[I 2025-11-08 03:41:54,657] Trial 5017 pruned. OOM: microsoft/deberta-v3-large bs=12 len=352
[I 2025-11-08 03:41:56,873] Trial 5016 pruned. OOM: xlm-roberta-base bs=8 len=288

[OOM] Trial 5017 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 62.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5018 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.0996507486904551e-05
  Dropout: 0.17401537783236207
================================================================================


[OOM] Trial 5016 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 142.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5019 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 6.831257305692131e-05
  Dropout: 0.1611704796787516
================================================================================

[I 2025-11-08 03:42:03,569] Trial 5019 pruned. OOM: roberta-base bs=64 len=352
[I 2025-11-08 03:42:04,346] Trial 5018 pruned. OOM: bert-base-uncased bs=64 len=192
[I 2025-11-08 03:42:05,199] Trial 5020 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-08 03:42:05,504] The parameter `tok.doc_stride` in Trial#5021 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-08 03:42:06,023] Trial 5022 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-08 03:42:06,617] Trial 5023 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)

[OOM] Trial 5019 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 102.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5018 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 182.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5021 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1668940921251669e-05
  Dropout: 0.17741912667835397
================================================================================


================================================================================
TRIAL 5024 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.4016622181960906e-05
  Dropout: 0.20930047276861355
================================================================================

[I 2025-11-08 03:45:36,237] Trial 5024 pruned. Pruned at step 15 with metric 0.6002

================================================================================
TRIAL 5025 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.423358153184999e-06
  Dropout: 0.21833911216151528
================================================================================

[I 2025-11-08 03:50:33,873] Trial 5021 pruned. Pruned at step 12 with metric 0.5997
[I 2025-11-08 03:50:34,519] Trial 5026 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5027 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.2180288503592284e-05
  Dropout: 0.1468641308901328
================================================================================

[I 2025-11-08 04:15:47,799] Trial 5027 finished with value: 0.4429347826086957 and parameters: {'seed': 20237, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.2180288503592284e-05, 'optim.weight_decay': 0.0008454274469921686, 'optim.beta1': 0.8249074584612595, 'optim.beta2': 0.9971046837348418, 'optim.eps': 1.071277107959109e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.03138653244246773, 'train.clip_grad': 1.4951278733415991, 'model.dropout': 0.1468641308901328, 'model.attn_dropout': 0.1715738893261376, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8309936487643947, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.21462525286449646, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1980920552948904, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 04:15:48,380] The parameter `tok.doc_stride` in Trial#5028 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5028 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.2008554084767735e-05
  Dropout: 0.03397138632179822
================================================================================

[I 2025-11-08 04:22:08,054] Trial 5028 pruned. Pruned at step 10 with metric 0.5521
[I 2025-11-08 04:22:08,684] Trial 5029 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-08 04:22:09,287] Trial 5030 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-08 04:22:09,852] The parameter `tok.doc_stride` in Trial#5031 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5031 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.6338712122929196e-05
  Dropout: 0.059339560872668784
================================================================================

[I 2025-11-08 04:37:42,877] Trial 5025 finished with value: 0.6315346667176565 and parameters: {'seed': 6957, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 7.423358153184999e-06, 'optim.weight_decay': 0.00411503739308155, 'optim.beta1': 0.9194117976984337, 'optim.beta2': 0.9653961634694389, 'optim.eps': 3.620995484546465e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.12386380473459797, 'sched.poly_power': 0.7316180331282536, 'train.clip_grad': 0.5894329847012709, 'model.dropout': 0.21833911216151528, 'model.attn_dropout': 0.1965023322801231, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8981755470705803, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.16187465609398521, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.595276878187844, 'loss.cls.alpha': 0.4021194963359668, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 5032 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.857175133349569e-06
  Dropout: 0.15692882615838144
================================================================================

[I 2025-11-08 04:50:16,253] Trial 5032 pruned. Pruned at step 27 with metric 0.5788

================================================================================
TRIAL 5033 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2471599576926102e-05
  Dropout: 0.2578201344034945
================================================================================

[I 2025-11-08 04:55:50,831] Trial 5031 finished with value: 0.6253559437290153 and parameters: {'seed': 46069, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.6338712122929196e-05, 'optim.weight_decay': 0.002104198151503577, 'optim.beta1': 0.9201470652215578, 'optim.beta2': 0.9671000108790891, 'optim.eps': 7.055496789932742e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.007677995891956897, 'sched.cosine_cycles': 4, 'train.clip_grad': 1.227746252598111, 'model.dropout': 0.059339560872668784, 'model.attn_dropout': 0.10891278445066088, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8630254691605003, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.4677937873673519, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16508714023950738, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 43 (patience=20)

================================================================================
TRIAL 5034 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.9295571785275615e-05
  Dropout: 0.07149919349436729
================================================================================

[I 2025-11-08 04:57:47,080] Trial 5034 pruned. Pruned at step 10 with metric 0.6447
[I 2025-11-08 04:57:47,704] Trial 5035 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-08 04:57:48,271] The parameter `tok.doc_stride` in Trial#5036 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5036 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0925044457030559e-05
  Dropout: 0.3674551189541037
================================================================================

[I 2025-11-08 05:00:27,096] Trial 5036 pruned. Pruned at step 9 with metric 0.6207

================================================================================
TRIAL 5037 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 6.319655357664566e-05
  Dropout: 0.17262753229765873
================================================================================

[I 2025-11-08 05:03:25,567] Trial 5037 pruned. Pruned at step 12 with metric 0.6387

================================================================================
TRIAL 5038 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.9215423897440557e-05
  Dropout: 0.038032488836785915
================================================================================

[I 2025-11-08 05:04:51,128] Trial 5038 pruned. Pruned at step 7 with metric 0.5780
[W 2025-11-08 05:04:51,720] The parameter `tok.doc_stride` in Trial#5039 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5039 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.4270588673094675e-06
  Dropout: 0.4149463648036592
================================================================================

[I 2025-11-08 05:12:36,669] Trial 5039 pruned. Pruned at step 25 with metric 0.5603
[I 2025-11-08 05:12:37,319] Trial 5040 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-08 05:12:37,935] Trial 5041 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-08 05:12:38,548] Trial 5042 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-08 05:12:39,144] Trial 5043 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5044 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.178440942725412e-05
  Dropout: 0.02348450332438441
================================================================================

[I 2025-11-08 05:22:00,816] Trial 5033 finished with value: 0.6602065536146883 and parameters: {'seed': 42893, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.2471599576926102e-05, 'optim.weight_decay': 1.3247323094173162e-06, 'optim.beta1': 0.8451399486178411, 'optim.beta2': 0.986337150670944, 'optim.eps': 8.177544486645484e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.16910018447800002, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.777848922995088, 'model.dropout': 0.2578201344034945, 'model.attn_dropout': 0.15718947486885854, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9390490520391115, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.311232928525026, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.662980548051468, 'loss.cls.alpha': 0.6681689233273483, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 05:22:01,453] The parameter `tok.doc_stride` in Trial#5045 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 5045 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.375976625134381e-05
  Dropout: 0.21078029864520226
================================================================================

[I 2025-11-08 05:36:14,266] Trial 5045 finished with value: 0.6468561584840655 and parameters: {'seed': 48367, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.375976625134381e-05, 'optim.weight_decay': 0.00014932234263551321, 'optim.beta1': 0.876634754487203, 'optim.beta2': 0.9772398769039494, 'optim.eps': 3.4162102936978915e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.17311578060189015, 'train.clip_grad': 0.8133614294637943, 'model.dropout': 0.21078029864520226, 'model.attn_dropout': 0.16865667664276096, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9146476281047116, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.3519834568672716, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1716499722376616, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 39 (patience=20)

================================================================================
TRIAL 5046 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.3098547149448878e-05
  Dropout: 0.13106955271352408
================================================================================

[I 2025-11-08 05:36:18,508] Trial 5046 pruned. OOM: roberta-base bs=64 len=288
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5046 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 144.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5047 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.0571711245774837e-05
  Dropout: 0.43264368205121617
================================================================================

[I 2025-11-08 06:01:37,056] Trial 5044 pruned. Pruned at step 27 with metric 0.6694

================================================================================
TRIAL 5048 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 5.8497311235609014e-05
  Dropout: 0.19797824049622229
================================================================================

[I 2025-11-08 06:37:27,283] Trial 5048 finished with value: 0.4533333333333333 and parameters: {'seed': 65379, 'model.name': 'bert-large-uncased', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.8497311235609014e-05, 'optim.weight_decay': 1.4650781366510074e-05, 'optim.beta1': 0.8797535636308507, 'optim.beta2': 0.963911961495902, 'optim.eps': 5.996098808110825e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04407523828681994, 'sched.poly_power': 0.5984156172522653, 'train.clip_grad': 0.7722850174344399, 'model.dropout': 0.19797824049622229, 'model.attn_dropout': 0.13258974589944902, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8126105935699925, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.3357679572059994, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18195686466409738, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5049 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.574587597516299e-05
  Dropout: 0.1542470870053193
================================================================================

[I 2025-11-08 06:40:47,161] Trial 5047 finished with value: 0.7197156138911676 and parameters: {'seed': 62975, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.0571711245774837e-05, 'optim.weight_decay': 1.4718481569005512e-05, 'optim.beta1': 0.8852484462702791, 'optim.beta2': 0.9671141119290628, 'optim.eps': 1.8599998557415536e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.13145322694945355, 'train.clip_grad': 0.9370718746444446, 'model.dropout': 0.43264368205121617, 'model.attn_dropout': 0.17036023320995142, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9365548737877022, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.43596595204455973, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.978670881621445, 'loss.cls.alpha': 0.24002188848429185, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 74 (patience=20)

================================================================================
TRIAL 5050 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.903236362875689e-05
  Dropout: 0.3777274985545349
================================================================================

[I 2025-11-08 06:54:37,455] Trial 5049 finished with value: 0.6863100957261308 and parameters: {'seed': 57735, 'model.name': 'bert-base-uncased', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.574587597516299e-05, 'optim.weight_decay': 1.3672940643187996e-05, 'optim.beta1': 0.9046300674690434, 'optim.beta2': 0.959640330465133, 'optim.eps': 4.319940053761656e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.04343975300124362, 'train.clip_grad': 1.3899465965115423, 'model.dropout': 0.1542470870053193, 'model.attn_dropout': 0.10728840359564182, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8191717650895146, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.19841183667750598, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.22320408291556, 'loss.cls.alpha': 0.32936833724079967, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 06:54:38,091] Trial 5051 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-08 06:54:38,687] Trial 5052 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5053 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.5078674465063776e-05
  Dropout: 0.40066293253954594
================================================================================

[I 2025-11-08 07:07:09,633] Trial 5053 pruned. Pruned at step 8 with metric 0.6124
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5054 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.5530421210629474e-05
  Dropout: 0.1219721725608327
================================================================================

[I 2025-11-08 07:39:23,243] Trial 5054 pruned. Pruned at step 27 with metric 0.6238
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5055 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 9.128868596704505e-06
  Dropout: 0.27399160543749523
================================================================================

[I 2025-11-08 07:42:09,249] Trial 5055 pruned. Pruned at step 10 with metric 0.6435
[I 2025-11-08 07:42:09,867] Trial 5056 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-08 07:42:10,437] The parameter `tok.doc_stride` in Trial#5057 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5057 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.469332624470018e-06
  Dropout: 0.17007880244038992
================================================================================

[I 2025-11-08 07:44:49,022] Trial 5050 finished with value: 0.6750132108450876 and parameters: {'seed': 37733, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.903236362875689e-05, 'optim.weight_decay': 0.12790731696001775, 'optim.beta1': 0.8032183470466586, 'optim.beta2': 0.9511593094778796, 'optim.eps': 9.223084617545799e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.02722500994353577, 'sched.poly_power': 0.8820115852969522, 'train.clip_grad': 1.108952586434066, 'model.dropout': 0.3777274985545349, 'model.attn_dropout': 0.06968786223452836, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9453638515254268, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.19399348440170477, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.3622765881934855, 'loss.cls.alpha': 0.4834873484350848, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5058 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 9.427702914953338e-06
  Dropout: 0.46946027890473696
================================================================================

[I 2025-11-08 07:52:33,260] Trial 5057 pruned. Pruned at step 20 with metric 0.6242
[I 2025-11-08 07:52:33,911] Trial 5059 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5060 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.58145123398658e-06
  Dropout: 0.21378964603457473
================================================================================

[I 2025-11-08 08:06:35,754] Trial 5060 finished with value: 0.6561290322580645 and parameters: {'seed': 57862, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 6.58145123398658e-06, 'optim.weight_decay': 5.709412446297044e-05, 'optim.beta1': 0.8518260419861302, 'optim.beta2': 0.9808742723170669, 'optim.eps': 5.4085726200830345e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1716688162794085, 'train.clip_grad': 0.8617262382295231, 'model.dropout': 0.21378964603457473, 'model.attn_dropout': 0.09078515396524786, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8186219838312118, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.3010982472576094, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.67664255041705, 'loss.cls.alpha': 0.7868447451062381, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 5061 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 4.986672837771657e-05
  Dropout: 0.07724807429606441
================================================================================

[I 2025-11-08 08:13:15,849] Trial 5061 pruned. Pruned at step 11 with metric 0.5941
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5062 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 4.294964014501535e-05
  Dropout: 0.044456032824715795
================================================================================

[I 2025-11-08 08:38:38,705] Trial 5062 finished with value: 0.4562334217506631 and parameters: {'seed': 2744, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 4.294964014501535e-05, 'optim.weight_decay': 0.0005639898006644544, 'optim.beta1': 0.9423654634765449, 'optim.beta2': 0.9761019026604293, 'optim.eps': 7.531224882179445e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.09213397499705733, 'sched.poly_power': 0.9081592112997514, 'train.clip_grad': 1.4129628302059474, 'model.dropout': 0.044456032824715795, 'model.attn_dropout': 0.27470897092932345, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.895208268413504, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4242159542379339, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.4856498882087585, 'loss.cls.alpha': 0.5693717849552524, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5063 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 9.866133107504808e-05
  Dropout: 0.23723124632052664
================================================================================

[I 2025-11-08 08:43:35,566] Trial 5058 finished with value: 0.4429347826086957 and parameters: {'seed': 39337, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 9.427702914953338e-06, 'optim.weight_decay': 0.024208735989503023, 'optim.beta1': 0.8415172968356681, 'optim.beta2': 0.9725622473952937, 'optim.eps': 6.789024577086168e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.12552378542376202, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1809550385383571, 'model.dropout': 0.46946027890473696, 'model.attn_dropout': 0.2651335267584147, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8935971430235604, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.3234615050666695, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.330410661023865, 'loss.cls.alpha': 0.2505770601871836, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5064 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 4.182430301702419e-05
  Dropout: 0.19016032686964449
================================================================================

[I 2025-11-08 09:01:39,063] Trial 5063 finished with value: 0.4273743016759777 and parameters: {'seed': 17914, 'model.name': 'bert-large-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.866133107504808e-05, 'optim.weight_decay': 0.06620419770847191, 'optim.beta1': 0.810329583594974, 'optim.beta2': 0.9564545587758341, 'optim.eps': 4.204742488517355e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.011391425022634127, 'sched.poly_power': 0.6248604969727172, 'train.clip_grad': 1.2547933733312842, 'model.dropout': 0.23723124632052664, 'model.attn_dropout': 0.061496915758042434, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9393742060254847, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.26008858906149335, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16635060008433372, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 09:01:39,674] The parameter `tok.doc_stride` in Trial#5065 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5065 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.4546101400473876e-05
  Dropout: 0.3977477526525609
================================================================================

[I 2025-11-08 09:03:09,540] Trial 5064 pruned. Pruned at step 9 with metric 0.5926
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5066 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.0012583366851523e-05
  Dropout: 0.27619124907382187
================================================================================

[I 2025-11-08 09:09:33,845] Trial 5066 pruned. Pruned at step 9 with metric 0.6428
[I 2025-11-08 09:09:34,483] Trial 5067 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5068 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.7862375683143678e-05
  Dropout: 0.26536559631777584
================================================================================

[I 2025-11-08 09:29:15,840] Trial 5065 finished with value: 0.4489247311827957 and parameters: {'seed': 15656, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.4546101400473876e-05, 'optim.weight_decay': 9.401790716585301e-05, 'optim.beta1': 0.9242061956376588, 'optim.beta2': 0.9768141904554926, 'optim.eps': 1.0034213238354862e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.03812964608530135, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1140657168803962, 'model.dropout': 0.3977477526525609, 'model.attn_dropout': 0.26351568772959855, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8447623506687715, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.27337563073626403, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.0398980155454405, 'loss.cls.alpha': 0.35672058173391874, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 09:29:16,487] Trial 5069 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-08 09:29:17,107] Trial 5070 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 09:29:17,725] Trial 5071 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5072 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.3802378449281612e-05
  Dropout: 0.08351709101461596
================================================================================

[I 2025-11-08 09:29:25,853] Trial 5068 pruned. OOM: xlm-roberta-base bs=8 len=352
[W 2025-11-08 09:29:27,222] The parameter `tok.doc_stride` in Trial#5073 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-08 09:29:27,248] Trial 5072 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5068 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 54.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5072 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 54.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5073 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.5209579546506886e-05
  Dropout: 0.07519781278808584
================================================================================


================================================================================
TRIAL 5074 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 3.086027077341714e-05
  Dropout: 0.14830640837086698
================================================================================

[I 2025-11-08 09:50:23,040] Trial 5074 finished with value: 0.43526170798898073 and parameters: {'seed': 1642, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.086027077341714e-05, 'optim.weight_decay': 0.00309196492793127, 'optim.beta1': 0.8838600354356555, 'optim.beta2': 0.9561094821766092, 'optim.eps': 5.233727594085044e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0077598390404591475, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.3097818701197341, 'model.dropout': 0.14830640837086698, 'model.attn_dropout': 0.18598235974610347, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8292198577851028, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.4310836632031493, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1545029749573861, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5075 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.2902624855712825e-05
  Dropout: 0.4082155329394232
================================================================================

[I 2025-11-08 09:52:49,722] Trial 5075 pruned. Pruned at step 11 with metric 0.5547
[I 2025-11-08 09:52:50,355] Trial 5076 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5077 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.6444709560914443e-05
  Dropout: 0.2144748338052124
================================================================================

[I 2025-11-08 09:55:55,543] Trial 5077 pruned. Pruned at step 14 with metric 0.6079
[W 2025-11-08 09:55:56,127] The parameter `tok.doc_stride` in Trial#5078 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5078 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 6.928139691941384e-05
  Dropout: 0.12874900685034085
================================================================================

[I 2025-11-08 10:09:01,550] Trial 5073 pruned. Pruned at step 15 with metric 0.5349

================================================================================
TRIAL 5079 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.471558896682067e-05
  Dropout: 0.17847502030880408
================================================================================

[I 2025-11-08 10:38:00,416] Trial 5078 finished with value: 0.4576719576719577 and parameters: {'seed': 51957, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.928139691941384e-05, 'optim.weight_decay': 0.000504687592332615, 'optim.beta1': 0.935310222936181, 'optim.beta2': 0.9923916228765154, 'optim.eps': 1.2099369278311612e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.09259133603629965, 'sched.poly_power': 0.6001868182699264, 'train.clip_grad': 1.450631187683619, 'model.dropout': 0.12874900685034085, 'model.attn_dropout': 0.2212863665664295, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.9205211428526635, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.22785764981091491, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.569641882706961, 'loss.cls.alpha': 0.29887475412426456, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5080 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.746712312604422e-05
  Dropout: 0.10250563809424788
================================================================================

[I 2025-11-08 10:53:35,428] Trial 5080 pruned. Pruned at step 10 with metric 0.4368
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5081 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.606951280382295e-05
  Dropout: 0.06228876575676848
================================================================================

[I 2025-11-08 10:59:04,738] Trial 5079 pruned. Pruned at step 26 with metric 0.5316
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5082 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 6.083109032080092e-06
  Dropout: 0.3894456038551835
================================================================================

[I 2025-11-08 10:59:21,034] Trial 5081 pruned. Pruned at step 13 with metric 0.6038
[I 2025-11-08 10:59:21,677] Trial 5083 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 10:59:22,277] Trial 5084 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5085 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.396999013239701e-05
  Dropout: 0.49223842999887224
================================================================================

[I 2025-11-08 11:17:30,644] Trial 5082 pruned. Pruned at step 11 with metric 0.6471

================================================================================
TRIAL 5086 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.4574269219441688e-05
  Dropout: 0.1452861228230974
================================================================================

[I 2025-11-08 11:22:58,973] Trial 5085 finished with value: 0.6816770186335404 and parameters: {'seed': 34925, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.396999013239701e-05, 'optim.weight_decay': 0.00718080561825227, 'optim.beta1': 0.8584915365247637, 'optim.beta2': 0.9545489340719624, 'optim.eps': 5.626022752922555e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.05044868221250593, 'sched.poly_power': 0.7689781076861928, 'train.clip_grad': 1.2716175284253095, 'model.dropout': 0.49223842999887224, 'model.attn_dropout': 0.23591549058728667, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8713278942497029, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.4404328586507588, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.14464140444919737, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 11:22:59,622] Trial 5087 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 5088 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.4901126185782641e-05
  Dropout: 0.18001399579052352
================================================================================

[I 2025-11-08 11:28:05,974] Trial 5088 pruned. Pruned at step 10 with metric 0.5927
[I 2025-11-08 11:28:06,604] Trial 5089 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[W 2025-11-08 11:28:07,173] The parameter `tok.doc_stride` in Trial#5090 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5090 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.638451262730941e-06
  Dropout: 0.44625465686043086
================================================================================

[I 2025-11-08 11:32:42,102] Trial 5086 pruned. Pruned at step 9 with metric 0.6252
[I 2025-11-08 11:32:42,742] Trial 5091 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[W 2025-11-08 11:32:43,304] The parameter `tok.doc_stride` in Trial#5092 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-08 11:32:43,364] Trial 5092 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-08 11:32:43,963] Trial 5093 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-08 11:32:44,565] Trial 5094 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5095 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 9.269797311514976e-06
  Dropout: 0.04164515360161234
================================================================================

[I 2025-11-08 11:34:26,343] Trial 5090 pruned. Pruned at step 17 with metric 0.5927
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5096 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.1684326946778138e-05
  Dropout: 0.44556010487427716
================================================================================

[I 2025-11-08 11:42:05,550] Trial 5095 pruned. Pruned at step 19 with metric 0.5827
[I 2025-11-08 11:42:06,208] Trial 5097 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-08 11:42:06,826] Trial 5098 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5099 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 9.226307712229186e-06
  Dropout: 0.41761485913954843
================================================================================

[I 2025-11-08 11:55:11,247] Trial 5096 pruned. Pruned at step 10 with metric 0.5620

================================================================================
TRIAL 5100 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.2887214662272372e-05
  Dropout: 0.2082403604358758
================================================================================

[I 2025-11-08 11:59:22,635] Trial 5100 pruned. Pruned at step 8 with metric 0.5314
[I 2025-11-08 11:59:23,259] Trial 5101 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-08 11:59:23,823] The parameter `tok.doc_stride` in Trial#5102 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5102 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.4665207626840024e-05
  Dropout: 0.3299341947465478
================================================================================

[I 2025-11-08 12:02:31,243] Trial 5102 pruned. Pruned at step 9 with metric 0.6297

================================================================================
TRIAL 5103 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.99371449236125e-06
  Dropout: 0.01618637377142254
================================================================================

[I 2025-11-08 12:07:22,968] Trial 5103 pruned. Pruned at step 11 with metric 0.6538
[I 2025-11-08 12:07:23,593] Trial 5104 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5105 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.8212179683403347e-05
  Dropout: 0.17956234040172256
================================================================================

[I 2025-11-08 12:09:48,649] Trial 5099 pruned. Pruned at step 9 with metric 0.6165
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5106 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.223281306545485e-06
  Dropout: 0.40163051509595143
================================================================================

[I 2025-11-08 12:16:19,036] Trial 5105 finished with value: 0.6779160382101559 and parameters: {'seed': 64561, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.8212179683403347e-05, 'optim.weight_decay': 0.00013232059050429635, 'optim.beta1': 0.90382852573068, 'optim.beta2': 0.9591728689576583, 'optim.eps': 6.033932712694307e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0428383104470983, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.223102291139484, 'model.dropout': 0.17956234040172256, 'model.attn_dropout': 0.1892604623065968, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8491546410016912, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.23170791432894233, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.14765718720105536, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 5107 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.0994795249410635e-05
  Dropout: 0.07632752857199433
================================================================================

[I 2025-11-08 12:31:27,179] Trial 5107 pruned. Pruned at step 27 with metric 0.6356
[I 2025-11-08 12:31:27,806] Trial 5108 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 12:31:28,407] Trial 5109 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5110 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.4841343882286376e-05
  Dropout: 0.1625642093736123
================================================================================

[I 2025-11-08 12:47:10,752] Trial 5110 pruned. Pruned at step 13 with metric 0.6631
[W 2025-11-08 12:47:11,338] The parameter `tok.doc_stride` in Trial#5111 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5111 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 8.648245175686415e-06
  Dropout: 0.012500125072569077
================================================================================

[I 2025-11-08 12:54:24,526] Trial 5111 pruned. Pruned at step 26 with metric 0.6095
[I 2025-11-08 12:54:25,169] Trial 5112 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-08 12:54:25,830] Trial 5113 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5114 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.9876918430698298e-05
  Dropout: 0.2612179521749842
================================================================================

[I 2025-11-08 13:04:08,432] Trial 5114 pruned. Pruned at step 10 with metric 0.6125

================================================================================
TRIAL 5115 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.703846644199261e-05
  Dropout: 0.1841596386202465
================================================================================

[I 2025-11-08 13:14:34,155] Trial 5115 pruned. Pruned at step 14 with metric 0.6042

================================================================================
TRIAL 5116 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.8221264974591457e-05
  Dropout: 0.18417263715490895
================================================================================

[I 2025-11-08 13:18:52,883] Trial 5116 pruned. Pruned at step 9 with metric 0.5692
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5117 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.4869249570471536e-05
  Dropout: 0.13320797851143654
================================================================================

[I 2025-11-08 13:39:50,682] Trial 5117 pruned. Pruned at step 8 with metric 0.6200
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5118 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.5083781087814112e-05
  Dropout: 0.13061595079205016
================================================================================

[I 2025-11-08 13:59:26,892] Trial 5106 finished with value: 0.7307591279222485 and parameters: {'seed': 55047, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 8.223281306545485e-06, 'optim.weight_decay': 5.6617489957957815e-05, 'optim.beta1': 0.8503040076851424, 'optim.beta2': 0.9854231614771286, 'optim.eps': 5.379654743637829e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.17323264772965596, 'train.clip_grad': 1.0873384040576197, 'model.dropout': 0.40163051509595143, 'model.attn_dropout': 0.12260493267033931, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9553701997228261, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4681933797895087, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.996461898367609, 'loss.cls.alpha': 0.7036817688588095, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5119 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 5.150776327271618e-05
  Dropout: 0.33645372176970056
================================================================================

[I 2025-11-08 14:04:15,136] Trial 5119 pruned. Pruned at step 12 with metric 0.5274

================================================================================
TRIAL 5120 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.3855709594550634e-05
  Dropout: 0.04067337709281437
================================================================================

[I 2025-11-08 14:09:31,064] Trial 5118 finished with value: 0.6750132108450876 and parameters: {'seed': 63539, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.5083781087814112e-05, 'optim.weight_decay': 9.219527595003987e-06, 'optim.beta1': 0.9049030955360066, 'optim.beta2': 0.9791688997063704, 'optim.eps': 1.4810494005042791e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.0417582966544905, 'train.clip_grad': 1.3686823260189798, 'model.dropout': 0.13061595079205016, 'model.attn_dropout': 0.15831705905089752, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8153481191382516, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.3820043271144537, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.3915064313032035, 'loss.cls.alpha': 0.5685265557675735, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 14:09:31,702] The parameter `tok.doc_stride` in Trial#5121 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-08 14:09:31,757] Trial 5121 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 5122 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.5614522288333916e-05
  Dropout: 0.1252033619935848
================================================================================

[I 2025-11-08 14:19:35,380] Trial 5122 pruned. Pruned at step 27 with metric 0.6285
[I 2025-11-08 14:19:36,021] Trial 5123 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-08 14:19:36,625] Trial 5124 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 14:19:37,232] Trial 5125 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 14:19:37,842] Trial 5126 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5127 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.923597507711916e-05
  Dropout: 0.44721984842082485
================================================================================

[I 2025-11-08 14:36:22,827] Trial 5127 pruned. Pruned at step 12 with metric 0.5986
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5128 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.6796411566093776e-05
  Dropout: 0.28375331657788383
================================================================================

[I 2025-11-08 14:49:22,154] Trial 5128 finished with value: 0.6781789638932496 and parameters: {'seed': 47425, 'model.name': 'roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.6796411566093776e-05, 'optim.weight_decay': 1.840264806260908e-05, 'optim.beta1': 0.8102452575798861, 'optim.beta2': 0.9855688807987019, 'optim.eps': 4.688293591315415e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1212605842864773, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.956617222954921, 'model.dropout': 0.28375331657788383, 'model.attn_dropout': 0.026477851587050696, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9068450732207448, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.4592765292473576, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.590982260293371, 'loss.cls.alpha': 0.6318591810181987, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 5129 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.6901705435823237e-05
  Dropout: 0.2776807574711815
================================================================================

[I 2025-11-08 14:55:49,672] Trial 5120 pruned. Pruned at step 10 with metric 0.6006
[I 2025-11-08 14:55:50,310] Trial 5130 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5131 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.1555495831104875e-05
  Dropout: 0.050114806840514205
================================================================================

[I 2025-11-08 15:01:47,493] Trial 5131 pruned. Pruned at step 8 with metric 0.5949
[I 2025-11-08 15:01:48,117] Trial 5132 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5133 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.820442240294677e-05
  Dropout: 0.3845187057410902
================================================================================

[I 2025-11-08 15:09:11,251] Trial 5129 pruned. Pruned at step 11 with metric 0.6447
[W 2025-11-08 15:09:11,866] The parameter `tok.doc_stride` in Trial#5134 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5134 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.6045595098624895e-05
  Dropout: 0.19550807756552496
================================================================================

[I 2025-11-08 15:14:42,216] Trial 5134 finished with value: 0.6676492262343405 and parameters: {'seed': 31071, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.6045595098624895e-05, 'optim.weight_decay': 0.015243792221107784, 'optim.beta1': 0.8186166871267112, 'optim.beta2': 0.9738940595223133, 'optim.eps': 5.031368087458837e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.16079597239268312, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8559788889435157, 'model.dropout': 0.19550807756552496, 'model.attn_dropout': 0.1500166336394414, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9313585878746621, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.19876510949288262, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19911892236332918, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 5135 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.587657089201338e-06
  Dropout: 0.4624434140721079
================================================================================

[I 2025-11-08 15:23:55,818] Trial 5135 finished with value: 0.7359848484848485 and parameters: {'seed': 39998, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 9.587657089201338e-06, 'optim.weight_decay': 8.929382474021673e-05, 'optim.beta1': 0.876900553285333, 'optim.beta2': 0.9703694401069562, 'optim.eps': 3.416951918931688e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18293745610394008, 'train.clip_grad': 1.0409109560112133, 'model.dropout': 0.4624434140721079, 'model.attn_dropout': 0.12676222582952518, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8700006267033192, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.44150898696004953, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.646970064803369, 'loss.cls.alpha': 0.4302227253774471, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 37 (patience=20)

================================================================================
TRIAL 5136 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 9.945331311055317e-06
  Dropout: 0.3994033564332762
================================================================================

[I 2025-11-08 15:48:26,721] Trial 5133 finished with value: 0.7254175858808698 and parameters: {'seed': 16422, 'model.name': 'xlm-roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.820442240294677e-05, 'optim.weight_decay': 0.03395621215762545, 'optim.beta1': 0.8314431269105365, 'optim.beta2': 0.9667837418825169, 'optim.eps': 8.52240899715731e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0314545119922746, 'sched.poly_power': 0.6047876196264675, 'train.clip_grad': 1.3255189978260253, 'model.dropout': 0.3845187057410902, 'model.attn_dropout': 0.09215129694209936, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9784949373770174, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.26753366585770877, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.679614108734792, 'loss.cls.alpha': 0.22358168805818818, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 15:48:27,350] Trial 5137 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 33 (patience=20)

================================================================================
TRIAL 5138 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.0318354388630856e-05
  Dropout: 0.013844552924437883
================================================================================

[I 2025-11-08 15:56:30,465] Trial 5138 pruned. Pruned at step 27 with metric 0.6494

================================================================================
TRIAL 5139 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.3703697731158446e-05
  Dropout: 0.4504485064377105
================================================================================

[I 2025-11-08 16:00:09,424] Trial 5139 pruned. Pruned at step 13 with metric 0.6210

================================================================================
TRIAL 5140 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 3.440024733743085e-05
  Dropout: 0.038841785367804554
================================================================================

[I 2025-11-08 16:03:42,384] Trial 5136 finished with value: 0.7201747201747202 and parameters: {'seed': 55141, 'model.name': 'bert-large-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.945331311055317e-06, 'optim.weight_decay': 0.007317674909711407, 'optim.beta1': 0.8293168410849976, 'optim.beta2': 0.9680935077752978, 'optim.eps': 4.644393660076505e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.11790498356879335, 'sched.poly_power': 0.7272278762415049, 'train.clip_grad': 1.131730447916644, 'model.dropout': 0.3994033564332762, 'model.attn_dropout': 0.17612313970683535, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9669986735914983, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.12534479196144527, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1625943482253266, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5141 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.9892956068962365e-05
  Dropout: 0.12240513528359112
================================================================================

[I 2025-11-08 16:11:57,195] Trial 5140 pruned. Pruned at step 27 with metric 0.6256

================================================================================
TRIAL 5142 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 8.223407054376277e-06
  Dropout: 0.3463404319254269
================================================================================

[I 2025-11-08 16:23:14,049] Trial 5142 pruned. Pruned at step 14 with metric 0.5872
[I 2025-11-08 16:23:14,693] Trial 5143 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5144 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.875389785956199e-06
  Dropout: 0.4458255351476919
================================================================================

[I 2025-11-08 16:30:01,291] Trial 5144 pruned. Pruned at step 11 with metric 0.6139
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5145 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 9.466296273387922e-06
  Dropout: 0.2643226007796523
================================================================================

[I 2025-11-08 16:33:24,694] Trial 5145 pruned. Pruned at step 9 with metric 0.5186
[W 2025-11-08 16:33:25,303] The parameter `tok.doc_stride` in Trial#5146 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5146 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.2075858094653532e-05
  Dropout: 0.43850992848356396
================================================================================

[I 2025-11-08 16:37:20,903] Trial 5146 pruned. Pruned at step 11 with metric 0.6242

================================================================================
TRIAL 5147 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.0931574633199362e-05
  Dropout: 0.43115417137785805
================================================================================

[I 2025-11-08 16:46:08,006] Trial 5141 pruned. Pruned at step 27 with metric 0.5986
[I 2025-11-08 16:46:08,744] Trial 5148 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 16:46:09,352] Trial 5149 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-08 16:46:09,923] The parameter `tok.doc_stride` in Trial#5150 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5150 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.8694099673975365e-05
  Dropout: 0.4670101048578116
================================================================================

[I 2025-11-08 16:52:49,682] Trial 5150 pruned. Pruned at step 27 with metric 0.6042

================================================================================
TRIAL 5151 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.7419028073468318e-05
  Dropout: 0.11053392315255875
================================================================================

[I 2025-11-08 17:12:09,566] Trial 5151 finished with value: 0.7099315461394318 and parameters: {'seed': 62093, 'model.name': 'bert-base-uncased', 'tok.max_length': 352, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.7419028073468318e-05, 'optim.weight_decay': 3.815326538216359e-06, 'optim.beta1': 0.9013082097976238, 'optim.beta2': 0.96233862243656, 'optim.eps': 1.003314866679952e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.023178323201925505, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.8627811603822129, 'model.dropout': 0.11053392315255875, 'model.attn_dropout': 0.2385441585238407, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8017646971904417, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.32066851717978806, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.038153945481323, 'loss.cls.alpha': 0.5956148975073139, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 48 (patience=20)

================================================================================
TRIAL 5152 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.442296845944965e-06
  Dropout: 0.024616850081552035
================================================================================

[I 2025-11-08 17:32:07,906] Trial 5152 finished with value: 0.7106154714850367 and parameters: {'seed': 53195, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 8.442296845944965e-06, 'optim.weight_decay': 0.0023980996391313987, 'optim.beta1': 0.8630812602927614, 'optim.beta2': 0.96932467150289, 'optim.eps': 1.7109961801402488e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06902302365356094, 'sched.poly_power': 0.592032941799097, 'train.clip_grad': 0.6525066552943655, 'model.dropout': 0.024616850081552035, 'model.attn_dropout': 0.11006082384198616, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8966651637847668, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.23347656116596455, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.989503402532282, 'loss.cls.alpha': 0.3566312184130376, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 17:32:08,533] Trial 5153 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-08 17:32:09,112] The parameter `tok.doc_stride` in Trial#5154 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 54 (patience=20)

================================================================================
TRIAL 5154 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.749713216921771e-05
  Dropout: 0.0972349412229826
================================================================================

[I 2025-11-08 17:32:35,099] Trial 5147 finished with value: 0.6798780487804879 and parameters: {'seed': 23349, 'model.name': 'xlm-roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.0931574633199362e-05, 'optim.weight_decay': 0.06192161631924242, 'optim.beta1': 0.937237621729671, 'optim.beta2': 0.9501464383142302, 'optim.eps': 2.788878517611497e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.0375277522557633, 'sched.poly_power': 0.7508371327420997, 'train.clip_grad': 1.3803551445754243, 'model.dropout': 0.43115417137785805, 'model.attn_dropout': 0.09266907550692895, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9003085277038342, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.2617957247502408, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.127301902824053, 'loss.cls.alpha': 0.19192312632486336, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5155 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.452826133110938e-05
  Dropout: 0.16879263295645489
================================================================================

[I 2025-11-08 17:38:14,056] Trial 5155 pruned. Pruned at step 11 with metric 0.6027
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5156 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 3.135001017219589e-05
  Dropout: 0.021126237605954307
================================================================================

[I 2025-11-08 17:49:36,693] Trial 5154 pruned. Pruned at step 27 with metric 0.6067

================================================================================
TRIAL 5157 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.060291720241917e-05
  Dropout: 0.044444372656056975
================================================================================

[I 2025-11-08 17:51:47,031] Trial 5157 pruned. Pruned at step 9 with metric 0.6083
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5158 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.4422681581108356e-05
  Dropout: 0.162567873874621
================================================================================

[I 2025-11-08 18:01:58,252] Trial 5158 finished with value: 0.7191311612364244 and parameters: {'seed': 52068, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 3.4422681581108356e-05, 'optim.weight_decay': 0.011867207934491347, 'optim.beta1': 0.8620193030452578, 'optim.beta2': 0.9737167864019114, 'optim.eps': 1.7592619555124075e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1985841197244268, 'sched.poly_power': 0.504192044100589, 'train.clip_grad': 0.8793241428915003, 'model.dropout': 0.162567873874621, 'model.attn_dropout': 0.1313448886690536, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8727071119233789, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.33841952378197165, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.996737277622109, 'loss.cls.alpha': 0.24126582696056612, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 5159 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.708828088146922e-05
  Dropout: 0.02106129520166411
================================================================================

[I 2025-11-08 18:09:53,237] Trial 5159 finished with value: 0.7361647361647361 and parameters: {'seed': 55740, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.708828088146922e-05, 'optim.weight_decay': 1.7400892939603958e-05, 'optim.beta1': 0.9185121357791338, 'optim.beta2': 0.9625187039582135, 'optim.eps': 4.409192839488165e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.024154692798382414, 'sched.poly_power': 0.8506538996177534, 'train.clip_grad': 0.7105900702074193, 'model.dropout': 0.02106129520166411, 'model.attn_dropout': 0.26032753219895466, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8083511604662719, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.4272269214195381, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18192799648058205, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 18:09:53,826] The parameter `tok.doc_stride` in Trial#5160 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 5160 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 3.8447049017582934e-05
  Dropout: 0.06348451831609676
================================================================================

[I 2025-11-08 18:12:38,480] Trial 5160 pruned. Pruned at step 9 with metric 0.5486

================================================================================
TRIAL 5161 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.661773854551405e-06
  Dropout: 0.14438328097276787
================================================================================

[I 2025-11-08 18:20:19,689] Trial 5161 pruned. Pruned at step 13 with metric 0.5713

================================================================================
TRIAL 5162 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.16142999674963e-05
  Dropout: 0.2291652705314154
================================================================================

[I 2025-11-08 18:26:21,956] Trial 5162 pruned. Pruned at step 9 with metric 0.5847
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5163 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.903646103866489e-05
  Dropout: 0.09376595158075407
================================================================================

[I 2025-11-08 18:32:46,981] Trial 5163 pruned. Pruned at step 29 with metric 0.6250
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5164 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.203043756951404e-05
  Dropout: 0.49821818301521736
================================================================================

[I 2025-11-08 18:50:11,999] Trial 5164 pruned. Pruned at step 13 with metric 0.6402

================================================================================
TRIAL 5165 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 6.053235095953101e-06
  Dropout: 0.35722870226570763
================================================================================

[I 2025-11-08 19:01:26,390] Trial 5165 pruned. Pruned at step 17 with metric 0.6038

================================================================================
TRIAL 5166 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 4.2463669840124574e-05
  Dropout: 0.14171099952546806
================================================================================

[I 2025-11-08 19:13:17,718] Trial 5166 finished with value: 0.4429347826086957 and parameters: {'seed': 14051, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 4.2463669840124574e-05, 'optim.weight_decay': 3.5922122170755946e-06, 'optim.beta1': 0.9418226822989088, 'optim.beta2': 0.9741641386703661, 'optim.eps': 1.5491150965665693e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.002171810403085561, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.363995238945266, 'model.dropout': 0.14171099952546806, 'model.attn_dropout': 0.2233354610492523, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8368043639869216, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.37703195238295084, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.96047955845452, 'loss.cls.alpha': 0.19097807937337608, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5167 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.0064301262024096e-05
  Dropout: 0.3451971198115601
================================================================================

[I 2025-11-08 19:30:34,518] Trial 5167 finished with value: 0.739553924336533 and parameters: {'seed': 55763, 'model.name': 'xlm-roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.0064301262024096e-05, 'optim.weight_decay': 5.161447713494173e-05, 'optim.beta1': 0.8945085286709962, 'optim.beta2': 0.9735687416184937, 'optim.eps': 7.766869361706586e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03594177364433666, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9812318718677236, 'model.dropout': 0.3451971198115601, 'model.attn_dropout': 0.17116649643114576, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8022636448927845, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.4794304991199519, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.14382988402889496, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 19:30:35,156] Trial 5168 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 5169 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 6.849941784290745e-06
  Dropout: 0.42501105975278775
================================================================================

[I 2025-11-08 19:31:02,421] Trial 5156 finished with value: 0.4489247311827957 and parameters: {'seed': 46829, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.135001017219589e-05, 'optim.weight_decay': 1.3120844033251619e-05, 'optim.beta1': 0.9354600480561783, 'optim.beta2': 0.9747341347265762, 'optim.eps': 1.5909932630159271e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.030956163606165932, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.0624093530805736, 'model.dropout': 0.021126237605954307, 'model.attn_dropout': 0.2911817864476318, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8018268105977935, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.30302591730092987, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.16658282202123614, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 19:31:03,017] The parameter `tok.doc_stride` in Trial#5170 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5170 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.934752875488389e-05
  Dropout: 0.14669694383210086
================================================================================

[I 2025-11-08 19:36:36,342] Trial 5170 pruned. Pruned at step 6 with metric 0.6027
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5171 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.009900723655422e-05
  Dropout: 0.2763134547861449
================================================================================

[I 2025-11-08 19:47:53,291] Trial 5171 pruned. Pruned at step 27 with metric 0.6338
[I 2025-11-08 19:47:53,939] Trial 5172 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5173 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 7.045365957414501e-06
  Dropout: 0.13013842464836856
================================================================================

[I 2025-11-08 20:06:48,995] Trial 5173 finished with value: 0.7002923976608186 and parameters: {'seed': 56236, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.045365957414501e-06, 'optim.weight_decay': 5.159481529139096e-05, 'optim.beta1': 0.9414894960236602, 'optim.beta2': 0.9814115081610539, 'optim.eps': 2.430576263765144e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04806005070946959, 'sched.poly_power': 0.8547676431976688, 'train.clip_grad': 1.3775053457483546, 'model.dropout': 0.13013842464836856, 'model.attn_dropout': 0.16484833143513045, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8468673339960631, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3486488198333611, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.621590225872171, 'loss.cls.alpha': 0.2904952432970904, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 20:06:49,625] Trial 5174 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 5175 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.4672516379793176e-05
  Dropout: 0.052332722050453515
================================================================================

[I 2025-11-08 20:17:26,025] Trial 5175 pruned. Pruned at step 11 with metric 0.5341
[I 2025-11-08 20:17:26,843] Trial 5176 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-08 20:17:27,456] Trial 5177 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-08 20:17:28,056] Trial 5178 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-08 20:17:28,631] The parameter `tok.doc_stride` in Trial#5179 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5179 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 5.754310005176091e-05
  Dropout: 0.4432869080575216
================================================================================

[I 2025-11-08 20:29:50,475] Trial 5179 finished with value: 0.6165174129353234 and parameters: {'seed': 58794, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 32, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.754310005176091e-05, 'optim.weight_decay': 0.12613324171529716, 'optim.beta1': 0.8633021176008846, 'optim.beta2': 0.9639946058215612, 'optim.eps': 1.640918469702316e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.028617392936549475, 'sched.poly_power': 1.1634000791770545, 'train.clip_grad': 0.9482063823591038, 'model.dropout': 0.4432869080575216, 'model.attn_dropout': 0.06187290063679911, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9528480437324286, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.16872241888243017, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.998832751045577, 'loss.cls.alpha': 0.19654226588226906, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 20:29:51,108] Trial 5180 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 20:29:51,720] Trial 5181 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 46 (patience=20)

================================================================================
TRIAL 5182 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 7.50202396669251e-05
  Dropout: 0.3547987902313739
================================================================================

[I 2025-11-08 20:35:48,705] Trial 5182 pruned. Pruned at step 11 with metric 0.6027
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5183 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 8.053249396776814e-06
  Dropout: 0.27257036134170914
================================================================================

[I 2025-11-08 20:45:55,245] Trial 5169 finished with value: 0.6963381446140067 and parameters: {'seed': 44732, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 6.849941784290745e-06, 'optim.weight_decay': 0.00017667626992092588, 'optim.beta1': 0.9203722005936951, 'optim.beta2': 0.9587800553721469, 'optim.eps': 3.6296540115132596e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.06045642418439773, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1743068816250435, 'model.dropout': 0.42501105975278775, 'model.attn_dropout': 0.08261977535450796, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9369194374646714, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.32470336837629293, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.0341101192550965, 'loss.cls.alpha': 0.4522694195231152, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5184 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.0113080189670938e-05
  Dropout: 0.21210768285427578
================================================================================

[I 2025-11-08 21:00:31,113] Trial 5184 finished with value: 0.6731665440274712 and parameters: {'seed': 55567, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.0113080189670938e-05, 'optim.weight_decay': 2.9780023140358183e-05, 'optim.beta1': 0.9269186168518552, 'optim.beta2': 0.9747161268133668, 'optim.eps': 2.6898645730443534e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.016559034690652844, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.2059830707550372, 'model.dropout': 0.21210768285427578, 'model.attn_dropout': 0.1732431805599466, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.868383728544236, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.30116003166035565, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.14699265783076199, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 21:00:31,769] Trial 5185 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 5186 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.0638099236525034e-05
  Dropout: 0.4051394202187313
================================================================================

[I 2025-11-08 21:00:40,331] Trial 5183 pruned. OOM: microsoft/deberta-v3-base bs=12 len=384
[I 2025-11-08 21:00:40,589] Trial 5186 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384
[I 2025-11-08 21:00:41,304] Trial 5188 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-08 21:00:41,580] The parameter `tok.doc_stride` in Trial#5187 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 5183 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 98.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5186 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 64 with grad_accum=8)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 58.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5187 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.6832280283626454e-05
  Dropout: 0.12038503618236736
================================================================================


================================================================================
TRIAL 5189 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.3139152240148412e-05
  Dropout: 0.3313901338184689
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 21:03:27,829] Trial 5187 pruned. Pruned at step 10 with metric 0.6285

================================================================================
TRIAL 5190 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.099261855108103e-05
  Dropout: 0.16862359296920718
================================================================================

[I 2025-11-08 21:08:19,828] Trial 5190 pruned. Pruned at step 8 with metric 0.5962
[I 2025-11-08 21:08:20,471] Trial 5191 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-08 21:08:21,118] Trial 5192 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5193 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 8.258854121854865e-06
  Dropout: 0.33549575623195266
================================================================================

[I 2025-11-08 21:17:08,170] Trial 5193 pruned. Pruned at step 19 with metric 0.6388
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5194 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.063908842970768e-05
  Dropout: 0.02504799048913174
================================================================================

[I 2025-11-08 21:18:28,982] Trial 5189 pruned. Pruned at step 27 with metric 0.5780
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5195 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.284861235203874e-06
  Dropout: 0.3766836202955463
================================================================================

[I 2025-11-08 21:24:32,407] Trial 5195 pruned. Pruned at step 10 with metric 0.6082

================================================================================
TRIAL 5196 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.0868037528982214e-05
  Dropout: 0.22200526932446912
================================================================================

[I 2025-11-08 21:27:08,907] Trial 5196 pruned. Pruned at step 10 with metric 0.6208
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5197 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.589850837787456e-06
  Dropout: 0.4109874038343402
================================================================================

[I 2025-11-08 21:42:51,334] Trial 5197 pruned. Pruned at step 8 with metric 0.6027
[I 2025-11-08 21:42:51,957] Trial 5198 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 21:42:52,560] Trial 5199 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 21:42:53,177] Trial 5200 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5201 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.488605718250984e-06
  Dropout: 0.3316976837161705
================================================================================

[I 2025-11-08 22:01:55,354] Trial 5194 pruned. Pruned at step 30 with metric 0.6383
[I 2025-11-08 22:01:56,106] Trial 5202 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5203 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.285194972995654e-06
  Dropout: 0.2818699257979139
================================================================================

[I 2025-11-08 22:04:24,875] Trial 5203 pruned. Pruned at step 13 with metric 0.5092
[I 2025-11-08 22:04:25,515] Trial 5204 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-08 22:04:26,099] The parameter `tok.doc_stride` in Trial#5205 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5205 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.033472825217194e-06
  Dropout: 0.12595148830690403
================================================================================

[I 2025-11-08 22:05:08,769] Trial 5201 finished with value: 0.634280225058323 and parameters: {'seed': 4369, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 8.488605718250984e-06, 'optim.weight_decay': 0.00015765289168569906, 'optim.beta1': 0.9227069549066984, 'optim.beta2': 0.9618275508831987, 'optim.eps': 2.5359603070178955e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.04342403052731093, 'train.clip_grad': 1.4041547606838076, 'model.dropout': 0.3316976837161705, 'model.attn_dropout': 0.1256597112535769, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9043155870414132, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.36330101910080187, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18405531926097882, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-08 22:05:09,395] The parameter `tok.doc_stride` in Trial#5206 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 5206 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.614357921972463e-05
  Dropout: 0.07137991919781572
================================================================================

[I 2025-11-08 22:09:56,400] Trial 5205 pruned. Pruned at step 14 with metric 0.6042
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5207 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.368778061817284e-06
  Dropout: 0.31422026924248414
================================================================================

[I 2025-11-08 22:20:09,250] Trial 5207 finished with value: 0.7204545454545455 and parameters: {'seed': 35931, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.368778061817284e-06, 'optim.weight_decay': 0.0007341930564846792, 'optim.beta1': 0.9419871286086503, 'optim.beta2': 0.9972637848833341, 'optim.eps': 2.721690240356589e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1740785907005927, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.7576280517125571, 'model.dropout': 0.31422026924248414, 'model.attn_dropout': 0.19108976948309564, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9213559302642088, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.052395374664233746, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.18018512260359132, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5208 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.114830359401204e-05
  Dropout: 0.09897153389697241
================================================================================

[I 2025-11-08 22:22:14,468] Trial 5208 pruned. Pruned at step 9 with metric 0.4946
[I 2025-11-08 22:22:15,105] Trial 5209 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-08 22:22:15,720] Trial 5210 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[W 2025-11-08 22:22:16,296] The parameter `tok.doc_stride` in Trial#5211 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5211 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.0853944924192919e-05
  Dropout: 0.459580081069502
================================================================================

[I 2025-11-08 22:25:30,095] Trial 5206 pruned. Pruned at step 10 with metric 0.6254
[W 2025-11-08 22:25:30,813] The parameter `tok.doc_stride` in Trial#5212 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5212 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 3.14262107945728e-05
  Dropout: 0.15539714992552772
================================================================================

[I 2025-11-08 22:28:31,051] Trial 5211 pruned. Pruned at step 7 with metric 0.6250
[I 2025-11-08 22:28:31,773] Trial 5213 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-08 22:28:32,353] The parameter `tok.doc_stride` in Trial#5214 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5214 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.06831510644364e-05
  Dropout: 0.4541316625146761
================================================================================

[I 2025-11-08 22:31:38,280] Trial 5214 pruned. Pruned at step 9 with metric 0.6517
[I 2025-11-08 22:31:38,933] Trial 5215 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5216 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 4.936806413450597e-05
  Dropout: 0.028716948485794785
================================================================================

[I 2025-11-08 22:39:07,182] Trial 5216 pruned. Pruned at step 30 with metric 0.6338
[W 2025-11-08 22:39:07,790] The parameter `tok.doc_stride` in Trial#5217 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5217 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1772264175428325e-05
  Dropout: 0.018894130853538624
================================================================================

[I 2025-11-08 22:42:00,315] Trial 5217 pruned. Pruned at step 11 with metric 0.6256
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5218 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.2920245984184975e-05
  Dropout: 0.4104665769488008
================================================================================

[I 2025-11-08 22:50:01,019] Trial 5218 pruned. Pruned at step 11 with metric 0.5807
[I 2025-11-08 22:50:01,664] Trial 5219 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-11-08 22:50:02,282] Trial 5220 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-08 22:50:02,895] Trial 5221 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-08 22:50:03,512] Trial 5222 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[I 2025-11-08 22:50:04,120] Trial 5223 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5224 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.357753120479397e-06
  Dropout: 0.06079905253200785
================================================================================

[I 2025-11-08 23:05:33,305] Trial 5212 finished with value: 0.4383561643835616 and parameters: {'seed': 35010, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.14262107945728e-05, 'optim.weight_decay': 0.0001974929292949778, 'optim.beta1': 0.8834759518038521, 'optim.beta2': 0.9897521076895393, 'optim.eps': 2.4327877643512058e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0007594931542296082, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.0089266032840039, 'model.dropout': 0.15539714992552772, 'model.attn_dropout': 0.23042856647718887, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8688157522939869, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.30116241917019887, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.04147338986301578, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-08 23:05:33,955] Trial 5225 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-08 23:05:34,530] The parameter `tok.doc_stride` in Trial#5226 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5226 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.4673411085665449e-05
  Dropout: 0.09845845523096856
================================================================================

[I 2025-11-08 23:25:52,178] Trial 5226 pruned. Pruned at step 9 with metric 0.5567
[W 2025-11-08 23:25:52,798] The parameter `tok.doc_stride` in Trial#5227 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5227 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.4984613020889245e-06
  Dropout: 0.20068794949426025
================================================================================

[I 2025-11-08 23:28:42,772] Trial 5224 finished with value: 0.6826520865584138 and parameters: {'seed': 53050, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 6.357753120479397e-06, 'optim.weight_decay': 0.0008424483228101094, 'optim.beta1': 0.8241103805026364, 'optim.beta2': 0.9787086200688809, 'optim.eps': 1.68806726106283e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.06076720717009746, 'train.clip_grad': 1.3776836135434154, 'model.dropout': 0.06079905253200785, 'model.attn_dropout': 0.055847970399390656, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8143901163514697, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.0028537554719097885, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1699763910703394, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 5228 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.4019482227978775e-05
  Dropout: 0.302374044532545
================================================================================

[I 2025-11-08 23:44:20,656] Trial 5228 finished with value: 0.7205335101875392 and parameters: {'seed': 40924, 'model.name': 'roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.4019482227978775e-05, 'optim.weight_decay': 8.928077122020701e-05, 'optim.beta1': 0.8942304702588971, 'optim.beta2': 0.9905248134339483, 'optim.eps': 1.6680757035732968e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.08158635529004728, 'train.clip_grad': 1.3339931126079083, 'model.dropout': 0.302374044532545, 'model.attn_dropout': 0.15440674653256736, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9462485229972294, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4269569726693224, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.039943539388689, 'loss.cls.alpha': 0.5474253627325502, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 5229 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.2608432879573021e-05
  Dropout: 0.4413721144669189
================================================================================

[I 2025-11-08 23:54:12,649] Trial 5229 pruned. Pruned at step 27 with metric 0.6364
[W 2025-11-08 23:54:13,256] The parameter `tok.doc_stride` in Trial#5230 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5230 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.585545250935995e-05
  Dropout: 0.23302701938086473
================================================================================

[I 2025-11-08 23:56:25,602] Trial 5230 pruned. Pruned at step 9 with metric 0.6629
[W 2025-11-08 23:56:26,207] The parameter `tok.doc_stride` in Trial#5231 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5231 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 6.508418195056436e-06
  Dropout: 0.2538843887844018
================================================================================

[I 2025-11-08 23:59:11,720] Trial 5227 pruned. Pruned at step 14 with metric 0.5477
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5232 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 4.5360348095248365e-05
  Dropout: 0.08356147553413534
================================================================================

[I 2025-11-09 00:09:52,126] Trial 5231 pruned. Pruned at step 14 with metric 0.6121

================================================================================
TRIAL 5233 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.1352197046054415e-05
  Dropout: 0.21196857725142776
================================================================================

[I 2025-11-09 00:13:26,121] Trial 5233 pruned. Pruned at step 10 with metric 0.6462
[I 2025-11-09 00:13:26,761] Trial 5234 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-09 00:13:27,378] Trial 5235 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[W 2025-11-09 00:13:27,968] The parameter `tok.doc_stride` in Trial#5236 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-09 00:13:28,025] Trial 5236 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5237 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 7.041953396643618e-06
  Dropout: 0.1853991488929511
================================================================================

[I 2025-11-09 00:18:28,113] Trial 5237 pruned. Pruned at step 11 with metric 0.5510
[I 2025-11-09 00:18:28,748] Trial 5238 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-09 00:18:29,353] Trial 5239 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5240 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.0059424409200013e-05
  Dropout: 0.3345957195906217
================================================================================

[I 2025-11-09 00:30:33,511] Trial 5240 pruned. Pruned at step 27 with metric 0.6382
[W 2025-11-09 00:30:34,112] The parameter `tok.doc_stride` in Trial#5241 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5241 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 6.1847740483950015e-06
  Dropout: 0.327420175087346
================================================================================

[I 2025-11-09 00:34:10,642] Trial 5232 pruned. Pruned at step 11 with metric 0.6356

================================================================================
TRIAL 5242 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 4.2519314159825586e-05
  Dropout: 0.15393623782943572
================================================================================

[I 2025-11-09 00:44:30,612] Trial 5242 pruned. Pruned at step 10 with metric 0.5805
[W 2025-11-09 00:44:31,405] The parameter `tok.doc_stride` in Trial#5243 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5243 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.3055271130414763e-05
  Dropout: 0.18548992758526003
================================================================================

[I 2025-11-09 01:02:51,025] Trial 5243 finished with value: 0.4562334217506631 and parameters: {'seed': 55222, 'model.name': 'xlm-roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 5.3055271130414763e-05, 'optim.weight_decay': 1.7200327210883403e-05, 'optim.beta1': 0.860147427555765, 'optim.beta2': 0.9670171554391491, 'optim.eps': 5.26873097613343e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.003794643612587776, 'train.clip_grad': 0.8189084904846831, 'model.dropout': 0.18548992758526003, 'model.attn_dropout': 0.1378901199965481, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8311448301454202, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.3942475525583435, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.040184551038374074, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 01:02:51,654] Trial 5244 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5245 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 6.022605480253348e-06
  Dropout: 0.4905852038540579
================================================================================

[I 2025-11-09 01:07:18,720] Trial 5245 pruned. Pruned at step 20 with metric 0.5845
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5246 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 9.074800821929833e-06
  Dropout: 0.11504817911172953
================================================================================

[I 2025-11-09 01:22:00,724] Trial 5246 finished with value: 0.7184065934065934 and parameters: {'seed': 10455, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 9.074800821929833e-06, 'optim.weight_decay': 1.4357838053530625e-06, 'optim.beta1': 0.942750499078361, 'optim.beta2': 0.9682326328255096, 'optim.eps': 1.2377671058012462e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.03536167794963255, 'sched.poly_power': 0.8713625529513558, 'train.clip_grad': 1.4853052291253999, 'model.dropout': 0.11504817911172953, 'model.attn_dropout': 0.14774398767069974, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8054402048300437, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.4288703128812561, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.5673210346034034, 'loss.cls.alpha': 0.4948141716891416, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 5247 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.2433522386225037e-05
  Dropout: 0.07145739912100993
================================================================================

[I 2025-11-09 01:32:23,641] Trial 5247 finished with value: 0.613358955197324 and parameters: {'seed': 35253, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.2433522386225037e-05, 'optim.weight_decay': 9.477355684953115e-06, 'optim.beta1': 0.9200974389174323, 'optim.beta2': 0.9540647440324485, 'optim.eps': 1.8398306732975056e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.02133619470819091, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.1581091337269411, 'model.dropout': 0.07145739912100993, 'model.attn_dropout': 0.2822658412287105, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8405728755044208, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3038789603773088, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16365550492788195, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 350 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 5248 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.16268398152983e-06
  Dropout: 0.04042431332631252
================================================================================

[I 2025-11-09 01:43:26,480] Trial 5241 pruned. Pruned at step 27 with metric 0.6082
[W 2025-11-09 01:43:27,389] The parameter `tok.doc_stride` in Trial#5249 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-09 01:43:27,448] Trial 5249 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-11-09 01:43:28,303] Trial 5250 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-09 01:43:29,066] The parameter `tok.doc_stride` in Trial#5251 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-09 01:43:29,124] Trial 5251 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
[GPU RESET] Performing periodic GPU reset after 350 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 350 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 350 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 350 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 5252 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.6935857613035787e-05
  Dropout: 0.33197229028199615
================================================================================

[I 2025-11-09 01:56:01,492] Trial 5248 finished with value: 0.6909991386735572 and parameters: {'seed': 50651, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 9.16268398152983e-06, 'optim.weight_decay': 5.245643464656723e-06, 'optim.beta1': 0.9297927925065717, 'optim.beta2': 0.9594811362402905, 'optim.eps': 1.2297526633275874e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.07573936045058836, 'train.clip_grad': 1.2613233815267695, 'model.dropout': 0.04042431332631252, 'model.attn_dropout': 0.23197453500284593, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8125667768420631, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.25045957587902073, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.582702294648746, 'loss.cls.alpha': 0.32817827205038663, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 01:56:02,158] Trial 5253 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-09 01:56:02,776] Trial 5254 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 52 (patience=20)

================================================================================
TRIAL 5255 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 6.800369051135349e-06
  Dropout: 0.3935866509898801
================================================================================

[I 2025-11-09 01:58:00,668] Trial 5252 pruned. Pruned at step 7 with metric 0.6155

================================================================================
TRIAL 5256 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.728157741552888e-05
  Dropout: 0.002805392783247561
================================================================================

[I 2025-11-09 02:08:55,622] Trial 5255 pruned. Pruned at step 9 with metric 0.6164
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5257 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.0785459585726866e-05
  Dropout: 0.2886211612672438
================================================================================

[I 2025-11-09 02:13:22,152] Trial 5256 finished with value: 0.7153739352320913 and parameters: {'seed': 62437, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.728157741552888e-05, 'optim.weight_decay': 0.0001857661270034251, 'optim.beta1': 0.80046040424864, 'optim.beta2': 0.9694785014317644, 'optim.eps': 4.468093940799504e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13817080536726656, 'sched.cosine_cycles': 4, 'train.clip_grad': 0.28634920117549056, 'model.dropout': 0.002805392783247561, 'model.attn_dropout': 0.1824442405174045, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.963373597905713, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.1890989356049714, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.14919298923227486, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5258 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 6.382421060430257e-06
  Dropout: 0.35845791369198954
================================================================================

[I 2025-11-09 02:29:23,841] Trial 5258 pruned. Pruned at step 9 with metric 0.6121
[I 2025-11-09 02:29:24,472] Trial 5259 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-09 02:29:25,094] Trial 5260 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5261 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.226595926369835e-05
  Dropout: 0.0717951185107989
================================================================================

[I 2025-11-09 02:33:48,716] Trial 5257 pruned. Pruned at step 9 with metric 0.5801
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5262 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.397033783590918e-05
  Dropout: 0.4036082067971158
================================================================================

[I 2025-11-09 02:42:52,341] Trial 5262 pruned. Pruned at step 9 with metric 0.5769

================================================================================
TRIAL 5263 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 7.50274412067273e-06
  Dropout: 0.08653228454571993
================================================================================

[I 2025-11-09 02:44:23,349] Trial 5261 finished with value: 0.6507956292326229 and parameters: {'seed': 45634, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 2.226595926369835e-05, 'optim.weight_decay': 4.208297271267015e-06, 'optim.beta1': 0.9207557315616677, 'optim.beta2': 0.9649861440120109, 'optim.eps': 8.54855859813952e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.026397898340501484, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.976321113591522, 'model.dropout': 0.0717951185107989, 'model.attn_dropout': 0.1476776079514761, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8804836696134194, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.31257982726111583, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.330375296310695, 'loss.cls.alpha': 0.19075706065480635, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-09 02:44:23,977] The parameter `tok.doc_stride` in Trial#5264 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5264 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.9219460659731145e-05
  Dropout: 0.00997516024782476
================================================================================

[I 2025-11-09 02:45:31,046] Trial 5263 pruned. Pruned at step 8 with metric 0.5186

================================================================================
TRIAL 5265 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.1105672741919145e-05
  Dropout: 0.3164757430802243
================================================================================

[I 2025-11-09 03:01:10,130] Trial 5265 pruned. Pruned at step 14 with metric 0.5676

================================================================================
TRIAL 5266 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.9812121913360574e-05
  Dropout: 0.1811695575396151
================================================================================

[I 2025-11-09 03:05:38,378] Trial 5264 pruned. Pruned at step 27 with metric 0.6250
[W 2025-11-09 03:05:39,270] The parameter `tok.doc_stride` in Trial#5267 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5267 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.109171895529231e-05
  Dropout: 0.2887160831875222
================================================================================

[I 2025-11-09 03:09:45,787] Trial 5267 pruned. Pruned at step 15 with metric 0.5997
[I 2025-11-09 03:09:46,431] Trial 5268 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-09 03:09:47,015] The parameter `tok.doc_stride` in Trial#5269 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-09 03:09:47,072] Trial 5269 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-09 03:09:47,677] The parameter `tok.doc_stride` in Trial#5270 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5270 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.4832291378776551e-05
  Dropout: 0.11538132014692276
================================================================================

[I 2025-11-09 03:10:33,027] Trial 5266 pruned. Pruned at step 7 with metric 0.5593
[W 2025-11-09 03:10:33,649] The parameter `tok.doc_stride` in Trial#5271 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5271 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.677952403054225e-06
  Dropout: 0.36068154773878247
================================================================================

[I 2025-11-09 03:18:27,798] Trial 5271 pruned. Pruned at step 28 with metric 0.5923
[I 2025-11-09 03:18:28,467] Trial 5272 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-09 03:18:29,087] Trial 5273 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5274 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.2382812664244284e-05
  Dropout: 0.2047985244810547
================================================================================

[I 2025-11-09 03:25:00,300] Trial 5270 pruned. Pruned at step 9 with metric 0.6388
[W 2025-11-09 03:25:01,005] The parameter `tok.doc_stride` in Trial#5275 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5275 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.6572809008814207e-05
  Dropout: 0.10205868212564546
================================================================================

[I 2025-11-09 03:33:35,050] Trial 5274 pruned. Pruned at step 27 with metric 0.6580
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5276 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.920415944301439e-06
  Dropout: 0.3439926916398637
================================================================================

[I 2025-11-09 03:39:53,008] Trial 5276 pruned. Pruned at step 9 with metric 0.6076

================================================================================
TRIAL 5277 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 7.91775311771758e-06
  Dropout: 0.48121925423566325
================================================================================

[I 2025-11-09 03:45:26,512] Trial 5277 pruned. Pruned at step 9 with metric 0.5556
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5278 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.047197476581267e-05
  Dropout: 0.13300245025017188
================================================================================

[I 2025-11-09 03:47:48,086] Trial 5278 pruned. Pruned at step 9 with metric 0.6037
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5279 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.5227093774545604e-05
  Dropout: 0.028233231558980262
================================================================================

[I 2025-11-09 03:50:10,832] Trial 5275 pruned. Pruned at step 27 with metric 0.5705
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5280 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.8632115227445393e-05
  Dropout: 0.4973026053727751
================================================================================

[I 2025-11-09 03:56:29,755] Trial 5279 pruned. Pruned at step 12 with metric 0.6519
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5281 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.3775039948528522e-05
  Dropout: 0.07021306264439428
================================================================================

[I 2025-11-09 03:56:49,596] Trial 5281 pruned. OOM: microsoft/deberta-v3-base bs=16 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5281 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 34.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5282 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.700400366474027e-06
  Dropout: 0.22319202818198
================================================================================

[I 2025-11-09 04:09:53,444] Trial 5280 finished with value: 0.6340438937309671 and parameters: {'seed': 61400, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.8632115227445393e-05, 'optim.weight_decay': 0.012857824997907458, 'optim.beta1': 0.8144179519307652, 'optim.beta2': 0.9529248479140311, 'optim.eps': 1.9532342594072783e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.06068029260216258, 'sched.poly_power': 0.9629159878154093, 'train.clip_grad': 1.3392191876206139, 'model.dropout': 0.4973026053727751, 'model.attn_dropout': 0.1802381757371586, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9788584249295871, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.07861739700881798, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7609644281295083, 'loss.cls.alpha': 0.20170467552929908, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 04:09:54,100] Trial 5283 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-09 04:09:54,717] Trial 5284 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 47 (patience=20)

================================================================================
TRIAL 5285 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.314436128037721e-05
  Dropout: 0.24395914310744088
================================================================================

[I 2025-11-09 04:10:05,594] Trial 5285 pruned. OOM: microsoft/deberta-v3-large bs=12 len=256

[OOM] Trial 5285 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 94.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5286 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.3060453644489614e-06
  Dropout: 0.41248056874409433
================================================================================

[I 2025-11-09 04:18:08,921] Trial 5286 pruned. Pruned at step 16 with metric 0.6167

================================================================================
TRIAL 5287 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 4.425740786308131e-05
  Dropout: 0.15072498610714433
================================================================================

[I 2025-11-09 04:20:40,159] Trial 5287 pruned. Pruned at step 10 with metric 0.5833
[I 2025-11-09 04:20:40,816] Trial 5288 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[W 2025-11-09 04:20:41,410] The parameter `tok.doc_stride` in Trial#5289 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5289 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 5.506980684639963e-06
  Dropout: 0.03916718281270888
================================================================================

[I 2025-11-09 04:20:48,953] Trial 5282 finished with value: 0.7105882352941176 and parameters: {'seed': 53371, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 9.700400366474027e-06, 'optim.weight_decay': 1.398146864627532e-06, 'optim.beta1': 0.8881675381868199, 'optim.beta2': 0.9731349330376152, 'optim.eps': 1.0599693184423346e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.13941635522037948, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.1871554635711146, 'model.dropout': 0.22319202818198, 'model.attn_dropout': 0.060462741916841214, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9489154015178133, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4496001382815343, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7368615219410732, 'loss.cls.alpha': 0.2937039959691211, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 04:20:49,604] Trial 5290 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 31 (patience=20)

================================================================================
TRIAL 5291 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.8150421037850324e-05
  Dropout: 0.13707792731171342
================================================================================

[I 2025-11-09 04:20:57,134] Trial 5291 pruned. OOM: microsoft/deberta-v3-large bs=12 len=288
[I 2025-11-09 04:20:57,938] Trial 5292 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)

[OOM] Trial 5291 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 124.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5293 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.0620956979118237e-05
  Dropout: 0.3268896091460877
================================================================================

[I 2025-11-09 04:29:31,457] Trial 5289 pruned. Pruned at step 9 with metric 0.6618
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5294 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.732874313242916e-05
  Dropout: 0.13713171523123896
================================================================================

[I 2025-11-09 04:31:56,019] Trial 5294 pruned. Pruned at step 10 with metric 0.4337
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5295 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.0027410396631203e-05
  Dropout: 0.278809804169076
================================================================================

[I 2025-11-09 04:38:40,953] Trial 5295 pruned. Pruned at step 11 with metric 0.6067
[I 2025-11-09 04:38:41,595] Trial 5296 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-09 04:38:42,221] Trial 5297 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-09 04:38:42,832] Trial 5298 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5299 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 8.31553723782621e-06
  Dropout: 0.4190603156017576
================================================================================

[I 2025-11-09 04:55:58,644] Trial 5299 pruned. Pruned at step 27 with metric 0.6571
[I 2025-11-09 04:55:59,483] Trial 5300 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5301 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.6993889157794607e-05
  Dropout: 0.15185015348535108
================================================================================

[I 2025-11-09 05:10:21,590] Trial 5293 pruned. Pruned at step 27 with metric 0.6537
[I 2025-11-09 05:10:22,339] Trial 5302 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5303 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 7.620037784658319e-05
  Dropout: 0.14123321502516906
================================================================================

[I 2025-11-09 05:19:49,492] Trial 5303 finished with value: 0.4368131868131868 and parameters: {'seed': 21713, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 7.620037784658319e-05, 'optim.weight_decay': 0.0005547916408046417, 'optim.beta1': 0.8647179978067728, 'optim.beta2': 0.971942437167851, 'optim.eps': 9.942638673340813e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.10057635347042955, 'train.clip_grad': 0.33089490638746333, 'model.dropout': 0.14123321502516906, 'model.attn_dropout': 0.29259640441511736, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9076631537357608, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.006755651753177529, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.053723216253855, 'loss.cls.alpha': 0.1815622446254286, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5304 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.3830025323207195e-05
  Dropout: 0.2493135059195424
================================================================================

[I 2025-11-09 05:24:58,040] Trial 5301 pruned. Pruned at step 27 with metric 0.6174
[I 2025-11-09 05:24:58,789] Trial 5305 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5306 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 5.0146392107130755e-06
  Dropout: 0.43463520674853406
================================================================================

[I 2025-11-09 05:27:10,005] Trial 5304 pruned. Pruned at step 7 with metric 0.5743
[I 2025-11-09 05:27:10,668] Trial 5307 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5308 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.1818038900511435e-05
  Dropout: 0.30371124648024594
================================================================================

[I 2025-11-09 05:42:07,399] Trial 5308 pruned. Pruned at step 18 with metric 0.6299
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5309 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 8.372573522077459e-06
  Dropout: 0.3527299638468969
================================================================================

[I 2025-11-09 05:42:13,577] Trial 5309 pruned. OOM: roberta-base bs=48 len=384
[W 2025-11-09 05:42:14,222] The parameter `tok.doc_stride` in Trial#5310 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 5309 exceeded GPU memory:
  Model: roberta-base
  Batch size: 48 (effective: 96 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 182.44 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5310 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.584333222440198e-05
  Dropout: 0.07876164369629261
================================================================================

[I 2025-11-09 05:48:23,901] Trial 5306 pruned. Pruned at step 14 with metric 0.6273
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5311 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 3.655611444812149e-05
  Dropout: 0.12020728117761276
================================================================================

[I 2025-11-09 05:52:41,497] Trial 5311 pruned. Pruned at step 9 with metric 0.6038

================================================================================
TRIAL 5312 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.601117784642879e-06
  Dropout: 0.2635485567266944
================================================================================

[I 2025-11-09 05:59:08,257] Trial 5312 pruned. Pruned at step 16 with metric 0.5949
[W 2025-11-09 05:59:08,871] The parameter `tok.doc_stride` in Trial#5313 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5313 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.2184076898115355e-05
  Dropout: 0.3914344207446014
================================================================================

[I 2025-11-09 06:04:50,037] Trial 5310 pruned. Pruned at step 27 with metric 0.6252

================================================================================
TRIAL 5314 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.648501292134237e-05
  Dropout: 0.1187669733433359
================================================================================

[I 2025-11-09 06:06:49,395] Trial 5313 pruned. Pruned at step 8 with metric 0.5272
[I 2025-11-09 06:06:50,061] Trial 5315 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-09 06:06:50,643] The parameter `tok.doc_stride` in Trial#5316 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5316 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.534533281186683e-05
  Dropout: 0.04866012140686313
================================================================================

[I 2025-11-09 06:09:18,636] Trial 5316 pruned. Pruned at step 9 with metric 0.5658
[I 2025-11-09 06:09:19,290] Trial 5317 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-09 06:09:19,908] Trial 5318 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5319 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.5384301583370555e-05
  Dropout: 0.43892679972459625
================================================================================

[I 2025-11-09 06:22:34,986] Trial 5314 pruned. Pruned at step 27 with metric 0.6210
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5320 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.198681801589602e-06
  Dropout: 0.45875513872821966
================================================================================

[I 2025-11-09 06:29:31,322] Trial 5319 pruned. Pruned at step 14 with metric 0.5807

================================================================================
TRIAL 5321 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 1.145947079385072e-05
  Dropout: 0.43753838067308626
================================================================================

[I 2025-11-09 06:41:27,212] Trial 5321 pruned. Pruned at step 12 with metric 0.5780

================================================================================
TRIAL 5322 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.4339236659233647e-05
  Dropout: 0.40108561564329803
================================================================================

[I 2025-11-09 06:51:17,403] Trial 5322 finished with value: 0.6725694444444444 and parameters: {'seed': 39621, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.4339236659233647e-05, 'optim.weight_decay': 0.13465392526193526, 'optim.beta1': 0.8404413331777633, 'optim.beta2': 0.9912146873489199, 'optim.eps': 4.466092738163255e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1474647555808886, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.22054960170306365, 'model.dropout': 0.40108561564329803, 'model.attn_dropout': 0.1867142645761637, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8517470497252176, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.025501444827435646, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.605321662453285, 'loss.cls.alpha': 0.5441651419840389, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 06:51:18,052] Trial 5323 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 5324 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.04976943725078e-05
  Dropout: 0.3853941154314632
================================================================================

[I 2025-11-09 06:51:24,705] Trial 5324 pruned. OOM: microsoft/deberta-v3-large bs=12 len=352
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

[OOM] Trial 5324 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 132.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 56.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5325 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 8.582256895234743e-06
  Dropout: 0.4889709148903013
================================================================================

[I 2025-11-09 07:00:41,467] Trial 5320 finished with value: 0.6950571635003613 and parameters: {'seed': 44395, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 6.198681801589602e-06, 'optim.weight_decay': 0.0002523857297865168, 'optim.beta1': 0.9119655212960368, 'optim.beta2': 0.965181739026502, 'optim.eps': 9.687475811835789e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.07589115833874088, 'sched.poly_power': 0.7247579329469278, 'train.clip_grad': 1.3755207625177948, 'model.dropout': 0.45875513872821966, 'model.attn_dropout': 0.09262528942191589, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9215160382895956, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3319621036858691, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.6687365411620405, 'loss.cls.alpha': 0.18577866747070795, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 56 (patience=20)

================================================================================
TRIAL 5326 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.0418993144562992e-05
  Dropout: 0.48360996934930905
================================================================================

[I 2025-11-09 07:06:31,752] Trial 5325 pruned. Pruned at step 9 with metric 0.6328
[W 2025-11-09 07:06:32,462] The parameter `tok.doc_stride` in Trial#5327 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5327 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.145885520764172e-05
  Dropout: 0.1718787148805845
================================================================================

[I 2025-11-09 07:08:52,940] Trial 5327 pruned. Pruned at step 9 with metric 0.5676
[W 2025-11-09 07:08:53,546] The parameter `tok.doc_stride` in Trial#5328 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5328 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2612884487402565e-05
  Dropout: 0.14941033097488005
================================================================================

[I 2025-11-09 07:13:44,800] Trial 5326 pruned. Pruned at step 15 with metric 0.6082
[I 2025-11-09 07:13:45,470] Trial 5329 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5330 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.1420203069495235e-05
  Dropout: 0.26206044803499284
================================================================================

[I 2025-11-09 07:17:03,838] Trial 5328 pruned. Pruned at step 9 with metric 0.6038

================================================================================
TRIAL 5331 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 6.217899367407498e-06
  Dropout: 0.24899584837484712
================================================================================

[I 2025-11-09 07:22:39,712] Trial 5331 pruned. Pruned at step 25 with metric 0.5510
[W 2025-11-09 07:22:40,324] The parameter `tok.doc_stride` in Trial#5332 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5332 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 3.2101385770863716e-05
  Dropout: 0.24194229775716375
================================================================================

[I 2025-11-09 07:36:25,588] Trial 5330 pruned. Pruned at step 27 with metric 0.6002

================================================================================
TRIAL 5333 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.543188689742627e-06
  Dropout: 0.038887960306593444
================================================================================

[I 2025-11-09 07:49:25,674] Trial 5332 finished with value: 0.4489247311827957 and parameters: {'seed': 60608, 'model.name': 'bert-large-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.2101385770863716e-05, 'optim.weight_decay': 9.366228306120464e-06, 'optim.beta1': 0.8867394949200861, 'optim.beta2': 0.9640639956457567, 'optim.eps': 1.1013604426148142e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0645446197896068, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8777020599064804, 'model.dropout': 0.24194229775716375, 'model.attn_dropout': 0.14431781593357784, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8464590464531999, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.4450800163487961, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1997024120882126, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5334 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.120696746915362e-05
  Dropout: 0.11245681395346796
================================================================================

[I 2025-11-09 07:59:54,955] Trial 5334 pruned. Pruned at step 28 with metric 0.6421
[I 2025-11-09 07:59:55,610] Trial 5335 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5336 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.3933738114691496e-05
  Dropout: 0.4520912391645957
================================================================================

[I 2025-11-09 08:04:02,807] Trial 5336 pruned. Pruned at step 9 with metric 0.5615
[I 2025-11-09 08:04:04,344] Trial 5333 finished with value: 0.6809049773755655 and parameters: {'seed': 29892, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 6.543188689742627e-06, 'optim.weight_decay': 0.00026725658617709927, 'optim.beta1': 0.9495737638059899, 'optim.beta2': 0.9638827916844299, 'optim.eps': 6.322719002969764e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.013704006837847976, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.2021191359385648, 'model.dropout': 0.038887960306593444, 'model.attn_dropout': 0.21635553469353627, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8854911607721179, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.28930059752580495, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.881908728718172, 'loss.cls.alpha': 0.3998750254101748, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5337 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 8.334461836139392e-06
  Dropout: 0.4596368979100228
================================================================================


================================================================================
TRIAL 5338 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 2.0525402300068707e-05
  Dropout: 0.18848686558821748
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-09 08:11:03,851] Trial 5338 pruned. Pruned at step 11 with metric 0.5986

================================================================================
TRIAL 5339 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 5.515823180642765e-05
  Dropout: 0.44265784094064137
================================================================================

[I 2025-11-09 08:27:33,330] Trial 5337 pruned. Pruned at step 14 with metric 0.5474
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5340 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.40181217760686e-06
  Dropout: 0.3142508078690289
================================================================================

[I 2025-11-09 08:32:11,526] Trial 5339 finished with value: 0.4562334217506631 and parameters: {'seed': 38644, 'model.name': 'xlm-roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 5.515823180642765e-05, 'optim.weight_decay': 0.0006570022433022981, 'optim.beta1': 0.8918739673666541, 'optim.beta2': 0.9651633259562168, 'optim.eps': 4.3801432418014864e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.07182593629779674, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.9659226408948751, 'model.dropout': 0.44265784094064137, 'model.attn_dropout': 0.22956967735510747, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.976944687938731, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.3432747234871598, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.967724109094214, 'loss.cls.alpha': 0.32801787195303217, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 08:32:12,174] Trial 5341 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5342 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.425185655393938e-06
  Dropout: 0.06870609460262114
================================================================================

[I 2025-11-09 08:38:18,639] Trial 5340 finished with value: 0.7726490496768519 and parameters: {'seed': 61575, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.40181217760686e-06, 'optim.weight_decay': 0.00028610925500637576, 'optim.beta1': 0.8179295193198187, 'optim.beta2': 0.9655926466715897, 'optim.eps': 6.557118303197886e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.12868801957308962, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.5176789901969042, 'model.dropout': 0.3142508078690289, 'model.attn_dropout': 0.017660423530038258, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9280745367250587, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.2705896387764978, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.199282615012725, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5343 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.168528267337352e-06
  Dropout: 0.19915441964674077
================================================================================

[I 2025-11-09 08:41:54,380] Trial 5343 pruned. Pruned at step 13 with metric 0.5750
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5344 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.079691928804472e-05
  Dropout: 0.25554361562165845
================================================================================

[I 2025-11-09 08:50:10,490] Trial 5342 finished with value: 0.7104619518412623 and parameters: {'seed': 30468, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 6.425185655393938e-06, 'optim.weight_decay': 7.451421687880598e-05, 'optim.beta1': 0.8793176125276251, 'optim.beta2': 0.9797156418362233, 'optim.eps': 2.1112867745055745e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03353634070531893, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4583017516943322, 'model.dropout': 0.06870609460262114, 'model.attn_dropout': 0.1930513272153052, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8536486090712296, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.451849725984608, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.17174609131719784, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-09 08:50:11,110] The parameter `tok.doc_stride` in Trial#5345 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 46 (patience=20)

================================================================================
TRIAL 5345 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.349381163671862e-05
  Dropout: 0.37394354758430937
================================================================================

[I 2025-11-09 08:55:41,654] Trial 5345 pruned. Pruned at step 10 with metric 0.5065
[I 2025-11-09 08:55:42,360] Trial 5346 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-09 08:55:42,950] The parameter `tok.doc_stride` in Trial#5347 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5347 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.533411105505012e-06
  Dropout: 0.22583655059996893
================================================================================

[I 2025-11-09 09:09:21,275] Trial 5347 pruned. Pruned at step 11 with metric 0.6112
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5348 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.258522879020448e-06
  Dropout: 0.42004654026372124
================================================================================

[I 2025-11-09 09:12:07,154] Trial 5344 finished with value: 0.43526170798898073 and parameters: {'seed': 26948, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 2.079691928804472e-05, 'optim.weight_decay': 0.0030929884937654466, 'optim.beta1': 0.8141334121401573, 'optim.beta2': 0.9693779129153748, 'optim.eps': 2.2439678918418446e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.11062691764559611, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.9502862035687121, 'model.dropout': 0.25554361562165845, 'model.attn_dropout': 0.03535414219323323, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9682019084650445, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.323171032612873, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19810648394852834, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5349 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 4.602463887997225e-05
  Dropout: 0.2714251583390581
================================================================================

[I 2025-11-09 09:15:29,228] Trial 5348 pruned. Pruned at step 15 with metric 0.5713
[I 2025-11-09 09:15:29,887] Trial 5350 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5351 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.9940022420698905e-05
  Dropout: 0.10996459894701491
================================================================================

[I 2025-11-09 09:26:07,802] Trial 5349 finished with value: 0.4429347826086957 and parameters: {'seed': 57985, 'model.name': 'xlm-roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 4.602463887997225e-05, 'optim.weight_decay': 0.0007743122962653459, 'optim.beta1': 0.9075846992763011, 'optim.beta2': 0.9728413195005516, 'optim.eps': 1.3473706746387013e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0030624774146161236, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.44590359209830666, 'model.dropout': 0.2714251583390581, 'model.attn_dropout': 0.23568958763395104, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8277553706802072, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.4010799092703804, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1875495658077231, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-09 09:26:08,423] The parameter `tok.doc_stride` in Trial#5352 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5352 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.669688774067173e-06
  Dropout: 0.2948724478414853
================================================================================

[I 2025-11-09 09:29:00,654] Trial 5352 pruned. Pruned at step 10 with metric 0.5693

================================================================================
TRIAL 5353 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.9021712991621583e-05
  Dropout: 0.2956418210127592
================================================================================

[I 2025-11-09 09:34:04,368] Trial 5353 pruned. Pruned at step 16 with metric 0.6067
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5354 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.494475708144684e-06
  Dropout: 0.11239596063216437
================================================================================

[I 2025-11-09 09:42:38,992] Trial 5354 finished with value: 0.6472140426020115 and parameters: {'seed': 65533, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.494475708144684e-06, 'optim.weight_decay': 0.01597777692051087, 'optim.beta1': 0.8802950243459748, 'optim.beta2': 0.9627736142796964, 'optim.eps': 1.526536301898688e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.12028035272534977, 'train.clip_grad': 1.0046672071343346, 'model.dropout': 0.11239596063216437, 'model.attn_dropout': 0.062206180963011795, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8771162354216331, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.12364913387552236, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.620136917704834, 'loss.cls.alpha': 0.7313004408445727, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 5355 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 8.786931740668029e-06
  Dropout: 0.23778082783334795
================================================================================

[I 2025-11-09 09:52:33,589] Trial 5355 finished with value: 0.6897699757869249 and parameters: {'seed': 62059, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 8.786931740668029e-06, 'optim.weight_decay': 0.00026330971272446114, 'optim.beta1': 0.8136876671073259, 'optim.beta2': 0.9648833082508376, 'optim.eps': 6.015426873641775e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.16034305771764049, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.7347240356576644, 'model.dropout': 0.23778082783334795, 'model.attn_dropout': 0.047212198013514936, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9291677861115519, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3052162018609377, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19902776360092558, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-09 09:52:34,199] The parameter `tok.doc_stride` in Trial#5356 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 5356 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.2147792831275834e-05
  Dropout: 0.18954042326899506
================================================================================

[I 2025-11-09 10:01:09,406] Trial 5356 pruned. Pruned at step 28 with metric 0.5545

================================================================================
TRIAL 5357 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 5.62589008130264e-05
  Dropout: 0.17398895369599376
================================================================================

[I 2025-11-09 10:06:55,561] Trial 5357 pruned. Pruned at step 11 with metric 0.5986
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5358 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.3269511482789165e-05
  Dropout: 0.42164466613595447
================================================================================

[I 2025-11-09 10:11:45,995] Trial 5351 pruned. Pruned at step 24 with metric 0.6294
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5359 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.048530984529438e-05
  Dropout: 0.41301778332256806
================================================================================

[I 2025-11-09 10:15:08,621] Trial 5358 finished with value: 0.6870865981479597 and parameters: {'seed': 55499, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.3269511482789165e-05, 'optim.weight_decay': 0.0013808068291933707, 'optim.beta1': 0.8023071014276377, 'optim.beta2': 0.9674724700083128, 'optim.eps': 2.561303379117775e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.0968093141819319, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.3773284847388143, 'model.dropout': 0.42164466613595447, 'model.attn_dropout': 0.038784389108624515, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9013887765815426, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.30975953740968915, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18619732548106918, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5360 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 2.7251624120002608e-05
  Dropout: 0.11570862875970192
================================================================================

[I 2025-11-09 10:15:16,185] Trial 5359 pruned. OOM: roberta-base bs=48 len=224
[I 2025-11-09 10:15:17,288] Trial 5360 pruned. OOM: microsoft/deberta-v3-large bs=16 len=256
[I 2025-11-09 10:15:17,547] Trial 5361 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5359 exceeded GPU memory:
  Model: roberta-base
  Batch size: 48 (effective: 96 with grad_accum=2)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 62.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5360 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 62.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5363 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 8.5514461813612e-06
  Dropout: 0.2797458567439186
================================================================================


================================================================================
TRIAL 5362 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 8.638706447202616e-06
  Dropout: 0.267016700019585
================================================================================

[I 2025-11-09 10:20:36,755] Trial 5363 pruned. Pruned at step 9 with metric 0.5770
[I 2025-11-09 10:20:37,426] Trial 5364 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5365 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.719488992130469e-05
  Dropout: 0.4800068508145995
================================================================================

[I 2025-11-09 10:24:52,806] Trial 5365 pruned. Pruned at step 13 with metric 0.6107

================================================================================
TRIAL 5366 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.9022842144884455e-05
  Dropout: 0.06764173079798948
================================================================================

[I 2025-11-09 10:28:55,207] Trial 5362 pruned. Pruned at step 14 with metric 0.6053
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5367 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.47362540466974e-06
  Dropout: 0.37874109073812123
================================================================================

[I 2025-11-09 10:37:00,471] Trial 5367 finished with value: 0.6951219512195121 and parameters: {'seed': 59242, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.47362540466974e-06, 'optim.weight_decay': 8.995855739452024e-06, 'optim.beta1': 0.8696250295867897, 'optim.beta2': 0.9835525378177197, 'optim.eps': 1.1469951403328223e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.07787987914862496, 'train.clip_grad': 0.8788273891324007, 'model.dropout': 0.37874109073812123, 'model.attn_dropout': 0.21785627042369832, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9440970891436493, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.4274571790120541, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8071168481524316, 'loss.cls.alpha': 0.8283939783011944, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-09 10:37:01,099] The parameter `tok.doc_stride` in Trial#5368 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 5368 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.653499414209914e-06
  Dropout: 0.29296059698399357
================================================================================

[I 2025-11-09 10:43:43,987] Trial 5366 finished with value: 0.44141689373297005 and parameters: {'seed': 21626, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.9022842144884455e-05, 'optim.weight_decay': 6.144835535244496e-05, 'optim.beta1': 0.875774836749266, 'optim.beta2': 0.9702830707156824, 'optim.eps': 1.757268863681466e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1330387137663715, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.6065362731934549, 'model.dropout': 0.06764173079798948, 'model.attn_dropout': 0.1855872614495464, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9855752548931341, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.20731737852112464, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19954694222847302, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5369 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6690127642608903e-05
  Dropout: 0.17742832631561462
================================================================================

[I 2025-11-09 11:00:02,760] Trial 5369 pruned. Pruned at step 28 with metric 0.5725
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5370 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 9.132478669487102e-06
  Dropout: 0.4023062273043422
================================================================================

[I 2025-11-09 11:02:31,820] Trial 5368 finished with value: 0.6868055555555556 and parameters: {'seed': 54261, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.653499414209914e-06, 'optim.weight_decay': 4.537631551301349e-05, 'optim.beta1': 0.8463974257417031, 'optim.beta2': 0.9563918743141361, 'optim.eps': 9.01509133648305e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1415174327466673, 'sched.poly_power': 1.126420666443019, 'train.clip_grad': 0.6644148343529096, 'model.dropout': 0.29296059698399357, 'model.attn_dropout': 0.0011864935305442303, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9631373076317098, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.15913652199066122, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18413639631844, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 39 (patience=20)

================================================================================
TRIAL 5371 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 8.322074057728895e-06
  Dropout: 0.3250356864488503
================================================================================

[I 2025-11-09 11:07:55,335] Trial 5371 pruned. Pruned at step 17 with metric 0.6042
[I 2025-11-09 11:07:56,025] Trial 5372 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5373 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.053988906459199e-05
  Dropout: 0.2958232512925668
================================================================================

[I 2025-11-09 11:10:34,625] Trial 5373 pruned. Pruned at step 12 with metric 0.5584
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5374 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 8.913978179580464e-06
  Dropout: 0.2882422028956852
================================================================================

[I 2025-11-09 11:25:58,657] Trial 5374 pruned. Pruned at step 27 with metric 0.6450
[I 2025-11-09 11:25:59,296] Trial 5375 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5376 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 9.826262072952643e-06
  Dropout: 0.38676588553876534
================================================================================

[I 2025-11-09 11:35:43,865] Trial 5370 finished with value: 0.6468561584840655 and parameters: {'seed': 49177, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 320, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.132478669487102e-06, 'optim.weight_decay': 2.9390021645302536e-06, 'optim.beta1': 0.8860613668863588, 'optim.beta2': 0.9637303379201376, 'optim.eps': 1.5125338743029888e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.10891512361216238, 'train.clip_grad': 1.330655598822224, 'model.dropout': 0.4023062273043422, 'model.attn_dropout': 0.12863441409359538, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9096800891092383, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.38863838085579877, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1836256648600395, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 11:35:44,534] Trial 5377 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-09 11:35:45,157] Trial 5378 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 5379 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 3.2557070073614e-05
  Dropout: 0.4820562651333099
================================================================================

[I 2025-11-09 11:39:04,431] Trial 5379 pruned. Pruned at step 10 with metric 0.5652
[I 2025-11-09 11:39:05,092] Trial 5380 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5381 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 8.157469977016956e-06
  Dropout: 0.32347841390018495
================================================================================

[I 2025-11-09 11:51:44,483] Trial 5376 pruned. Pruned at step 18 with metric 0.5923
[I 2025-11-09 11:51:45,139] Trial 5382 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5383 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.0451553814393075e-05
  Dropout: 0.32939130596127014
================================================================================

[I 2025-11-09 11:56:30,187] Trial 5381 pruned. Pruned at step 27 with metric 0.6421
[I 2025-11-09 11:56:30,857] Trial 5384 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5385 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.1376003846736017e-05
  Dropout: 0.33992122196823593
================================================================================

[I 2025-11-09 12:04:58,890] Trial 5385 pruned. Pruned at step 27 with metric 0.6608
[W 2025-11-09 12:04:59,508] The parameter `tok.doc_stride` in Trial#5386 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5386 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.3062317849849848e-05
  Dropout: 0.15448893695161198
================================================================================

[I 2025-11-09 12:07:45,426] Trial 5386 pruned. Pruned at step 9 with metric 0.5905
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5387 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 7.779067119371975e-06
  Dropout: 0.3490321557948025
================================================================================

[I 2025-11-09 12:11:29,139] Trial 5383 pruned. Pruned at step 17 with metric 0.6484
[I 2025-11-09 12:11:29,809] Trial 5388 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5389 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.599376228563548e-06
  Dropout: 0.3566378210527648
================================================================================

[I 2025-11-09 12:19:17,090] Trial 5387 pruned. Pruned at step 12 with metric 0.6009
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5390 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.255798695652888e-05
  Dropout: 0.21733907803788174
================================================================================

[I 2025-11-09 12:24:39,854] Trial 5390 pruned. Pruned at step 10 with metric 0.6207

================================================================================
TRIAL 5391 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.181323858380716e-06
  Dropout: 0.47968961389224846
================================================================================

[I 2025-11-09 12:29:42,120] Trial 5389 finished with value: 0.6630506245890861 and parameters: {'seed': 39007, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 7.599376228563548e-06, 'optim.weight_decay': 0.010284862321319303, 'optim.beta1': 0.8919109105508516, 'optim.beta2': 0.9785457043508319, 'optim.eps': 1.0968405389920205e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.07279856428919984, 'train.clip_grad': 0.6146270386307374, 'model.dropout': 0.3566378210527648, 'model.attn_dropout': 0.09537428819891011, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8712075473108095, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.29266027467338984, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.19996876870535396, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 5392 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 6.879238715892897e-06
  Dropout: 0.37155442609906114
================================================================================

[I 2025-11-09 12:43:39,748] Trial 5391 finished with value: 0.5962121212121212 and parameters: {'seed': 27792, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.181323858380716e-06, 'optim.weight_decay': 4.552021670244633e-06, 'optim.beta1': 0.90741411086763, 'optim.beta2': 0.9842669725112002, 'optim.eps': 8.102571936443707e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.012145483454208992, 'sched.poly_power': 1.4577337109652742, 'train.clip_grad': 1.4658479872265735, 'model.dropout': 0.47968961389224846, 'model.attn_dropout': 0.21047039064398138, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9867659682466682, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.2975290990504836, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.987426875770936, 'loss.cls.alpha': 0.7311567510979591, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 12:43:40,452] Trial 5393 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 5394 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.669700045900691e-06
  Dropout: 0.2306416849133933
================================================================================

[I 2025-11-09 13:05:33,105] Trial 5394 pruned. Pruned at step 9 with metric 0.6382
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5395 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 7.919077465338845e-06
  Dropout: 0.3765432911091371
================================================================================

[I 2025-11-09 13:11:28,965] Trial 5395 pruned. Pruned at step 8 with metric 0.5338
[I 2025-11-09 13:11:29,614] Trial 5396 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-09 13:11:30,236] Trial 5397 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5398 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.680296498485706e-05
  Dropout: 0.13489614112220982
================================================================================

[I 2025-11-09 13:23:03,484] Trial 5398 pruned. Pruned at step 27 with metric 0.4306
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5399 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.574972643785109e-05
  Dropout: 0.26255960850415466
================================================================================

[I 2025-11-09 13:30:00,524] Trial 5399 finished with value: 0.71546220042565 and parameters: {'seed': 47769, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.574972643785109e-05, 'optim.weight_decay': 6.862019211920076e-05, 'optim.beta1': 0.8764589025636362, 'optim.beta2': 0.9575721717002942, 'optim.eps': 1.62881776382412e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03686698560789734, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.157165334545755, 'model.dropout': 0.26255960850415466, 'model.attn_dropout': 0.21636828196595534, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8715066882685973, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.4117065265906644, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18046898625318178, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5400 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.9533179432740232e-05
  Dropout: 0.23429151125020287
================================================================================

[I 2025-11-09 13:39:17,870] Trial 5400 pruned. Pruned at step 13 with metric 0.5919
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5401 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 5.404250304310975e-05
  Dropout: 0.37988861600183976
================================================================================

[I 2025-11-09 13:43:32,171] Trial 5401 pruned. Pruned at step 9 with metric 0.6299
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5402 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.635258012926499e-05
  Dropout: 0.07614785244219935
================================================================================

[I 2025-11-09 14:14:27,644] Trial 5402 finished with value: 0.6654726368159204 and parameters: {'seed': 31401, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.635258012926499e-05, 'optim.weight_decay': 0.0020986085484268053, 'optim.beta1': 0.8821391081768241, 'optim.beta2': 0.9681292032850027, 'optim.eps': 9.781975629060332e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1463605406349367, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.9805447939148197, 'model.dropout': 0.07614785244219935, 'model.attn_dropout': 0.14175805395504193, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8624046042015043, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.18077703085340607, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.552414132104437, 'loss.cls.alpha': 0.6057256161063901, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5403 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 8.987106873772197e-06
  Dropout: 0.34203854032810327
================================================================================

[I 2025-11-09 14:27:00,210] Trial 5403 finished with value: 0.6752146199476806 and parameters: {'seed': 50504, 'model.name': 'roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.987106873772197e-06, 'optim.weight_decay': 5.930106296532314e-05, 'optim.beta1': 0.813819489551855, 'optim.beta2': 0.9545377243405818, 'optim.eps': 5.086311247610779e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.10368584102589902, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.5277921287914749, 'model.dropout': 0.34203854032810327, 'model.attn_dropout': 0.00799865017378182, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9318057402771383, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.3525247925096518, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18131900106562607, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-09 14:27:00,824] The parameter `tok.doc_stride` in Trial#5404 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5404 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.688370217516146e-05
  Dropout: 0.45714140958501814
================================================================================

[I 2025-11-09 14:30:11,010] Trial 5404 pruned. Pruned at step 8 with metric 0.5427
[W 2025-11-09 14:30:11,633] The parameter `tok.doc_stride` in Trial#5405 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5405 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.352325660811502e-05
  Dropout: 0.3133914576164807
================================================================================

[I 2025-11-09 14:34:28,694] Trial 5405 pruned. Pruned at step 10 with metric 0.5371

================================================================================
TRIAL 5406 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.040090453595167e-05
  Dropout: 0.022157448012496767
================================================================================

[I 2025-11-09 14:45:08,483] Trial 5406 pruned. Pruned at step 9 with metric 0.6205
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5407 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 4.6622229485585715e-05
  Dropout: 0.4166187776680604
================================================================================

[I 2025-11-09 14:47:19,799] Trial 5407 pruned. Pruned at step 7 with metric 0.6208
[W 2025-11-09 14:47:20,452] The parameter `tok.doc_stride` in Trial#5408 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5408 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.141515475987072e-06
  Dropout: 0.35801107900270945
================================================================================

[I 2025-11-09 14:57:30,942] Trial 5408 pruned. Pruned at step 11 with metric 0.5629
[I 2025-11-09 14:57:31,679] Trial 5409 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[I 2025-11-09 14:57:32,310] Trial 5410 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5411 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.9444628874166042e-05
  Dropout: 0.4932697003611457
================================================================================

[I 2025-11-09 14:59:55,764] Trial 5411 pruned. Pruned at step 9 with metric 0.5629

================================================================================
TRIAL 5412 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.554975193040654e-06
  Dropout: 0.3766021614080435
================================================================================

[I 2025-11-09 15:06:09,907] Trial 5412 pruned. Pruned at step 14 with metric 0.5485
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5413 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.0747172080147744e-05
  Dropout: 0.3402444087957064
================================================================================

[I 2025-11-09 15:09:55,004] Trial 5413 pruned. Pruned at step 13 with metric 0.6299

================================================================================
TRIAL 5414 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 8.545545378875333e-06
  Dropout: 0.47818499154255456
================================================================================

[I 2025-11-09 15:18:16,304] Trial 5414 finished with value: 0.7201512128530925 and parameters: {'seed': 37913, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 8.545545378875333e-06, 'optim.weight_decay': 0.006518781164843572, 'optim.beta1': 0.897522908756813, 'optim.beta2': 0.971103541428203, 'optim.eps': 5.7921920119823695e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.04835351721427557, 'train.clip_grad': 1.4470231843341204, 'model.dropout': 0.47818499154255456, 'model.attn_dropout': 0.23820743782629478, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8862381694421948, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.39429574503502146, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.18216093697492441, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 15:18:16,957] Trial 5415 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5416 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.5577251976950944e-05
  Dropout: 0.17023064802855525
================================================================================

[I 2025-11-09 15:27:14,201] Trial 5392 finished with value: 0.6737331296154825 and parameters: {'seed': 60258, 'model.name': 'roberta-large', 'tok.max_length': 352, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.879238715892897e-06, 'optim.weight_decay': 4.922725899863078e-05, 'optim.beta1': 0.9098034440255931, 'optim.beta2': 0.9689305853685865, 'optim.eps': 1.20619081294888e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.08558440029703261, 'sched.poly_power': 0.7356677361050826, 'train.clip_grad': 1.3775765353832472, 'model.dropout': 0.37155442609906114, 'model.attn_dropout': 0.2663143356672286, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.907856183710613, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.43974647643409326, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.399534176258819, 'loss.cls.alpha': 0.8338876497216173, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 15:27:14,867] Trial 5417 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 47 (patience=20)

================================================================================
TRIAL 5418 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 9.942843144709713e-06
  Dropout: 0.4850242423484619
================================================================================

[I 2025-11-09 15:33:47,595] Trial 5418 pruned. Pruned at step 14 with metric 0.5827
[I 2025-11-09 15:33:48,253] Trial 5419 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-09 15:33:48,866] Trial 5420 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5421 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 5.4141359764502886e-05
  Dropout: 0.014815674777434975
================================================================================

[I 2025-11-09 15:53:41,091] Trial 5421 pruned. Pruned at step 10 with metric 0.6027
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5422 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.797860985109649e-06
  Dropout: 0.4980581082333323
================================================================================

[I 2025-11-09 16:00:16,917] Trial 5422 pruned. Pruned at step 9 with metric 0.5958
[W 2025-11-09 16:00:17,531] The parameter `tok.doc_stride` in Trial#5423 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5423 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.460215090313043e-06
  Dropout: 0.4107382663080609
================================================================================

[I 2025-11-09 16:01:51,885] Trial 5416 pruned. Pruned at step 27 with metric 0.5968

================================================================================
TRIAL 5424 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.771120913210753e-06
  Dropout: 0.4919814206025614
================================================================================

[I 2025-11-09 16:06:10,787] Trial 5424 pruned. Pruned at step 23 with metric 0.6121
[I 2025-11-09 16:06:11,426] Trial 5425 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5426 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.0827339707764538e-05
  Dropout: 0.19541847752639946
================================================================================

[I 2025-11-09 16:13:01,893] Trial 5423 pruned. Pruned at step 18 with metric 0.6210
[W 2025-11-09 16:13:02,531] The parameter `tok.doc_stride` in Trial#5427 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5427 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.743514423714201e-06
  Dropout: 0.27127929160930087
================================================================================

[I 2025-11-09 16:25:15,638] Trial 5426 pruned. Pruned at step 9 with metric 0.5845

================================================================================
TRIAL 5428 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.4112655467456142e-05
  Dropout: 0.08402081115636911
================================================================================

[I 2025-11-09 16:39:07,352] Trial 5427 pruned. Pruned at step 44 with metric 0.6321
[I 2025-11-09 16:39:08,051] Trial 5429 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-09 16:39:08,677] Trial 5430 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5431 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.129507126510951e-05
  Dropout: 0.47533850672867656
================================================================================

[I 2025-11-09 16:41:49,364] Trial 5431 pruned. Pruned at step 10 with metric 0.6112
[I 2025-11-09 16:41:50,025] Trial 5432 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5433 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 5.298615864895403e-06
  Dropout: 0.3034906418486981
================================================================================

[I 2025-11-09 16:53:35,488] Trial 5433 pruned. Pruned at step 14 with metric 0.5941
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5434 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.734952898408197e-05
  Dropout: 0.4869083527934131
================================================================================

[I 2025-11-09 16:55:38,406] Trial 5428 pruned. Pruned at step 33 with metric 0.6208
[I 2025-11-09 16:55:39,324] Trial 5435 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5436 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.72553440883658e-06
  Dropout: 0.3046393019133352
================================================================================

[I 2025-11-09 16:58:05,441] Trial 5436 pruned. Pruned at step 10 with metric 0.6517
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5437 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 7.837790481410353e-06
  Dropout: 0.37749214136576725
================================================================================

[I 2025-11-09 17:07:59,868] Trial 5437 finished with value: 0.7014890318944056 and parameters: {'seed': 51946, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 7.837790481410353e-06, 'optim.weight_decay': 0.00013717264105862187, 'optim.beta1': 0.8176180491790147, 'optim.beta2': 0.970936129048232, 'optim.eps': 6.476177675081358e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.13046287902222584, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9056076046449706, 'model.dropout': 0.37749214136576725, 'model.attn_dropout': 0.055688249658270655, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9028527722567649, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.37546582321226507, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.17626140875349475, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 17:08:00,516] Trial 5438 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 5439 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.2492335877739265e-05
  Dropout: 0.44573987348624855
================================================================================

[I 2025-11-09 17:28:07,244] Trial 5439 pruned. Pruned at step 9 with metric 0.5919
[W 2025-11-09 17:28:07,921] The parameter `tok.doc_stride` in Trial#5440 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5440 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.515966524572807e-06
  Dropout: 0.4251199375384192
================================================================================

[I 2025-11-09 17:31:30,941] Trial 5440 pruned. Pruned at step 11 with metric 0.6027
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5441 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 8.936580802546516e-06
  Dropout: 0.0661546441798514
================================================================================

[I 2025-11-09 17:53:05,693] Trial 5441 pruned. Pruned at step 11 with metric 0.5819
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5442 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.6714363417408184e-06
  Dropout: 0.264544174562799
================================================================================

[I 2025-11-09 18:04:31,213] Trial 5442 finished with value: 0.7099315461394318 and parameters: {'seed': 64042, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 5.6714363417408184e-06, 'optim.weight_decay': 0.0011247217964381801, 'optim.beta1': 0.9126778474783912, 'optim.beta2': 0.9848060639277088, 'optim.eps': 1.785589548131853e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.04021805004330438, 'train.clip_grad': 0.7301024268518918, 'model.dropout': 0.264544174562799, 'model.attn_dropout': 0.0002662800413807259, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8662806155880228, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.08819604954817685, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.18331341030293205, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 5443 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 8.382222846033356e-06
  Dropout: 0.18334906485362776
================================================================================

[I 2025-11-09 18:04:47,830] Trial 5443 pruned. OOM: bert-base-uncased bs=64 len=384
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

[OOM] Trial 5443 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 64 with grad_accum=1)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 292.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5444 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 9.820258856497587e-06
  Dropout: 0.3948062850459579
================================================================================

[I 2025-11-09 18:15:15,036] Trial 5434 finished with value: 0.6411257743064906 and parameters: {'seed': 49509, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.734952898408197e-05, 'optim.weight_decay': 0.00030275359729124094, 'optim.beta1': 0.8966770144354376, 'optim.beta2': 0.9930891692575761, 'optim.eps': 6.761216371283713e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.11029941290477838, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.147396262784873, 'model.dropout': 0.4869083527934131, 'model.attn_dropout': 0.11841303387541552, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9574244015415693, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.18404871074339943, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.409512647168791, 'loss.cls.alpha': 0.6376992479608778, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 18:15:15,701] Trial 5445 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 5446 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.3814514697781197e-05
  Dropout: 0.3265557240630533
================================================================================

[I 2025-11-09 18:21:35,955] Trial 5444 finished with value: 0.5974714940232182 and parameters: {'seed': 46482, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.820258856497587e-06, 'optim.weight_decay': 0.11531186257623792, 'optim.beta1': 0.8473068531782703, 'optim.beta2': 0.9952673689398196, 'optim.eps': 5.298478418331185e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0471409515042963, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.8668564105981256, 'model.dropout': 0.3948062850459579, 'model.attn_dropout': 0.17945763139915782, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8193493809306853, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.031166079269869446, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.19681466106097445, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5447 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.2962937632132744e-05
  Dropout: 0.3847975499662576
================================================================================

[I 2025-11-09 18:24:01,605] Trial 5447 pruned. Pruned at step 9 with metric 0.6027
[I 2025-11-09 18:24:02,269] Trial 5448 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-09 18:24:02,904] Trial 5449 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5450 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.3948741269662525e-05
  Dropout: 0.3097143746613358
================================================================================

[I 2025-11-09 18:24:27,262] Trial 5446 pruned. Pruned at step 6 with metric 0.6200
[I 2025-11-09 18:24:27,928] Trial 5451 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5452 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.49204095964367e-05
  Dropout: 0.17598485181710824
================================================================================

[I 2025-11-09 18:32:22,672] Trial 5450 pruned. Pruned at step 11 with metric 0.6315
[W 2025-11-09 18:32:23,549] The parameter `tok.doc_stride` in Trial#5453 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-09 18:32:23,608] Trial 5453 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5454 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 7.359473818446462e-06
  Dropout: 0.3573484068539542
================================================================================

[I 2025-11-09 18:36:08,365] Trial 5454 pruned. Pruned at step 12 with metric 0.5451
[I 2025-11-09 18:36:09,022] Trial 5455 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5456 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.5098049866774196e-05
  Dropout: 0.4223541019721274
================================================================================

[I 2025-11-09 18:40:41,742] Trial 5456 pruned. Pruned at step 9 with metric 0.6069
[I 2025-11-09 18:40:42,396] Trial 5457 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5458 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.7922431594883525e-05
  Dropout: 0.4139155851709924
================================================================================

[I 2025-11-09 18:47:48,056] Trial 5458 pruned. Pruned at step 27 with metric 0.6702

================================================================================
TRIAL 5459 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 8.73231443658632e-06
  Dropout: 0.24601031486493782
================================================================================

[I 2025-11-09 18:52:36,562] Trial 5459 pruned. Pruned at step 18 with metric 0.6535

================================================================================
TRIAL 5460 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.5958961775583e-06
  Dropout: 0.48983385080475145
================================================================================

[I 2025-11-09 19:09:02,102] Trial 5452 finished with value: 0.4273743016759777 and parameters: {'seed': 59756, 'model.name': 'xlm-roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.49204095964367e-05, 'optim.weight_decay': 0.004212371450158789, 'optim.beta1': 0.890172008958256, 'optim.beta2': 0.969772598962665, 'optim.eps': 6.378643706400138e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18586082225523137, 'sched.poly_power': 1.0404803390065398, 'train.clip_grad': 0.7873509721626362, 'model.dropout': 0.17598485181710824, 'model.attn_dropout': 0.17264588956656943, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8881240032789167, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.3057934730593014, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.856963308045009, 'loss.cls.alpha': 0.7394204409842322, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-09 19:09:02,768] Trial 5461 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-09 19:09:03,408] Trial 5462 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5463 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.07943486972205e-06
  Dropout: 0.2721229398930951
================================================================================

[I 2025-11-09 19:09:56,078] Trial 5460 pruned. Pruned at step 27 with metric 0.6328
[W 2025-11-09 19:09:56,715] The parameter `tok.doc_stride` in Trial#5464 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-09 19:09:56,773] Trial 5464 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5465 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.8950417541514435e-05
  Dropout: 0.10254848438808335
================================================================================

[I 2025-11-09 19:17:31,099] Trial 5463 pruned. Pruned at step 18 with metric 0.6294
[I 2025-11-09 19:17:31,769] Trial 5466 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5467 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.1087070241812875e-05
  Dropout: 0.1877995798242582
================================================================================

[I 2025-11-09 19:22:23,286] Trial 5467 pruned. Pruned at step 16 with metric 0.6125

================================================================================
TRIAL 5468 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.5850366810863888e-05
  Dropout: 0.12758441125796016
================================================================================

[I 2025-11-09 19:40:34,674] Trial 5468 finished with value: 0.7266666666666667 and parameters: {'seed': 35109, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.5850366810863888e-05, 'optim.weight_decay': 1.4248017633238385e-05, 'optim.beta1': 0.9432571745165669, 'optim.beta2': 0.9908556372430474, 'optim.eps': 1.6899747332089545e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.08692634206056399, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8838509495544633, 'model.dropout': 0.12758441125796016, 'model.attn_dropout': 0.19481497731493422, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8091764676671371, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.3918774289819628, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.348567734284328, 'loss.cls.alpha': 0.4793201718265719, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-09 19:40:35,289] The parameter `tok.doc_stride` in Trial#5469 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 5469 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.966998029138899e-06
  Dropout: 0.28746292729923706
================================================================================

[I 2025-11-09 19:53:39,287] Trial 5469 pruned. Pruned at step 27 with metric 0.5485

================================================================================
TRIAL 5470 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 3.8894974646279005e-05
  Dropout: 0.0981611199201523
================================================================================

[I 2025-11-09 20:17:28,526] Trial 5465 pruned. Pruned at step 32 with metric 0.5968
[I 2025-11-09 20:17:29,287] Trial 5471 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-09 20:17:29,916] Trial 5472 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-11-09 20:17:30,553] Trial 5473 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5474 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.5718200694586618e-05
  Dropout: 0.38494780371967396
================================================================================

[I 2025-11-09 20:37:36,079] Trial 5470 finished with value: 0.6846035498517768 and parameters: {'seed': 44867, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 3.8894974646279005e-05, 'optim.weight_decay': 1.4940721593626199e-05, 'optim.beta1': 0.9367427700415117, 'optim.beta2': 0.9736354769913975, 'optim.eps': 2.156772859607581e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.056384492646919845, 'train.clip_grad': 1.2799841785803583, 'model.dropout': 0.0981611199201523, 'model.attn_dropout': 0.18837119120222928, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8304066069443484, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.343417923393931, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.5971131866344885, 'loss.cls.alpha': 0.4131816104457113, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 51 (patience=20)

================================================================================
TRIAL 5475 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.8947473071707733e-05
  Dropout: 0.42330544347206167
================================================================================

[I 2025-11-09 20:40:01,349] Trial 5475 pruned. Pruned at step 9 with metric 0.5900
[W 2025-11-09 20:40:01,972] The parameter `tok.doc_stride` in Trial#5476 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5476 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.213008675138401e-05
  Dropout: 0.188866607294311
================================================================================

[I 2025-11-09 20:42:39,616] Trial 5476 pruned. Pruned at step 7 with metric 0.5823
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5477 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 4.47321100622868e-05
  Dropout: 0.37144214271998505
================================================================================

[I 2025-11-09 20:46:04,240] Trial 5477 pruned. Pruned at step 11 with metric 0.6254
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5478 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.075568620356427e-06
  Dropout: 0.4380077213628686
================================================================================

[I 2025-11-09 20:51:43,691] Trial 5478 pruned. Pruned at step 27 with metric 0.6472
[I 2025-11-09 20:51:44,366] Trial 5479 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-09 20:51:45,021] Trial 5480 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-09 20:51:45,669] Trial 5481 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5482 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.8184480595483353e-05
  Dropout: 0.08255073655343886
================================================================================

[I 2025-11-09 20:55:09,374] Trial 5482 pruned. Pruned at step 10 with metric 0.6232
[W 2025-11-09 20:55:10,005] The parameter `tok.doc_stride` in Trial#5483 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5483 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3363096290491294e-05
  Dropout: 0.24505578694627325
================================================================================

[I 2025-11-09 20:58:55,085] Trial 5483 pruned. Pruned at step 12 with metric 0.5769
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5484 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 7.490021812072622e-05
  Dropout: 0.1488987319386839
================================================================================

[I 2025-11-09 21:03:47,721] Trial 5474 pruned. Pruned at step 27 with metric 0.6341

================================================================================
TRIAL 5485 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 8.565751946612859e-06
  Dropout: 0.2862285832826674
================================================================================

[I 2025-11-09 21:09:29,935] Trial 5485 pruned. Pruned at step 11 with metric 0.5382
[I 2025-11-09 21:09:30,590] Trial 5486 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-09 21:09:31,225] Trial 5487 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5488 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.3418384638530917e-05
  Dropout: 0.4149093532527766
================================================================================

[I 2025-11-09 21:16:02,112] Trial 5488 pruned. Pruned at step 9 with metric 0.5796

================================================================================
TRIAL 5489 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.445575394811282e-05
  Dropout: 0.043996233058114106
================================================================================

[I 2025-11-09 21:29:39,082] Trial 5489 finished with value: 0.7143710191082803 and parameters: {'seed': 32956, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.445575394811282e-05, 'optim.weight_decay': 0.00013794013265546872, 'optim.beta1': 0.8697203338963349, 'optim.beta2': 0.9879146767749228, 'optim.eps': 2.6950563862994847e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.11137678719493199, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8059383622343295, 'model.dropout': 0.043996233058114106, 'model.attn_dropout': 0.27611738355751103, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.937071778077643, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.09181283837523779, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18145617845915965, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5490 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.775227807331071e-06
  Dropout: 0.25159755426376984
================================================================================

[I 2025-11-09 21:33:28,094] Trial 5490 pruned. Pruned at step 13 with metric 0.6164
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5491 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.039525627113493e-06
  Dropout: 0.3582028285260287
================================================================================

[I 2025-11-09 21:38:16,983] Trial 5491 pruned. Pruned at step 16 with metric 0.6174
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5492 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 4.056325734447947e-05
  Dropout: 0.16017600683203706
================================================================================

[I 2025-11-09 21:43:48,805] Trial 5492 pruned. Pruned at step 9 with metric 0.5593
[I 2025-11-09 21:43:49,492] Trial 5493 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5494 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 4.5185328993579025e-05
  Dropout: 0.40308513463933016
================================================================================

[I 2025-11-09 21:44:00,111] Trial 5484 finished with value: 0.4444444444444444 and parameters: {'seed': 61616, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 320, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 7.490021812072622e-05, 'optim.weight_decay': 0.000774594141837831, 'optim.beta1': 0.9438344498419419, 'optim.beta2': 0.9714800297504964, 'optim.eps': 5.029561001670561e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.041365573199024785, 'train.clip_grad': 1.2098238037911593, 'model.dropout': 0.1488987319386839, 'model.attn_dropout': 0.16194364338322004, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8940503558930322, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.27910255181572563, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.221156691957281, 'loss.cls.alpha': 0.31710085428637735, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5495 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 9.937287065417503e-06
  Dropout: 0.31796746874767906
================================================================================

[I 2025-11-09 21:48:43,270] Trial 5494 pruned. Pruned at step 9 with metric 0.5417
[I 2025-11-09 21:48:43,954] Trial 5496 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[W 2025-11-09 21:48:44,555] The parameter `tok.doc_stride` in Trial#5497 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-09 21:48:44,612] Trial 5497 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5498 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.2918989797519902e-05
  Dropout: 0.17253586873642224
================================================================================

[I 2025-11-09 22:00:59,908] Trial 5495 pruned. Pruned at step 12 with metric 0.5383
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5499 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.3634725527223546e-05
  Dropout: 0.3270910418442473
================================================================================

[I 2025-11-09 22:02:36,233] Trial 5498 finished with value: 0.6691479200289845 and parameters: {'seed': 52184, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.2918989797519902e-05, 'optim.weight_decay': 1.2855659620709864e-06, 'optim.beta1': 0.9025169185372248, 'optim.beta2': 0.9971317528214428, 'optim.eps': 1.000619533705723e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.047866056810165626, 'sched.poly_power': 0.7286104899129044, 'train.clip_grad': 1.224532573791269, 'model.dropout': 0.17253586873642224, 'model.attn_dropout': 0.19522632401852424, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8249442689147701, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.3993960013687664, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.3840620135542885, 'loss.cls.alpha': 0.676548311228209, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 5500 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 2.7533285816810658e-05
  Dropout: 0.1695680088435561
================================================================================

[I 2025-11-09 22:02:44,280] Trial 5500 pruned. OOM: microsoft/deberta-v3-large bs=12 len=256
[I 2025-11-09 22:02:45,069] Trial 5501 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-09 22:02:45,715] Trial 5502 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)

[OOM] Trial 5500 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 56.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5503 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 9.164527515660931e-06
  Dropout: 0.32228712650532987
================================================================================

[I 2025-11-09 22:07:24,688] Trial 5503 pruned. Pruned at step 9 with metric 0.6079
[I 2025-11-09 22:07:25,375] Trial 5504 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5505 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.5178717141297978e-05
  Dropout: 0.2820813131616157
================================================================================

[I 2025-11-09 22:21:49,358] Trial 5505 pruned. Pruned at step 15 with metric 0.6372
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5506 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 4.573664892975815e-05
  Dropout: 0.031289637264031556
================================================================================

[I 2025-11-09 22:23:45,329] Trial 5506 pruned. Pruned at step 8 with metric 0.6252

================================================================================
TRIAL 5507 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 9.711911615877596e-06
  Dropout: 0.28363010595002014
================================================================================

[I 2025-11-09 22:31:26,109] Trial 5507 pruned. Pruned at step 14 with metric 0.5880
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5508 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.4257712741451553e-05
  Dropout: 0.35288187392227316
================================================================================

[I 2025-11-09 22:40:55,440] Trial 5508 pruned. Pruned at step 12 with metric 0.5763
[W 2025-11-09 22:40:56,079] The parameter `tok.doc_stride` in Trial#5509 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5509 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.7556715341899725e-05
  Dropout: 0.30707409781068484
================================================================================

[I 2025-11-09 22:43:35,885] Trial 5509 pruned. Pruned at step 14 with metric 0.5531

================================================================================
TRIAL 5510 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.9896288271733734e-05
  Dropout: 0.4289851384775319
================================================================================

[I 2025-11-09 23:03:21,596] Trial 5499 finished with value: 0.44594594594594594 and parameters: {'seed': 63338, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.3634725527223546e-05, 'optim.weight_decay': 1.6389103435611667e-05, 'optim.beta1': 0.82193983772502, 'optim.beta2': 0.9569138956723041, 'optim.eps': 5.295724314731996e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.12748209411587264, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.6612106452789552, 'model.dropout': 0.3270910418442473, 'model.attn_dropout': 0.051223443294059576, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8560687417189954, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.20292667098783862, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19979363444514212, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5511 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.1101380029121735e-05
  Dropout: 0.008801293182583414
================================================================================

[I 2025-11-09 23:11:30,366] Trial 5510 finished with value: 0.450402144772118 and parameters: {'seed': 27119, 'model.name': 'bert-large-uncased', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.9896288271733734e-05, 'optim.weight_decay': 0.00016641785948431524, 'optim.beta1': 0.8410030304325209, 'optim.beta2': 0.9689934661849486, 'optim.eps': 2.8802820209773125e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.08892723661292198, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.161898578686786, 'model.dropout': 0.4289851384775319, 'model.attn_dropout': 0.2127467428765232, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9326058126524884, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.4642136080260935, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.999958836225675, 'loss.cls.alpha': 0.3910075294993596, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5512 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.197824544752556e-05
  Dropout: 0.145080622604345
================================================================================

[I 2025-11-09 23:15:34,161] Trial 5512 pruned. Pruned at step 9 with metric 0.5347
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5513 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 7.683700028816212e-06
  Dropout: 0.23308089000437845
================================================================================

[I 2025-11-09 23:16:52,093] Trial 5511 pruned. Pruned at step 7 with metric 0.5967
[W 2025-11-09 23:16:52,824] The parameter `tok.doc_stride` in Trial#5514 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5514 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.3325938892602144e-05
  Dropout: 0.3619333618179074
================================================================================

[I 2025-11-09 23:21:05,532] Trial 5514 pruned. Pruned at step 11 with metric 0.5477
[W 2025-11-09 23:21:06,215] The parameter `tok.doc_stride` in Trial#5515 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-09 23:21:06,275] Trial 5515 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5516 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 9.044073601783382e-06
  Dropout: 0.21940750459362515
================================================================================

[I 2025-11-09 23:24:36,630] Trial 5516 pruned. Pruned at step 12 with metric 0.6372
[I 2025-11-09 23:24:37,285] Trial 5517 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5518 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.9782222377561363e-05
  Dropout: 0.023808459130158513
================================================================================

[I 2025-11-09 23:45:49,124] Trial 5518 pruned. Pruned at step 9 with metric 0.6298
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5519 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.420983044296812e-05
  Dropout: 0.03483911130456982
================================================================================

[I 2025-11-09 23:50:35,548] Trial 5519 pruned. Pruned at step 13 with metric 0.6462
[I 2025-11-09 23:50:36,238] Trial 5520 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5521 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.5450909119518174e-05
  Dropout: 0.09190321989980701
================================================================================

[I 2025-11-10 00:00:31,286] Trial 5513 pruned. Pruned at step 16 with metric 0.5886
[I 2025-11-10 00:00:32,040] Trial 5522 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 00:00:32,676] Trial 5523 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5524 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 7.758230086015301e-06
  Dropout: 0.40087272506267646
================================================================================

[I 2025-11-10 00:11:32,202] Trial 5521 finished with value: 0.7149011227570299 and parameters: {'seed': 37376, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.5450909119518174e-05, 'optim.weight_decay': 1.1677916285808699e-05, 'optim.beta1': 0.8664039185616031, 'optim.beta2': 0.9712411194506906, 'optim.eps': 1.031600052022941e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.003895110415745754, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.1764265704429309, 'model.dropout': 0.09190321989980701, 'model.attn_dropout': 0.2094191995286525, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8021642033461549, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.43321940210680965, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16351850774611595, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 00:11:32,885] Trial 5525 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 5526 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.5294125202111447e-05
  Dropout: 0.02265279151004813
================================================================================

[I 2025-11-10 00:16:58,992] Trial 5524 pruned. Pruned at step 27 with metric 0.6387
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5527 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.1093344605760993e-05
  Dropout: 0.012524616125610816
================================================================================

[I 2025-11-10 00:17:08,245] Trial 5526 pruned. OOM: roberta-base bs=64 len=256
[I 2025-11-10 00:17:08,448] Trial 5527 pruned. OOM: roberta-base bs=48 len=384
[I 2025-11-10 00:17:09,206] Trial 5528 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5527 exceeded GPU memory:
  Model: roberta-base
  Batch size: 48 (effective: 192 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5526 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 210.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5529 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 5.915909911903105e-06
  Dropout: 0.010456389896748255
================================================================================


================================================================================
TRIAL 5530 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 8.465862009008037e-06
  Dropout: 0.0534717914866526
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-10 00:30:06,724] Trial 5529 pruned. Pruned at step 27 with metric 0.6135
[W 2025-11-10 00:30:07,403] The parameter `tok.doc_stride` in Trial#5531 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5531 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.3668126420588972e-05
  Dropout: 0.032807742608893936
================================================================================

[I 2025-11-10 00:32:52,569] Trial 5530 pruned. Pruned at step 16 with metric 0.6148
[I 2025-11-10 00:32:53,261] Trial 5532 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 00:32:53,902] Trial 5533 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 00:32:54,536] Trial 5534 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-10 00:32:55,138] The parameter `tok.doc_stride` in Trial#5535 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5535 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.727962127001624e-06
  Dropout: 0.3122074905023772
================================================================================

[I 2025-11-10 00:38:28,258] Trial 5535 pruned. Pruned at step 27 with metric 0.6385

================================================================================
TRIAL 5536 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 8.116350343501681e-06
  Dropout: 0.22802826971374057
================================================================================

[I 2025-11-10 00:48:30,883] Trial 5536 pruned. Pruned at step 21 with metric 0.5639
[I 2025-11-10 00:48:31,756] Trial 5537 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 00:48:32,411] Trial 5538 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-10 00:48:33,044] Trial 5539 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5540 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.070990503856746e-05
  Dropout: 0.45699297549758516
================================================================================

[I 2025-11-10 00:53:41,216] Trial 5531 pruned. Pruned at step 7 with metric 0.6174

================================================================================
TRIAL 5541 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.2123952367315704e-05
  Dropout: 0.40601871745029927
================================================================================

[I 2025-11-10 01:01:38,721] Trial 5540 pruned. Pruned at step 7 with metric 0.5923

================================================================================
TRIAL 5542 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 0.00010657017663471452
  Dropout: 0.253824863147745
================================================================================

[I 2025-11-10 01:02:45,407] Trial 5541 pruned. Pruned at step 9 with metric 0.5444
[I 2025-11-10 01:02:46,091] Trial 5543 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-10 01:02:46,740] Trial 5544 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5545 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.067093818613656e-05
  Dropout: 0.40002878849610835
================================================================================

[I 2025-11-10 01:08:15,896] Trial 5542 pruned. Pruned at step 8 with metric 0.4306
[W 2025-11-10 01:08:16,549] The parameter `tok.doc_stride` in Trial#5546 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 01:08:16,609] Trial 5546 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5547 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 8.343483820100277e-06
  Dropout: 0.26571814122637094
================================================================================

[I 2025-11-10 01:20:37,482] Trial 5545 finished with value: 0.7307591279222485 and parameters: {'seed': 51968, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.067093818613656e-05, 'optim.weight_decay': 0.0003761114761872147, 'optim.beta1': 0.8709225927416745, 'optim.beta2': 0.9862779345029116, 'optim.eps': 4.8989032100533156e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.05810932842045696, 'train.clip_grad': 0.49565719078334675, 'model.dropout': 0.40002878849610835, 'model.attn_dropout': 0.10224360575901581, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.928165345726247, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.18416844490631112, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.19986384177616062, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 49 (patience=20)

================================================================================
TRIAL 5548 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.0387425988805783e-05
  Dropout: 0.2592965267130738
================================================================================

[I 2025-11-10 01:22:20,845] Trial 5547 pruned. Pruned at step 10 with metric 0.5733
[I 2025-11-10 01:22:21,554] Trial 5549 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 01:22:22,186] Trial 5550 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-10 01:22:22,827] Trial 5551 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5552 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.5069272899257262e-05
  Dropout: 0.49334786822992105
================================================================================

[I 2025-11-10 01:31:03,157] Trial 5552 pruned. Pruned at step 9 with metric 0.6252
[W 2025-11-10 01:31:03,978] The parameter `tok.doc_stride` in Trial#5553 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5553 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6230090332141993e-05
  Dropout: 0.45773435397205925
================================================================================

[I 2025-11-10 01:34:25,729] Trial 5548 pruned. Pruned at step 8 with metric 0.5579
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5554 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.3582413295019288e-05
  Dropout: 0.3314514625686329
================================================================================

[I 2025-11-10 01:37:11,446] Trial 5553 pruned. Pruned at step 20 with metric 0.6161
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5555 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.005651700774053e-05
  Dropout: 0.0961673330588557
================================================================================

[I 2025-11-10 01:39:50,057] Trial 5555 pruned. Pruned at step 7 with metric 0.5558
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5556 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.531204410104295e-05
  Dropout: 0.05045470009232027
================================================================================

[I 2025-11-10 01:48:10,663] Trial 5554 pruned. Pruned at step 11 with metric 0.5919
[W 2025-11-10 01:48:11,348] The parameter `tok.doc_stride` in Trial#5557 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5557 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.0725360168172338e-05
  Dropout: 0.15563626248419865
================================================================================

[I 2025-11-10 01:48:43,886] Trial 5556 pruned. Pruned at step 13 with metric 0.5917
[W 2025-11-10 01:48:44,552] The parameter `tok.doc_stride` in Trial#5558 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5558 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.661977173081144e-06
  Dropout: 0.14797140424692007
================================================================================

[I 2025-11-10 01:51:15,722] Trial 5557 pruned. Pruned at step 10 with metric 0.6083
[I 2025-11-10 01:51:16,406] Trial 5559 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5560 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.108737529208623e-05
  Dropout: 0.042828187554331376
================================================================================

[I 2025-11-10 01:56:48,382] Trial 5560 pruned. Pruned at step 9 with metric 0.5962
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5561 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 3.285556395829667e-05
  Dropout: 0.05992401540492456
================================================================================

[I 2025-11-10 01:59:03,160] Trial 5558 pruned. Pruned at step 27 with metric 0.6512
[W 2025-11-10 01:59:03,811] The parameter `tok.doc_stride` in Trial#5562 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5562 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.5616107729111707e-05
  Dropout: 0.3759048045612765
================================================================================

[I 2025-11-10 02:02:02,551] Trial 5562 pruned. Pruned at step 11 with metric 0.5639
[W 2025-11-10 02:02:03,242] The parameter `tok.doc_stride` in Trial#5563 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5563 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.007420405584867e-05
  Dropout: 0.30622478292634536
================================================================================

[I 2025-11-10 02:07:13,133] Trial 5563 pruned. Pruned at step 27 with metric 0.6200
[I 2025-11-10 02:07:13,803] Trial 5564 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-10 02:07:14,413] The parameter `tok.doc_stride` in Trial#5565 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5565 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.231744348038514e-06
  Dropout: 0.2707682273435281
================================================================================

[I 2025-11-10 02:19:59,006] Trial 5565 pruned. Pruned at step 23 with metric 0.5375

================================================================================
TRIAL 5566 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.3344611804123885e-05
  Dropout: 0.32014744337632484
================================================================================

[I 2025-11-10 02:28:20,878] Trial 5566 pruned. Pruned at step 16 with metric 0.5900
[I 2025-11-10 02:28:21,545] Trial 5567 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5568 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.044748212378782e-05
  Dropout: 0.27494061249570045
================================================================================

[I 2025-11-10 02:31:51,787] Trial 5568 pruned. Pruned at step 10 with metric 0.6134
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5569 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.7726071628596195e-06
  Dropout: 0.21234673040493496
================================================================================

[I 2025-11-10 02:45:06,044] Trial 5569 pruned. Pruned at step 9 with metric 0.6095
[W 2025-11-10 02:45:06,738] The parameter `tok.doc_stride` in Trial#5570 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5570 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.025942053631516e-05
  Dropout: 0.053416199021174116
================================================================================

[I 2025-11-10 02:45:52,595] Trial 5561 finished with value: 0.4444444444444444 and parameters: {'seed': 64711, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.285556395829667e-05, 'optim.weight_decay': 1.2919733874954026e-05, 'optim.beta1': 0.8353215014714054, 'optim.beta2': 0.9746660901806747, 'optim.eps': 3.4458247486490166e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.15437227659922687, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.26397033448375, 'model.dropout': 0.05992401540492456, 'model.attn_dropout': 0.20421951279250242, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9031957726678298, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.26290741294210035, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18384750266303276, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5571 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 3.740710730936194e-05
  Dropout: 0.019412927675486066
================================================================================

[I 2025-11-10 02:51:01,697] Trial 5571 pruned. Pruned at step 10 with metric 0.5218
[I 2025-11-10 02:51:02,358] Trial 5572 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5573 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.2825923161091348e-05
  Dropout: 0.10505286220781461
================================================================================

[I 2025-11-10 02:53:43,616] Trial 5570 pruned. Pruned at step 7 with metric 0.6053
[I 2025-11-10 02:53:44,302] Trial 5574 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 02:53:44,944] Trial 5575 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-10 02:53:45,583] Trial 5576 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[W 2025-11-10 02:53:46,194] The parameter `tok.doc_stride` in Trial#5577 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5577 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.2149186642103967e-05
  Dropout: 0.07390251014940682
================================================================================

[I 2025-11-10 03:00:49,044] Trial 5577 pruned. Pruned at step 27 with metric 0.5859
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5578 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 3.193541622404931e-05
  Dropout: 0.18746067263123062
================================================================================

[I 2025-11-10 03:19:10,213] Trial 5573 finished with value: 0.6583333333333333 and parameters: {'seed': 45549, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.2825923161091348e-05, 'optim.weight_decay': 0.0010774739960112015, 'optim.beta1': 0.8472777509132958, 'optim.beta2': 0.9754526288657832, 'optim.eps': 1.3049861021052138e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13216028734986368, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.0093791469355724, 'model.dropout': 0.10505286220781461, 'model.attn_dropout': 0.22965472221483907, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8317571495992748, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.05836985408391061, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.219312445663757, 'loss.cls.alpha': 0.5364902481097745, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 03:19:10,908] Trial 5579 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5580 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 5.658816277496233e-06
  Dropout: 0.45336917317211045
================================================================================

[I 2025-11-10 03:27:41,049] Trial 5578 finished with value: 0.4273743016759777 and parameters: {'seed': 48681, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.193541622404931e-05, 'optim.weight_decay': 1.5999176659184657e-05, 'optim.beta1': 0.9107529963661349, 'optim.beta2': 0.9771134601698638, 'optim.eps': 9.695362456517836e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.04508022101700517, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.352831668398064, 'model.dropout': 0.18746067263123062, 'model.attn_dropout': 0.21465605431553528, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8737485439985414, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.25580850861206933, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.99700521070387, 'loss.cls.alpha': 0.2980609131613683, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5581 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0478683308719686e-05
  Dropout: 0.16248531768682842
================================================================================

[I 2025-11-10 03:32:44,810] Trial 5581 pruned. Pruned at step 13 with metric 0.6250
[I 2025-11-10 03:32:45,469] Trial 5582 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5583 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 8.598737465094842e-05
  Dropout: 0.06094030970325497
================================================================================

[I 2025-11-10 03:32:53,035] Trial 5580 pruned. OOM: microsoft/deberta-v3-base bs=16 len=352
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5580 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 34.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5584 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.998640482275434e-05
  Dropout: 0.3172012435837413
================================================================================

[I 2025-11-10 04:20:45,762] Trial 5584 finished with value: 0.43989071038251365 and parameters: {'seed': 63405, 'model.name': 'roberta-large', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.998640482275434e-05, 'optim.weight_decay': 0.0672892801404762, 'optim.beta1': 0.8267457911446983, 'optim.beta2': 0.9868608392039732, 'optim.eps': 9.75449925463504e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.12058758142232759, 'sched.poly_power': 0.6979077492623066, 'train.clip_grad': 0.4760145876563454, 'model.dropout': 0.3172012435837413, 'model.attn_dropout': 0.056944845825058, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8357072502397985, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.19470953775328287, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.6816328458141148, 'loss.cls.alpha': 0.4185433532730025, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5585 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 3.851518235163585e-05
  Dropout: 0.3593712007251433
================================================================================

[I 2025-11-10 04:34:06,882] Trial 5585 finished with value: 0.6554621848739496 and parameters: {'seed': 60432, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 32, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 3.851518235163585e-05, 'optim.weight_decay': 0.1467748458943463, 'optim.beta1': 0.83537657912209, 'optim.beta2': 0.9741876343291295, 'optim.eps': 6.624382357994059e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.020192322487764734, 'sched.poly_power': 0.6084209342996562, 'train.clip_grad': 1.3315432232587454, 'model.dropout': 0.3593712007251433, 'model.attn_dropout': 0.16051383935186223, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9782139928312545, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.10520289598139462, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.9266666739505007, 'loss.cls.alpha': 0.6841473140518222, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 5586 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.386089678168149e-05
  Dropout: 0.05519066311111121
================================================================================

[I 2025-11-10 04:34:12,486] Trial 5586 pruned. OOM: bert-base-uncased bs=48 len=384
[I 2025-11-10 04:34:14,101] Trial 5583 pruned. OOM: microsoft/deberta-v3-large bs=8 len=192
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-10 04:34:14,996] Trial 5588 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

[OOM] Trial 5586 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 384 with grad_accum=8)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5587 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.009684147299156e-05
  Dropout: 0.30503234176762195
================================================================================


[OOM] Trial 5583 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 16 with grad_accum=2)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

[I 2025-11-10 04:34:15,917] Trial 5589 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-10 04:34:16,673] Trial 5590 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-10 04:34:17,284] The parameter `tok.doc_stride` in Trial#5591 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5591 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.2629670099606599e-05
  Dropout: 0.41409056446726844
================================================================================

[I 2025-11-10 04:47:30,698] Trial 5587 pruned. Pruned at step 27 with metric 0.6447
[I 2025-11-10 04:47:31,380] Trial 5592 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5593 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.283211738193955e-06
  Dropout: 0.49317098100091034
================================================================================

[I 2025-11-10 04:54:38,319] Trial 5593 pruned. Pruned at step 14 with metric 0.6341

================================================================================
TRIAL 5594 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.453803064816114e-05
  Dropout: 0.1432032698597373
================================================================================

[I 2025-11-10 04:56:43,552] Trial 5591 finished with value: 0.6863100957261308 and parameters: {'seed': 40069, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.2629670099606599e-05, 'optim.weight_decay': 0.007317232795538005, 'optim.beta1': 0.8726334238800082, 'optim.beta2': 0.9681091005536377, 'optim.eps': 1.8650878708145107e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.038064266263734395, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8152219215029074, 'model.dropout': 0.41409056446726844, 'model.attn_dropout': 0.15449857891987295, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9286820292944653, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4209943904706686, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.1431657620642968, 'loss.cls.alpha': 0.5999241011612181, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 04:56:44,256] Trial 5595 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5596 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.765090779427107e-05
  Dropout: 0.18122359685896683
================================================================================

[I 2025-11-10 05:00:21,077] Trial 5596 pruned. Pruned at step 10 with metric 0.6286
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5597 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 8.608222296625688e-06
  Dropout: 0.24932525541833656
================================================================================

[I 2025-11-10 05:00:27,752] Trial 5597 pruned. OOM: microsoft/deberta-v3-large bs=16 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5597 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 48 with grad_accum=3)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5598 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.35526866287062e-06
  Dropout: 0.38417344464262787
================================================================================

[I 2025-11-10 05:03:49,501] Trial 5598 pruned. Pruned at step 12 with metric 0.5676

================================================================================
TRIAL 5599 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.137021354401794e-05
  Dropout: 0.43423426914755286
================================================================================

[I 2025-11-10 05:06:54,558] Trial 5599 pruned. Pruned at step 12 with metric 0.6583
[I 2025-11-10 05:06:55,226] Trial 5600 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5601 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0235182490551981e-05
  Dropout: 0.18071831968706195
================================================================================

[I 2025-11-10 05:19:28,581] Trial 5594 pruned. Pruned at step 11 with metric 0.5726
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5602 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.999111647838528e-05
  Dropout: 0.10983994356299343
================================================================================

[I 2025-11-10 05:27:47,696] Trial 5602 pruned. Pruned at step 15 with metric 0.5997
[W 2025-11-10 05:27:48,338] The parameter `tok.doc_stride` in Trial#5603 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 05:27:48,399] Trial 5603 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 05:27:49,036] Trial 5604 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5605 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.7983431966890684e-05
  Dropout: 0.06144081025386719
================================================================================

[I 2025-11-10 05:29:37,768] Trial 5601 finished with value: 0.7057134654033879 and parameters: {'seed': 62072, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.0235182490551981e-05, 'optim.weight_decay': 1.0909360212432458e-06, 'optim.beta1': 0.8801975197530204, 'optim.beta2': 0.9723720335976476, 'optim.eps': 1.1908684598859283e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.027516504575358373, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.6327766334832972, 'model.dropout': 0.18071831968706195, 'model.attn_dropout': 0.12667818458631658, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8318080901736439, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3683451459284236, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16551955223585396, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 39 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 400 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 5606 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 3.121044601038936e-05
  Dropout: 0.019736018318646464
================================================================================

[I 2025-11-10 05:33:46,497] Trial 5605 pruned. Pruned at step 12 with metric 0.6223
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
[GPU RESET] Performing periodic GPU reset after 400 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 5607 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 4.545523398832978e-05
  Dropout: 0.0116063988329718
================================================================================

[I 2025-11-10 06:14:24,630] Trial 5606 finished with value: 0.44594594594594594 and parameters: {'seed': 62837, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.121044601038936e-05, 'optim.weight_decay': 4.6043886717338627e-05, 'optim.beta1': 0.9302616859123994, 'optim.beta2': 0.9643515204415798, 'optim.eps': 3.0841798354400077e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1153946419309439, 'sched.poly_power': 0.7247755305416211, 'train.clip_grad': 1.459714356262414, 'model.dropout': 0.019736018318646464, 'model.attn_dropout': 0.2538286954638501, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.835065026287812, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.41701433981902636, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1832952035125412, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5608 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.1379006295388466e-06
  Dropout: 0.3528094289238758
================================================================================

[I 2025-11-10 06:15:53,580] Trial 5607 pruned. Pruned at step 10 with metric 0.5808

================================================================================
TRIAL 5609 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.93083388316183e-06
  Dropout: 0.28418060485615426
================================================================================

[I 2025-11-10 06:25:43,794] Trial 5609 pruned. Pruned at step 11 with metric 0.5705
[I 2025-11-10 06:25:44,638] Trial 5610 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5611 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 4.672191246339764e-05
  Dropout: 0.23928961042356794
================================================================================

[I 2025-11-10 06:42:25,223] Trial 5611 finished with value: 0.7166109253065776 and parameters: {'seed': 64944, 'model.name': 'roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 4.672191246339764e-05, 'optim.weight_decay': 0.00017978393648968851, 'optim.beta1': 0.854575648962494, 'optim.beta2': 0.9710254368357116, 'optim.eps': 1.0286890518430054e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1907682800399717, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.0155748274291654, 'model.dropout': 0.23928961042356794, 'model.attn_dropout': 0.11567848822248471, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9904347615282254, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.48341250274772807, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1990769431790544, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 06:42:25,893] Trial 5612 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 5613 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.0774175854041508e-05
  Dropout: 0.321863423244873
================================================================================

[I 2025-11-10 06:58:02,400] Trial 5613 pruned. Pruned at step 16 with metric 0.6056
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5614 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.392767962424999e-05
  Dropout: 0.44653163224795034
================================================================================

[I 2025-11-10 07:07:20,427] Trial 5614 finished with value: 0.7254298642533936 and parameters: {'seed': 62838, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.392767962424999e-05, 'optim.weight_decay': 0.02243185831892205, 'optim.beta1': 0.8191056861383276, 'optim.beta2': 0.9538782758831991, 'optim.eps': 6.536915297862295e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04989008998088992, 'sched.poly_power': 1.5528276543893722, 'train.clip_grad': 1.2344860180558703, 'model.dropout': 0.44653163224795034, 'model.attn_dropout': 0.13564983637810119, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9635095927734686, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.1684559128425476, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1797227952764848, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-10 07:07:21,063] The parameter `tok.doc_stride` in Trial#5615 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5615 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 5.083389725932921e-06
  Dropout: 0.3424657070611765
================================================================================

[I 2025-11-10 07:15:45,716] Trial 5608 finished with value: 0.7359848484848485 and parameters: {'seed': 38446, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 6.1379006295388466e-06, 'optim.weight_decay': 2.6691109970771922e-05, 'optim.beta1': 0.8839814924137778, 'optim.beta2': 0.9755671660792494, 'optim.eps': 3.8936423888629496e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18966796073332678, 'train.clip_grad': 0.9374333759034555, 'model.dropout': 0.3528094289238758, 'model.attn_dropout': 0.08567841595570805, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9067997164411997, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.339391291745419, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.4315338482721716, 'loss.cls.alpha': 0.5379961517878269, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 5616 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.5802186234949254e-05
  Dropout: 0.3878357142833313
================================================================================

[I 2025-11-10 07:21:48,953] Trial 5616 pruned. Pruned at step 13 with metric 0.5218
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5617 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.2244209194276022e-05
  Dropout: 0.33140055962295
================================================================================

[I 2025-11-10 07:26:48,268] Trial 5617 finished with value: 0.7389865036923859 and parameters: {'seed': 38766, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.2244209194276022e-05, 'optim.weight_decay': 0.012031841038023619, 'optim.beta1': 0.8192603726241565, 'optim.beta2': 0.9628082995204673, 'optim.eps': 3.4960004585883045e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.18967851455685866, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.4927518720734761, 'model.dropout': 0.33140055962295, 'model.attn_dropout': 0.03620881658420652, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8442931546801626, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4610112852009269, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.9653886346873306, 'loss.cls.alpha': 0.48515098930151795, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5618 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.966544862107897e-05
  Dropout: 0.039513607321565426
================================================================================

[I 2025-11-10 07:39:06,663] Trial 5618 finished with value: 0.45478723404255317 and parameters: {'seed': 49984, 'model.name': 'xlm-roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.966544862107897e-05, 'optim.weight_decay': 0.0004435825994964548, 'optim.beta1': 0.9251106936603876, 'optim.beta2': 0.9893728719540171, 'optim.eps': 1.951812152251037e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04089384639966596, 'sched.poly_power': 0.8391517572805474, 'train.clip_grad': 1.3653298190269227, 'model.dropout': 0.039513607321565426, 'model.attn_dropout': 0.22306965975270623, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8038882593896066, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.2853444494247917, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.111877725008037, 'loss.cls.alpha': 0.7694452486087398, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 07:39:07,325] Trial 5619 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5620 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.2383068823777507e-05
  Dropout: 0.043532193707390233
================================================================================

[I 2025-11-10 07:47:30,790] Trial 5620 finished with value: 0.6737471174770211 and parameters: {'seed': 64726, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.2383068823777507e-05, 'optim.weight_decay': 6.691759863289347e-06, 'optim.beta1': 0.9080603831156483, 'optim.beta2': 0.9737702589628856, 'optim.eps': 3.5006639179178936e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.001319531666678083, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.4922896155352412, 'model.dropout': 0.043532193707390233, 'model.attn_dropout': 0.20632616967581288, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8081431239098167, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.3467269172052108, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.235192334373492, 'loss.cls.alpha': 0.41979903958786025, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 07:47:31,458] Trial 5621 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 5622 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 3.731847578921562e-05
  Dropout: 0.017377845653456958
================================================================================

[I 2025-11-10 07:51:29,836] Trial 5615 pruned. Pruned at step 14 with metric 0.6538
[I 2025-11-10 07:51:30,613] Trial 5623 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5624 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1401395924902978e-05
  Dropout: 0.2596232214370809
================================================================================

[I 2025-11-10 07:54:29,224] Trial 5622 pruned. Pruned at step 12 with metric 0.6117
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5625 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 7.994336093366367e-06
  Dropout: 0.09003871011319009
================================================================================

[I 2025-11-10 07:58:32,816] Trial 5624 pruned. Pruned at step 11 with metric 0.5725

================================================================================
TRIAL 5626 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 9.499556687631968e-06
  Dropout: 0.051805759967787246
================================================================================

[I 2025-11-10 08:06:45,960] Trial 5625 finished with value: 0.6215384615384616 and parameters: {'seed': 57923, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 7.994336093366367e-06, 'optim.weight_decay': 0.00014384277346184894, 'optim.beta1': 0.8581699052909655, 'optim.beta2': 0.9936554849377723, 'optim.eps': 3.560006632013023e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.0542948091129207, 'train.clip_grad': 1.4592677921120853, 'model.dropout': 0.09003871011319009, 'model.attn_dropout': 0.29291324949350833, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8425398050529724, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.09132029012093235, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.15511163258316943, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 5627 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 4.980190120067842e-05
  Dropout: 0.3627745841497053
================================================================================

[I 2025-11-10 08:10:48,401] Trial 5626 pruned. Pruned at step 10 with metric 0.6297

================================================================================
TRIAL 5628 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.026800504079536e-05
  Dropout: 0.3780455763223772
================================================================================

[I 2025-11-10 08:14:08,881] Trial 5628 pruned. Pruned at step 6 with metric 0.5864
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5629 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.536586415507294e-05
  Dropout: 0.15164596835687882
================================================================================

[I 2025-11-10 08:39:31,896] Trial 5627 finished with value: 0.4533333333333333 and parameters: {'seed': 30379, 'model.name': 'bert-large-uncased', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 4.980190120067842e-05, 'optim.weight_decay': 1.124358739657015e-06, 'optim.beta1': 0.821630911624595, 'optim.beta2': 0.9596312171664975, 'optim.eps': 9.433064723689912e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13487681551679098, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.014901337157788, 'model.dropout': 0.3627745841497053, 'model.attn_dropout': 0.07720430106967978, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8523954640947009, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.42879744200511716, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.621114697177799, 'loss.cls.alpha': 0.6343603332113216, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 08:39:32,598] Trial 5630 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5631 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 4.903676245728643e-05
  Dropout: 0.41139027280247614
================================================================================

[I 2025-11-10 08:43:11,883] Trial 5631 pruned. Pruned at step 7 with metric 0.5443
[I 2025-11-10 08:43:12,550] Trial 5632 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-10 08:43:13,193] Trial 5633 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-10 08:43:13,844] Trial 5634 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-10 08:43:14,505] Trial 5635 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-10 08:43:15,119] The parameter `tok.doc_stride` in Trial#5636 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5636 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0226124044835076e-05
  Dropout: 0.2569687109732141
================================================================================

[I 2025-11-10 08:47:04,191] Trial 5636 pruned. Pruned at step 7 with metric 0.6135

================================================================================
TRIAL 5637 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.579409612336311e-06
  Dropout: 0.4674650451019211
================================================================================

[I 2025-11-10 08:56:24,928] Trial 5637 finished with value: 0.7049242424242425 and parameters: {'seed': 37585, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 9.579409612336311e-06, 'optim.weight_decay': 2.4557411992998306e-05, 'optim.beta1': 0.8465683661385368, 'optim.beta2': 0.9682795559075622, 'optim.eps': 9.403673044177903e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1471367754432548, 'sched.cosine_cycles': 4, 'train.clip_grad': 0.5891180634158988, 'model.dropout': 0.4674650451019211, 'model.attn_dropout': 0.014848810606086442, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8883383079793364, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.4185837999804978, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.3434555242175144, 'loss.cls.alpha': 0.5155367130447992, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5638 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.353743398732692e-05
  Dropout: 0.12334060574297154
================================================================================

[I 2025-11-10 09:05:49,240] Trial 5638 finished with value: 0.725609756097561 and parameters: {'seed': 58459, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 2.353743398732692e-05, 'optim.weight_decay': 0.02777624277017168, 'optim.beta1': 0.8808229103227193, 'optim.beta2': 0.9570740731024053, 'optim.eps': 1.644538831780184e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1467645597976948, 'sched.poly_power': 1.0263899682525863, 'train.clip_grad': 0.8022140654974235, 'model.dropout': 0.12334060574297154, 'model.attn_dropout': 0.1500873878210922, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8614069332754978, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.22829271424433523, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8776781698244775, 'loss.cls.alpha': 0.5413908481049371, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 5639 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 6.559502919619139e-05
  Dropout: 0.3599984763504274
================================================================================

[I 2025-11-10 09:17:38,687] Trial 5639 finished with value: 0.4368131868131868 and parameters: {'seed': 8766, 'model.name': 'xlm-roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 6.559502919619139e-05, 'optim.weight_decay': 0.08910058413742201, 'optim.beta1': 0.8431821909741998, 'optim.beta2': 0.9577964292412855, 'optim.eps': 4.060958952090848e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.013644081336425019, 'sched.poly_power': 1.2160556138941792, 'train.clip_grad': 1.1918698525995695, 'model.dropout': 0.3599984763504274, 'model.attn_dropout': 0.1792811253373009, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9762613410077364, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.18820145280665065, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.661020383240346, 'loss.cls.alpha': 0.5398433323848049, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5640 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.541765204813805e-05
  Dropout: 0.06464679381535157
================================================================================

[I 2025-11-10 09:24:02,747] Trial 5640 pruned. Pruned at step 13 with metric 0.6343
[I 2025-11-10 09:24:03,422] Trial 5641 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-10 09:24:04,078] Trial 5642 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5643 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.060572566980847e-06
  Dropout: 0.06298050929184021
================================================================================

[I 2025-11-10 09:27:20,334] Trial 5643 pruned. Pruned at step 9 with metric 0.5676
[W 2025-11-10 09:27:20,974] The parameter `tok.doc_stride` in Trial#5644 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 09:27:21,036] Trial 5644 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5645 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 3.187291116635358e-05
  Dropout: 0.26097572849747525
================================================================================

[I 2025-11-10 09:41:36,612] Trial 5645 finished with value: 0.705701394585726 and parameters: {'seed': 42971, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.187291116635358e-05, 'optim.weight_decay': 4.983441429748072e-05, 'optim.beta1': 0.8642581899622245, 'optim.beta2': 0.9702172440603513, 'optim.eps': 1.7153955213830097e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.032048358701218915, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.8059111192219784, 'model.dropout': 0.26097572849747525, 'model.attn_dropout': 0.27137478262936415, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.9000992904006233, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.361307537305933, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.19900577981480086, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5646 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 5.642914752718803e-06
  Dropout: 0.44686601868232206
================================================================================

[I 2025-11-10 09:42:23,567] Trial 5629 pruned. Pruned at step 27 with metric 0.5696
[I 2025-11-10 09:42:24,345] Trial 5647 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5648 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.982531922299247e-05
  Dropout: 0.10679074269874195
================================================================================

[I 2025-11-10 09:49:18,847] Trial 5648 pruned. Pruned at step 10 with metric 0.5807
[I 2025-11-10 09:49:19,538] Trial 5649 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5650 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.863198591949093e-05
  Dropout: 0.12032858479168937
================================================================================

[I 2025-11-10 10:06:45,238] Trial 5650 finished with value: 0.6917293233082706 and parameters: {'seed': 47134, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.863198591949093e-05, 'optim.weight_decay': 0.0012848268830392845, 'optim.beta1': 0.9108996684729491, 'optim.beta2': 0.9772734808544482, 'optim.eps': 2.449645486377084e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.03872628579529562, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.00380282903185, 'model.dropout': 0.12032858479168937, 'model.attn_dropout': 0.07178028680037225, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8224068815079092, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.48326864107332107, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.17352576230303388, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 10:06:45,900] Trial 5651 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-10 10:06:46,547] Trial 5652 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[I 2025-11-10 10:06:47,197] Trial 5653 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5654 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.0257488140661947e-05
  Dropout: 0.427781964673677
================================================================================

[I 2025-11-10 10:14:48,573] Trial 5654 pruned. Pruned at step 27 with metric 0.6155
[I 2025-11-10 10:14:49,244] Trial 5655 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-10 10:14:49,894] Trial 5656 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5657 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.8872537659339005e-05
  Dropout: 0.18356581566695485
================================================================================

[I 2025-11-10 10:16:11,764] Trial 5646 pruned. Pruned at step 10 with metric 0.5545

================================================================================
TRIAL 5658 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 6.131495507417687e-06
  Dropout: 0.13410603672008647
================================================================================

[I 2025-11-10 10:22:57,124] Trial 5658 pruned. Pruned at step 11 with metric 0.6083
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5659 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.74133047197534e-06
  Dropout: 0.42961698376829754
================================================================================

[I 2025-11-10 10:28:04,469] Trial 5659 pruned. Pruned at step 9 with metric 0.6447

================================================================================
TRIAL 5660 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.8265918951299016e-05
  Dropout: 0.018573514560045155
================================================================================

[I 2025-11-10 10:28:27,498] Trial 5657 pruned. Pruned at step 9 with metric 0.6207
[W 2025-11-10 10:28:28,181] The parameter `tok.doc_stride` in Trial#5661 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5661 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 8.351321290238248e-06
  Dropout: 0.2860073753654148
================================================================================

[I 2025-11-10 10:33:33,084] Trial 5660 pruned. Pruned at step 11 with metric 0.6038
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5662 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.959077095084446e-05
  Dropout: 0.3874584688212488
================================================================================

[I 2025-11-10 10:36:28,147] Trial 5661 pruned. Pruned at step 29 with metric 0.6155

================================================================================
TRIAL 5663 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.256757885413692e-05
  Dropout: 0.2507610717175204
================================================================================

[I 2025-11-10 10:39:14,829] Trial 5663 pruned. OOM: bert-base-uncased bs=64 len=352

[OOM] Trial 5663 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 108.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5664 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.014491560133249e-05
  Dropout: 0.2618903728353139
================================================================================

[I 2025-11-10 10:52:32,057] Trial 5664 finished with value: 0.7092996660720003 and parameters: {'seed': 54042, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.014491560133249e-05, 'optim.weight_decay': 0.00031718387097835174, 'optim.beta1': 0.8793350233476591, 'optim.beta2': 0.9706010939602462, 'optim.eps': 9.025335403947087e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.009731956943548208, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.3668090544524353, 'model.dropout': 0.2618903728353139, 'model.attn_dropout': 0.17221132427110786, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8178664222257193, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.35666058310291954, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1662747846331873, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-10 10:52:32,690] The parameter `tok.doc_stride` in Trial#5665 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5665 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.8501577023366352e-05
  Dropout: 0.3844838302355944
================================================================================

[I 2025-11-10 10:58:01,713] Trial 5665 pruned. Pruned at step 12 with metric 0.6165
[W 2025-11-10 10:58:02,347] The parameter `tok.doc_stride` in Trial#5666 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 10:58:02,408] Trial 5666 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5667 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 8.240063335219702e-05
  Dropout: 0.280371519312386
================================================================================

[I 2025-11-10 11:23:26,837] Trial 5662 pruned. Pruned at step 27 with metric 0.6115
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5668 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 7.725002851508177e-06
  Dropout: 0.3337649146564888
================================================================================

[I 2025-11-10 11:33:31,657] Trial 5667 finished with value: 0.4533333333333333 and parameters: {'seed': 53789, 'model.name': 'bert-large-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.240063335219702e-05, 'optim.weight_decay': 1.7643776078688385e-05, 'optim.beta1': 0.8440766068870326, 'optim.beta2': 0.9982976062157864, 'optim.eps': 1.570329002872424e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13336280607225326, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9540442622869473, 'model.dropout': 0.280371519312386, 'model.attn_dropout': 0.04486620922593873, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9338704717993924, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.3243352760476985, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.998072295380132, 'loss.cls.alpha': 0.789581878682734, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 11:33:32,345] Trial 5669 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-10 11:33:32,963] The parameter `tok.doc_stride` in Trial#5670 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 11:33:33,020] Trial 5670 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5671 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.0048054878150377e-05
  Dropout: 0.03545896754681816
================================================================================

[I 2025-11-10 11:36:50,178] Trial 5671 pruned. Pruned at step 11 with metric 0.6095
[I 2025-11-10 11:36:50,851] Trial 5672 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5673 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.9426506566760198e-05
  Dropout: 0.39035974519954253
================================================================================

[I 2025-11-10 11:37:13,201] Trial 5668 finished with value: 0.6899159663865546 and parameters: {'seed': 50662, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 7.725002851508177e-06, 'optim.weight_decay': 0.04445762912975225, 'optim.beta1': 0.9065718791688075, 'optim.beta2': 0.9589157336676087, 'optim.eps': 1.2545905149936378e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.004033229604644112, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.0063365694042663, 'model.dropout': 0.3337649146564888, 'model.attn_dropout': 0.25163621485224585, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8667195442159245, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.481830154065054, 'loss.cls.type': 'focal', 'loss.cls.gamma': 2.303048431520888, 'loss.cls.alpha': 0.8825200547474937, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 5674 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 5.816116924936696e-06
  Dropout: 0.3601673605980382
================================================================================

[I 2025-11-10 11:44:49,854] Trial 5674 pruned. Pruned at step 11 with metric 0.5807
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5675 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.336831539544971e-05
  Dropout: 0.4734392728738351
================================================================================

[I 2025-11-10 11:54:51,292] Trial 5675 finished with value: 0.43370165745856354 and parameters: {'seed': 65479, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 9.336831539544971e-05, 'optim.weight_decay': 0.02583453565620017, 'optim.beta1': 0.9242228792198793, 'optim.beta2': 0.9702653100248847, 'optim.eps': 5.059372111641593e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.044225011910782513, 'sched.poly_power': 1.325458741410884, 'train.clip_grad': 0.7648311983777184, 'model.dropout': 0.4734392728738351, 'model.attn_dropout': 0.2422480098316585, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8039375446882222, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.3906635454471851, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18252374858170325, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5676 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 5.8544675760658565e-06
  Dropout: 0.4316531904664359
================================================================================

[I 2025-11-10 11:55:39,897] Trial 5673 pruned. Pruned at step 31 with metric 0.5485

================================================================================
TRIAL 5677 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2785923644406873e-05
  Dropout: 0.12068797996484505
================================================================================

[I 2025-11-10 12:09:09,681] Trial 5677 pruned. Pruned at step 12 with metric 0.5272

================================================================================
TRIAL 5678 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.2156294053664067e-05
  Dropout: 0.03710160287505484
================================================================================

[I 2025-11-10 12:51:10,982] Trial 5678 finished with value: 0.6776729559748428 and parameters: {'seed': 40767, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.2156294053664067e-05, 'optim.weight_decay': 0.00011906840458626189, 'optim.beta1': 0.8645394582147421, 'optim.beta2': 0.9980778442728956, 'optim.eps': 5.427685342123008e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.14580223007811674, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.42440577011214164, 'model.dropout': 0.03710160287505484, 'model.attn_dropout': 0.17769044279484764, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8951438882564413, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.27810275937086965, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1991133117437009, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 43 (patience=20)

================================================================================
TRIAL 5679 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 4.902232149567557e-05
  Dropout: 0.49596716817745257
================================================================================

[I 2025-11-10 12:54:06,202] Trial 5679 pruned. Pruned at step 6 with metric 0.6095

================================================================================
TRIAL 5680 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 3.052660273917541e-05
  Dropout: 0.40621569839693283
================================================================================

[I 2025-11-10 13:04:20,618] Trial 5676 finished with value: 0.7254298642533936 and parameters: {'seed': 46813, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.8544675760658565e-06, 'optim.weight_decay': 8.714746453701525e-06, 'optim.beta1': 0.8363870262189257, 'optim.beta2': 0.9739246205511827, 'optim.eps': 1.0104763188374869e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.0768670756063359, 'train.clip_grad': 0.5498343954917717, 'model.dropout': 0.4316531904664359, 'model.attn_dropout': 0.034329419405673994, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9532367964902126, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.209181526196963, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.15553456383085096, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 13:04:21,305] Trial 5681 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 41 (patience=20)

================================================================================
TRIAL 5682 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 2.0581329441118338e-05
  Dropout: 0.1903428926269577
================================================================================

[I 2025-11-10 13:04:29,851] Trial 5682 pruned. OOM: microsoft/deberta-v3-large bs=16 len=192
[W 2025-11-10 13:04:30,607] The parameter `tok.doc_stride` in Trial#5683 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 13:04:32,145] Trial 5680 pruned. OOM: bert-large-uncased bs=12 len=128

[OOM] Trial 5682 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 16 with grad_accum=1)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 72.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5683 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0842221240682345e-05
  Dropout: 0.21830685845642048
================================================================================


[OOM] Trial 5680 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 52.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

[I 2025-11-10 13:04:33,296] Trial 5684 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5685 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 7.150854423301202e-06
  Dropout: 0.2425039846423182
================================================================================

[I 2025-11-10 13:08:21,792] Trial 5685 pruned. Pruned at step 12 with metric 0.5923
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5686 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.2010583329098497e-05
  Dropout: 0.0342757425681256
================================================================================

[I 2025-11-10 13:09:01,989] Trial 5683 pruned. Pruned at step 14 with metric 0.6527
[I 2025-11-10 13:09:02,697] Trial 5687 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-10 13:09:03,361] Trial 5688 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5689 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.269718302830702e-05
  Dropout: 0.2517596233100566
================================================================================

[I 2025-11-10 13:13:07,546] Trial 5689 pruned. Pruned at step 9 with metric 0.6384
[I 2025-11-10 13:13:08,254] Trial 5690 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5691 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.652413945929745e-05
  Dropout: 0.35919103061246516
================================================================================

[I 2025-11-10 13:23:22,555] Trial 5691 pruned. Pruned at step 27 with metric 0.6472
[I 2025-11-10 13:23:23,258] Trial 5692 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-10 13:23:23,880] The parameter `tok.doc_stride` in Trial#5693 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5693 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0172613338718806e-05
  Dropout: 0.05743776694227614
================================================================================

[I 2025-11-10 13:33:14,532] Trial 5686 pruned. Pruned at step 27 with metric 0.5521
[W 2025-11-10 13:33:15,215] The parameter `tok.doc_stride` in Trial#5694 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5694 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.6114017650205745e-06
  Dropout: 0.40370682476176395
================================================================================

[I 2025-11-10 13:34:40,926] Trial 5693 finished with value: 0.6298611111111111 and parameters: {'seed': 49056, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.0172613338718806e-05, 'optim.weight_decay': 0.07679382030887523, 'optim.beta1': 0.8279137452756842, 'optim.beta2': 0.9685239938623126, 'optim.eps': 6.917073205264843e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18242345115833594, 'train.clip_grad': 0.46090226557635505, 'model.dropout': 0.05743776694227614, 'model.attn_dropout': 0.26087509400076203, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8386276090549063, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.10149535769523091, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.4322019143117624, 'loss.cls.alpha': 0.18155444338929794, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 5695 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 6.587124425156152e-05
  Dropout: 0.0025588321813811293
================================================================================

[I 2025-11-10 13:37:00,899] Trial 5695 pruned. Pruned at step 11 with metric 0.5884
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5696 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.9728661087494302e-05
  Dropout: 0.32258014787166495
================================================================================

[I 2025-11-10 13:41:14,603] Trial 5696 pruned. Pruned at step 11 with metric 0.5878
[W 2025-11-10 13:41:15,241] The parameter `tok.doc_stride` in Trial#5697 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5697 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 6.547228881205884e-05
  Dropout: 0.14218375139020525
================================================================================

[I 2025-11-10 13:42:51,796] Trial 5697 pruned. Pruned at step 6 with metric 0.6027

================================================================================
TRIAL 5698 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 4.561677723383878e-05
  Dropout: 0.30342640985955044
================================================================================

[I 2025-11-10 13:50:34,471] Trial 5698 pruned. Pruned at step 20 with metric 0.6038
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5699 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 3.730714664199324e-05
  Dropout: 0.0052744229624214455
================================================================================

[I 2025-11-10 13:55:48,189] Trial 5699 pruned. Pruned at step 7 with metric 0.5721
[I 2025-11-10 13:55:48,926] Trial 5700 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5701 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.587522697992701e-06
  Dropout: 0.4550644573041762
================================================================================

[I 2025-11-10 14:19:01,963] Trial 5694 finished with value: 0.7082634957781222 and parameters: {'seed': 27703, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 7.6114017650205745e-06, 'optim.weight_decay': 0.09170205313776145, 'optim.beta1': 0.8784295266492043, 'optim.beta2': 0.9750511248392217, 'optim.eps': 9.796414642314456e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.07925462516713512, 'train.clip_grad': 1.3387917924547015, 'model.dropout': 0.40370682476176395, 'model.attn_dropout': 0.2672969926537671, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8779427350896787, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.28307812909082525, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.998513715910687, 'loss.cls.alpha': 0.7290563150884728, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 33 (patience=20)

================================================================================
TRIAL 5702 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 3.33290537948722e-05
  Dropout: 0.4632579799994119
================================================================================

[I 2025-11-10 15:06:51,614] Trial 5701 finished with value: 0.7204277921332185 and parameters: {'seed': 59644, 'model.name': 'roberta-large', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.587522697992701e-06, 'optim.weight_decay': 0.00033490058260200907, 'optim.beta1': 0.884597409418719, 'optim.beta2': 0.9598427881985585, 'optim.eps': 9.53252199748148e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.030542080155981068, 'sched.poly_power': 0.6124788284781099, 'train.clip_grad': 1.387605938042633, 'model.dropout': 0.4550644573041762, 'model.attn_dropout': 0.16989130998487115, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9485399785373971, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.3798028195782166, 'loss.cls.type': 'focal', 'loss.cls.gamma': 2.9300186884366113, 'loss.cls.alpha': 0.4058305303887585, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 47 (patience=20)

================================================================================
TRIAL 5703 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.3894476211383396e-05
  Dropout: 0.3597960889688072
================================================================================

[I 2025-11-10 15:17:06,762] Trial 5702 pruned. Pruned at step 18 with metric 0.5839

================================================================================
TRIAL 5704 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 3.103052209594466e-05
  Dropout: 0.3818947553446519
================================================================================

[I 2025-11-10 15:30:21,425] Trial 5703 pruned. Pruned at step 13 with metric 0.6134
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5705 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.088569623223005e-05
  Dropout: 0.03378274387261276
================================================================================

[I 2025-11-10 15:34:09,289] Trial 5705 pruned. Pruned at step 7 with metric 0.5439

================================================================================
TRIAL 5706 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.852485165364661e-05
  Dropout: 0.046074973015547555
================================================================================

[I 2025-11-10 15:45:43,703] Trial 5706 finished with value: 0.6737471174770211 and parameters: {'seed': 59412, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.852485165364661e-05, 'optim.weight_decay': 5.466074191311408e-06, 'optim.beta1': 0.8899412356775128, 'optim.beta2': 0.9737901560292469, 'optim.eps': 7.230050470676548e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.010162249067571795, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1440703128113316, 'model.dropout': 0.046074973015547555, 'model.attn_dropout': 0.2474933750875843, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8274951083064035, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.4365991187778687, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1825835142201766, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 15:45:44,386] Trial 5707 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-10 15:45:45,054] Trial 5708 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 5709 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.1354222554515025e-05
  Dropout: 0.21959337877003063
================================================================================

[I 2025-11-10 15:47:47,996] Trial 5704 finished with value: 0.450402144772118 and parameters: {'seed': 30311, 'model.name': 'xlm-roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 3.103052209594466e-05, 'optim.weight_decay': 0.00132762097482677, 'optim.beta1': 0.8660360600096462, 'optim.beta2': 0.9623569980763632, 'optim.eps': 1.1383045608750418e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.05804648332852124, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4555099871633643, 'model.dropout': 0.3818947553446519, 'model.attn_dropout': 0.2621229309705774, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8990009353367129, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.3988002410862622, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.699322484459591, 'loss.cls.alpha': 0.5165314063393698, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5710 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.7737349800883124e-05
  Dropout: 0.25118137482901987
================================================================================

[I 2025-11-10 15:52:21,916] Trial 5709 pruned. Pruned at step 14 with metric 0.5557
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5711 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.8899390443054938e-05
  Dropout: 0.02473298055953259
================================================================================

[I 2025-11-10 15:55:51,585] Trial 5711 pruned. Pruned at step 9 with metric 0.6256
[W 2025-11-10 15:55:52,220] The parameter `tok.doc_stride` in Trial#5712 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 15:55:52,280] Trial 5712 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5713 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.03895179514606e-05
  Dropout: 0.07545452680092399
================================================================================

[I 2025-11-10 15:59:06,304] Trial 5713 pruned. Pruned at step 12 with metric 0.6205

================================================================================
TRIAL 5714 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.9570446430630138e-05
  Dropout: 0.250915852911686
================================================================================

[I 2025-11-10 16:04:27,460] Trial 5710 pruned. Pruned at step 12 with metric 0.4593
[I 2025-11-10 16:04:28,161] Trial 5715 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5716 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 8.610675037778314e-06
  Dropout: 0.04126043376024444
================================================================================

[I 2025-11-10 16:07:05,645] Trial 5714 pruned. Pruned at step 8 with metric 0.5450

================================================================================
TRIAL 5717 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.3771129065210047e-05
  Dropout: 0.3505039112099393
================================================================================

[I 2025-11-10 16:21:32,981] Trial 5716 pruned. Pruned at step 9 with metric 0.6002

================================================================================
TRIAL 5718 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.158716560494778e-05
  Dropout: 0.42218101725673907
================================================================================

[I 2025-11-10 16:36:35,952] Trial 5717 pruned. Pruned at step 15 with metric 0.5548
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5719 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.1412211380979778e-05
  Dropout: 0.39222150362189867
================================================================================

[I 2025-11-10 16:50:44,839] Trial 5718 finished with value: 0.44141689373297005 and parameters: {'seed': 21322, 'model.name': 'xlm-roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 7.158716560494778e-05, 'optim.weight_decay': 4.3501507975650085e-05, 'optim.beta1': 0.8233098361701572, 'optim.beta2': 0.9865161552292423, 'optim.eps': 7.898938695753705e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.18241762450823729, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.1544970388992308, 'model.dropout': 0.42218101725673907, 'model.attn_dropout': 0.0499168041099739, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.908725531619837, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.46254049587509216, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.19969179089399156, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-10 16:50:45,496] The parameter `tok.doc_stride` in Trial#5720 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 16:50:45,553] Trial 5720 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5721 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.7495611386286653e-05
  Dropout: 0.3182350656219079
================================================================================

[I 2025-11-10 17:05:21,306] Trial 5719 finished with value: 0.608479755538579 and parameters: {'seed': 64628, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 320, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.1412211380979778e-05, 'optim.weight_decay': 2.512579985614318e-05, 'optim.beta1': 0.9082617270638966, 'optim.beta2': 0.9668501550192076, 'optim.eps': 1.3430515614584293e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.03409262001735674, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8086000512280418, 'model.dropout': 0.39222150362189867, 'model.attn_dropout': 0.21258791377576294, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8036358985293831, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3874100117867893, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1671843730038543, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 5722 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 0.00012067826404271083
  Dropout: 0.12711862351371653
================================================================================

[I 2025-11-10 17:07:00,701] Trial 5722 pruned. Pruned at step 9 with metric 0.5521
[W 2025-11-10 17:07:01,345] The parameter `tok.doc_stride` in Trial#5723 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 17:07:01,407] Trial 5723 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-10 17:07:02,050] Trial 5724 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-10 17:07:02,695] Trial 5725 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-10 17:07:03,347] Trial 5726 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5727 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.3318704612569695e-06
  Dropout: 0.18648052010083108
================================================================================

[I 2025-11-10 17:12:53,086] Trial 5727 pruned. Pruned at step 26 with metric 0.6537
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5728 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.0844786757290064e-05
  Dropout: 0.402154293373699
================================================================================

[I 2025-11-10 17:25:44,007] Trial 5728 finished with value: 0.6660633484162897 and parameters: {'seed': 61098, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.0844786757290064e-05, 'optim.weight_decay': 7.878168171248665e-06, 'optim.beta1': 0.8741859634580406, 'optim.beta2': 0.977147589388604, 'optim.eps': 3.9779770834250934e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.19525452089215403, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.0633117684907263, 'model.dropout': 0.402154293373699, 'model.attn_dropout': 0.00018305812356471385, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.910838271576321, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.45376589253395255, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.2930222110628553, 'loss.cls.alpha': 0.49976171697031196, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 17:25:44,679] Trial 5729 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-10 17:25:45,327] The parameter `tok.doc_stride` in Trial#5730 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5730 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.8424669386787696e-05
  Dropout: 0.3136373221879535
================================================================================

[I 2025-11-10 17:32:46,069] Trial 5730 pruned. Pruned at step 27 with metric 0.6176

================================================================================
TRIAL 5731 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 2.2639349069155838e-05
  Dropout: 0.0023368824251964477
================================================================================

[I 2025-11-10 17:40:32,505] Trial 5731 pruned. Pruned at step 27 with metric 0.5807
[I 2025-11-10 17:40:33,180] Trial 5732 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-10 17:40:33,793] The parameter `tok.doc_stride` in Trial#5733 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5733 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 4.785533245029364e-05
  Dropout: 0.21763105975504826
================================================================================

[I 2025-11-10 17:41:49,798] Trial 5721 finished with value: 0.43526170798898073 and parameters: {'seed': 29692, 'model.name': 'bert-large-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.7495611386286653e-05, 'optim.weight_decay': 0.003657250152754421, 'optim.beta1': 0.8353861507604006, 'optim.beta2': 0.9597829255096443, 'optim.eps': 5.492124873811035e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.03249223244243648, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.9134635393648562, 'model.dropout': 0.3182350656219079, 'model.attn_dropout': 0.19364508936725258, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.984003051482902, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.30849438161874165, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.091647312533607, 'loss.cls.alpha': 0.5350448192386538, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5734 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.4017837292930805e-05
  Dropout: 0.10573777075550353
================================================================================

[I 2025-11-10 17:46:53,898] Trial 5734 pruned. Pruned at step 13 with metric 0.6538
[W 2025-11-10 17:46:54,542] The parameter `tok.doc_stride` in Trial#5735 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5735 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.692164983353617e-06
  Dropout: 0.4070842825485968
================================================================================

[I 2025-11-10 17:49:34,768] Trial 5733 pruned. Pruned at step 9 with metric 0.5788
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5736 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.9165945498798162e-05
  Dropout: 0.24521666321989244
================================================================================

[I 2025-11-10 17:53:49,100] Trial 5735 pruned. Pruned at step 17 with metric 0.6125

================================================================================
TRIAL 5737 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.8068925316341938e-05
  Dropout: 0.009271358959588374
================================================================================

[I 2025-11-10 18:09:12,118] Trial 5737 pruned. Pruned at step 13 with metric 0.5283
[I 2025-11-10 18:09:12,799] Trial 5738 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5739 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0205136064214935e-05
  Dropout: 0.4989053443656321
================================================================================

[I 2025-11-10 18:21:43,388] Trial 5739 pruned. Pruned at step 9 with metric 0.5556
[I 2025-11-10 18:21:44,094] Trial 5740 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5741 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.331959220186114e-06
  Dropout: 0.32471923312018447
================================================================================

[I 2025-11-10 18:24:32,711] Trial 5736 finished with value: 0.6957466063348416 and parameters: {'seed': 60203, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.9165945498798162e-05, 'optim.weight_decay': 8.211455966564969e-06, 'optim.beta1': 0.8596708833213142, 'optim.beta2': 0.9725340952619711, 'optim.eps': 5.004602401049144e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.16790419844798452, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.19498393391948005, 'model.dropout': 0.24521666321989244, 'model.attn_dropout': 0.14636966181761402, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.914885661692255, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3956037916943501, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.992291025346189, 'loss.cls.alpha': 0.3000630106386473, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 56 (patience=20)

================================================================================
TRIAL 5742 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.519731027400661e-05
  Dropout: 0.39908426961787413
================================================================================

[I 2025-11-10 18:38:22,723] Trial 5742 finished with value: 0.672689075630252 and parameters: {'seed': 52348, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.519731027400661e-05, 'optim.weight_decay': 4.7360548007536906e-05, 'optim.beta1': 0.8187260151440369, 'optim.beta2': 0.9669167459025729, 'optim.eps': 2.8832413214850747e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.13993113024005738, 'train.clip_grad': 0.6596253979931459, 'model.dropout': 0.39908426961787413, 'model.attn_dropout': 0.09247247498739798, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9550540706237627, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.29682739516899564, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18219921658886223, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 18:38:23,388] Trial 5743 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-10 18:38:24,044] Trial 5744 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[I 2025-11-10 18:38:24,703] Trial 5745 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-10 18:38:25,325] The parameter `tok.doc_stride` in Trial#5746 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 48 (patience=20)

================================================================================
TRIAL 5746 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 8.643265235552348e-06
  Dropout: 0.361989492470352
================================================================================

[I 2025-11-10 18:44:54,922] Trial 5746 pruned. Pruned at step 11 with metric 0.6189

================================================================================
TRIAL 5747 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 7.68205094378369e-06
  Dropout: 0.33178837795346017
================================================================================

[I 2025-11-10 18:49:33,346] Trial 5747 pruned. Pruned at step 9 with metric 0.5603
[I 2025-11-10 18:49:34,045] Trial 5748 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 18:49:34,699] Trial 5749 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-10 18:49:35,357] Trial 5750 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5751 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 8.869696107202836e-06
  Dropout: 0.22800192801140545
================================================================================

[I 2025-11-10 18:56:02,518] Trial 5751 pruned. Pruned at step 12 with metric 0.6540
[W 2025-11-10 18:56:03,166] The parameter `tok.doc_stride` in Trial#5752 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5752 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.9913060344203375e-06
  Dropout: 0.19526187822558122
================================================================================

[I 2025-11-10 18:58:54,908] Trial 5752 pruned. Pruned at step 10 with metric 0.5949
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5753 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.760494374650788e-05
  Dropout: 0.3551418399707476
================================================================================

[I 2025-11-10 19:02:51,382] Trial 5753 pruned. Pruned at step 16 with metric 0.5884
[W 2025-11-10 19:02:52,030] The parameter `tok.doc_stride` in Trial#5754 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5754 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.1230575801142527e-05
  Dropout: 0.06191741521575282
================================================================================

[I 2025-11-10 19:04:58,235] Trial 5754 pruned. Pruned at step 10 with metric 0.5059
[W 2025-11-10 19:04:58,889] The parameter `tok.doc_stride` in Trial#5755 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5755 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 8.726789161265238e-06
  Dropout: 0.44739037853887603
================================================================================

[I 2025-11-10 19:09:22,955] Trial 5755 pruned. Pruned at step 16 with metric 0.5770

================================================================================
TRIAL 5756 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 4.2109196600459856e-05
  Dropout: 0.26901987163135616
================================================================================

[I 2025-11-10 19:13:59,226] Trial 5741 pruned. Pruned at step 27 with metric 0.6328

================================================================================
TRIAL 5757 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.7926888948796504e-05
  Dropout: 0.21399438939208448
================================================================================

[I 2025-11-10 19:21:03,234] Trial 5757 pruned. Pruned at step 11 with metric 0.6207

================================================================================
TRIAL 5758 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.8976334171631727e-05
  Dropout: 0.07770299667545115
================================================================================

[I 2025-11-10 19:26:40,215] Trial 5758 pruned. Pruned at step 8 with metric 0.5763
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5759 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.6359119345380725e-05
  Dropout: 0.4678921897982822
================================================================================

[I 2025-11-10 19:29:12,756] Trial 5756 finished with value: 0.43989071038251365 and parameters: {'seed': 60832, 'model.name': 'xlm-roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 4.2109196600459856e-05, 'optim.weight_decay': 0.0018965220059907082, 'optim.beta1': 0.8457343585011464, 'optim.beta2': 0.9750271873062035, 'optim.eps': 1.767224634288115e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.18232255200418748, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.7270428691571433, 'model.dropout': 0.26901987163135616, 'model.attn_dropout': 0.1316942384518282, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.91229430382903, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.3917160841156587, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.586278730291445, 'loss.cls.alpha': 0.3777074042602834, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5760 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 9.621585079273912e-06
  Dropout: 0.3537488472212633
================================================================================

[I 2025-11-10 19:38:20,129] Trial 5759 finished with value: 0.7359848484848485 and parameters: {'seed': 45090, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.6359119345380725e-05, 'optim.weight_decay': 1.33017018095531e-05, 'optim.beta1': 0.8683753831080679, 'optim.beta2': 0.9928480946271863, 'optim.eps': 2.1244818435740004e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.14762540383504205, 'sched.poly_power': 0.5017621952248333, 'train.clip_grad': 0.5661682763038836, 'model.dropout': 0.4678921897982822, 'model.attn_dropout': 0.07437376098334277, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9005550597324191, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.4698298527696629, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.972875463022961, 'loss.cls.alpha': 0.5169489572006297, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 19:38:20,815] Trial 5761 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 5762 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 3.3985120426200574e-05
  Dropout: 0.12102927566825039
================================================================================

[I 2025-11-10 19:46:09,592] Trial 5762 pruned. Pruned at step 9 with metric 0.6537

================================================================================
TRIAL 5763 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 9.86554875752097e-06
  Dropout: 0.30553020579105983
================================================================================

[I 2025-11-10 19:54:33,494] Trial 5763 pruned. Pruned at step 27 with metric 0.6382
[I 2025-11-10 19:54:34,161] Trial 5764 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5765 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 4.249944419079814e-05
  Dropout: 0.15352877415133764
================================================================================

[I 2025-11-10 20:03:58,730] Trial 5765 pruned. Pruned at step 27 with metric 0.6338
[I 2025-11-10 20:03:59,402] Trial 5766 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-10 20:04:00,063] Trial 5767 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5768 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 8.13226697859128e-06
  Dropout: 0.20716490872218066
================================================================================

[I 2025-11-10 20:04:26,883] Trial 5760 pruned. Pruned at step 15 with metric 0.6384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5769 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.7595385443181556e-05
  Dropout: 0.3483049089802459
================================================================================

[I 2025-11-10 20:08:34,920] Trial 5768 pruned. Pruned at step 8 with metric 0.5905
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5770 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.5965692665960965e-05
  Dropout: 0.1842936448558663
================================================================================

[I 2025-11-10 20:19:39,911] Trial 5769 finished with value: 0.6868055555555556 and parameters: {'seed': 41808, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.7595385443181556e-05, 'optim.weight_decay': 0.1321073650608466, 'optim.beta1': 0.8743992951886636, 'optim.beta2': 0.9722891429609679, 'optim.eps': 2.0261617558119813e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.016819782813542926, 'train.clip_grad': 1.2363726399107928, 'model.dropout': 0.3483049089802459, 'model.attn_dropout': 0.22700268352839065, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8709296431270204, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.34214174783718876, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.73352066140676, 'loss.cls.alpha': 0.857314520056329, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5771 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.094512811791525e-06
  Dropout: 0.34301440651841386
================================================================================

[I 2025-11-10 20:24:03,012] Trial 5771 pruned. Pruned at step 27 with metric 0.5547

================================================================================
TRIAL 5772 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.0062435656124232e-05
  Dropout: 0.23942905751387816
================================================================================

[I 2025-11-10 20:28:37,829] Trial 5772 pruned. Pruned at step 8 with metric 0.5593
[W 2025-11-10 20:28:38,478] The parameter `tok.doc_stride` in Trial#5773 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 20:28:38,535] Trial 5773 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-10 20:28:39,159] The parameter `tok.doc_stride` in Trial#5774 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5774 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.0224735824034283e-05
  Dropout: 0.07806974925066462
================================================================================

[I 2025-11-10 20:41:15,698] Trial 5770 pruned. Pruned at step 12 with metric 0.6210
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5775 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 9.727471131516556e-06
  Dropout: 0.26877470267770937
================================================================================

[I 2025-11-10 20:41:23,881] Trial 5775 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384
[I 2025-11-10 20:41:24,721] Trial 5776 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-10 20:41:25,327] The parameter `tok.doc_stride` in Trial#5777 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-10 20:41:25,386] Trial 5777 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-10 20:41:26,005] The parameter `tok.doc_stride` in Trial#5778 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 5775 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5778 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 9.471178916154884e-06
  Dropout: 0.054596164319792806
================================================================================

[I 2025-11-10 20:45:18,465] Trial 5778 pruned. Pruned at step 10 with metric 0.5593
[W 2025-11-10 20:45:19,120] The parameter `tok.doc_stride` in Trial#5779 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5779 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 8.122381263639762e-06
  Dropout: 0.35896611100067677
================================================================================

[I 2025-11-10 20:50:27,421] Trial 5779 pruned. Pruned at step 15 with metric 0.5603
[I 2025-11-10 20:50:28,107] Trial 5780 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5781 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 2.7004916884111134e-05
  Dropout: 0.4061759500111138
================================================================================

[I 2025-11-10 20:55:09,563] Trial 5774 finished with value: 0.4444444444444444 and parameters: {'seed': 26098, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.0224735824034283e-05, 'optim.weight_decay': 0.0005493097609907421, 'optim.beta1': 0.8986842462928749, 'optim.beta2': 0.9969926189414635, 'optim.eps': 5.401728550308051e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.04343971036283344, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9424182237748677, 'model.dropout': 0.07806974925066462, 'model.attn_dropout': 0.12463133024807423, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8076360208999569, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.4495292820804127, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.05725991871006386, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 20:55:10,268] Trial 5782 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-10 20:55:10,889] The parameter `tok.doc_stride` in Trial#5783 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5783 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.40348453291185e-05
  Dropout: 0.03162517677227661
================================================================================

[I 2025-11-10 20:58:31,493] Trial 5781 finished with value: 0.6476317799847211 and parameters: {'seed': 64089, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.7004916884111134e-05, 'optim.weight_decay': 0.00012130717589134031, 'optim.beta1': 0.9213416852024925, 'optim.beta2': 0.9754474112937935, 'optim.eps': 1.2280321141923957e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.09054779388939949, 'train.clip_grad': 0.4946305883394107, 'model.dropout': 0.4061759500111138, 'model.attn_dropout': 0.04485140904984711, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8819226573601723, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.06733501739437392, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.16064824304157002, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 5784 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.201685946112014e-05
  Dropout: 0.34843043513780203
================================================================================

[I 2025-11-10 21:03:17,645] Trial 5783 pruned. Pruned at step 12 with metric 0.5579
[W 2025-11-10 21:03:18,321] The parameter `tok.doc_stride` in Trial#5785 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5785 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.916190617330599e-05
  Dropout: 0.4545398191896755
================================================================================

[I 2025-11-10 21:07:09,262] Trial 5785 pruned. Pruned at step 14 with metric 0.6428
[W 2025-11-10 21:07:09,922] The parameter `tok.doc_stride` in Trial#5786 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5786 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.2051191592235114e-05
  Dropout: 0.2693765216635224
================================================================================

[I 2025-11-10 21:09:27,965] Trial 5786 pruned. Pruned at step 8 with metric 0.5780
[I 2025-11-10 21:09:28,639] Trial 5787 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5788 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 5.773921016276553e-06
  Dropout: 0.4335180892097321
================================================================================

[I 2025-11-10 21:17:11,547] Trial 5784 pruned. Pruned at step 16 with metric 0.6297
[W 2025-11-10 21:17:12,215] The parameter `tok.doc_stride` in Trial#5789 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5789 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.6305007764321172e-05
  Dropout: 0.31651766648015484
================================================================================

[I 2025-11-10 21:19:55,532] Trial 5788 finished with value: 0.7049973835688121 and parameters: {'seed': 55115, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.773921016276553e-06, 'optim.weight_decay': 6.189113256836377e-06, 'optim.beta1': 0.8703674060218801, 'optim.beta2': 0.9840566271480572, 'optim.eps': 3.0710892555994956e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.19625047261528603, 'train.clip_grad': 1.3246555994993172, 'model.dropout': 0.4335180892097321, 'model.attn_dropout': 0.10618105133082908, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9081365773713399, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4449408481700849, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.660145352011939, 'loss.cls.alpha': 0.5764867920229559, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 5790 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.5901720962270726e-05
  Dropout: 0.05159252492354775
================================================================================

[I 2025-11-10 21:23:52,374] Trial 5790 pruned. Pruned at step 8 with metric 0.6165

================================================================================
TRIAL 5791 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.210876924406582e-06
  Dropout: 0.3507181789937832
================================================================================

[I 2025-11-10 21:26:43,053] Trial 5791 pruned. Pruned at step 9 with metric 0.5551
[I 2025-11-10 21:26:43,729] Trial 5792 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5793 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 8.218378450136133e-06
  Dropout: 0.4373367789463823
================================================================================

[I 2025-11-10 21:31:01,771] Trial 5789 pruned. Pruned at step 9 with metric 0.5945

================================================================================
TRIAL 5794 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 9.412053400180905e-06
  Dropout: 0.34312708376685896
================================================================================

[I 2025-11-10 21:37:25,094] Trial 5793 pruned. Pruned at step 27 with metric 0.6678
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5795 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.0897245898048914e-05
  Dropout: 0.3449101925512495
================================================================================

[I 2025-11-10 21:44:55,337] Trial 5795 pruned. Pruned at step 27 with metric 0.5949
[I 2025-11-10 21:44:56,028] Trial 5796 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5797 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.428448188434093e-05
  Dropout: 0.32937102793081674
================================================================================

[I 2025-11-10 21:50:28,991] Trial 5794 pruned. Pruned at step 9 with metric 0.6560
[I 2025-11-10 21:50:29,941] Trial 5798 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5799 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 2.2631696651192805e-05
  Dropout: 0.2827607207746325
================================================================================

[I 2025-11-10 22:08:52,602] Trial 5797 finished with value: 0.4444444444444444 and parameters: {'seed': 64638, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.428448188434093e-05, 'optim.weight_decay': 3.619105700173921e-05, 'optim.beta1': 0.9105780320502677, 'optim.beta2': 0.9805141414487809, 'optim.eps': 3.0213741301450424e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.19827823747551626, 'train.clip_grad': 1.287526384076506, 'model.dropout': 0.32937102793081674, 'model.attn_dropout': 0.03226085849033197, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9051398290313841, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.3779019457136325, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.0400918409730053, 'loss.cls.alpha': 0.5231851046699544, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5800 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0458175435954006e-05
  Dropout: 0.3237660633319102
================================================================================

[I 2025-11-10 22:14:15,205] Trial 5800 pruned. Pruned at step 13 with metric 0.6441
[I 2025-11-10 22:14:15,887] Trial 5801 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-10 22:14:16,545] Trial 5802 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-10 22:14:17,204] Trial 5803 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-10 22:14:17,859] Trial 5804 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 22:14:18,506] Trial 5805 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5806 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.1914505723994996e-05
  Dropout: 0.1880256450632606
================================================================================

[I 2025-11-10 22:39:05,995] Trial 5806 finished with value: 0.6702059202059202 and parameters: {'seed': 50989, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.1914505723994996e-05, 'optim.weight_decay': 3.8052147398763714e-05, 'optim.beta1': 0.8413546870223816, 'optim.beta2': 0.9647430886912801, 'optim.eps': 5.6288139516946835e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0210625512434856, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.7190566759798087, 'model.dropout': 0.1880256450632606, 'model.attn_dropout': 0.15596109546023162, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8517692477822079, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.3359572294556965, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.19996211698419017, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 22:39:06,687] Trial 5807 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 22:39:07,356] Trial 5808 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 45 (patience=20)

================================================================================
TRIAL 5809 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.30228075390005e-05
  Dropout: 0.244777603558598
================================================================================

[I 2025-11-10 22:48:57,687] Trial 5809 pruned. Pruned at step 9 with metric 0.5657
[I 2025-11-10 22:48:58,358] Trial 5810 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5811 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.1901724027386974e-05
  Dropout: 0.13163745338235447
================================================================================

[I 2025-11-10 22:51:40,974] Trial 5811 pruned. Pruned at step 8 with metric 0.6009

================================================================================
TRIAL 5812 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.5714572062556584e-05
  Dropout: 0.326108592154107
================================================================================

[I 2025-11-10 23:09:28,382] Trial 5812 finished with value: 0.6981094527363184 and parameters: {'seed': 33121, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.5714572062556584e-05, 'optim.weight_decay': 0.00011715263631307961, 'optim.beta1': 0.808281381466645, 'optim.beta2': 0.9789764695502278, 'optim.eps': 3.0592151685724064e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.049640502339530025, 'sched.poly_power': 0.7045429620856393, 'train.clip_grad': 1.4816530296592183, 'model.dropout': 0.326108592154107, 'model.attn_dropout': 0.1832204968838452, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8459050296155711, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.021236678931282726, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.710430253978148, 'loss.cls.alpha': 0.82215122465445, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 5813 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.285057929263304e-05
  Dropout: 0.013576069051971736
================================================================================

[I 2025-11-10 23:10:05,069] Trial 5799 finished with value: 0.44594594594594594 and parameters: {'seed': 62803, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.2631696651192805e-05, 'optim.weight_decay': 0.005010306501627886, 'optim.beta1': 0.82291331185536, 'optim.beta2': 0.9720529470080941, 'optim.eps': 1.5308459688259332e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.13201526252568205, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.5915152083004906, 'model.dropout': 0.2827607207746325, 'model.attn_dropout': 0.0035629073666757997, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9680324950979606, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.2226741269212488, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16859032070541438, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-10 23:10:05,767] Trial 5814 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-10 23:10:06,411] Trial 5815 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5816 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.945617961710934e-06
  Dropout: 0.374615186791451
================================================================================

[I 2025-11-10 23:24:03,912] Trial 5813 finished with value: 0.6825343388095879 and parameters: {'seed': 52278, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.285057929263304e-05, 'optim.weight_decay': 4.04864551157887e-05, 'optim.beta1': 0.8255715966273871, 'optim.beta2': 0.9642996518395837, 'optim.eps': 7.483764432671325e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.10021165598406855, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.060347713065557, 'model.dropout': 0.013576069051971736, 'model.attn_dropout': 0.1864254437973209, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9786552928051079, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.15310198703833983, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.03023396428962348, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5817 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.155485257267494e-05
  Dropout: 0.15175348071626188
================================================================================

[I 2025-11-10 23:30:55,185] Trial 5817 pruned. Pruned at step 6 with metric 0.5510
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5818 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 4.339349000130846e-05
  Dropout: 0.1377889047803727
================================================================================

[I 2025-11-10 23:52:17,316] Trial 5816 finished with value: 0.6156250000000001 and parameters: {'seed': 48638, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 6.945617961710934e-06, 'optim.weight_decay': 0.0012550782368605422, 'optim.beta1': 0.8966655444952635, 'optim.beta2': 0.992587732344314, 'optim.eps': 1.5307377069192216e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.12916500026995728, 'train.clip_grad': 0.6608248181005597, 'model.dropout': 0.374615186791451, 'model.attn_dropout': 0.14730832996326193, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9979184750682559, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.09799296712580942, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1839445597771006, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 56 (patience=20)

================================================================================
TRIAL 5819 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 0.00013322264724432731
  Dropout: 0.3341565608530927
================================================================================

[I 2025-11-11 00:09:47,333] Trial 5818 finished with value: 0.4368131868131868 and parameters: {'seed': 18296, 'model.name': 'roberta-large', 'tok.max_length': 288, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 4.339349000130846e-05, 'optim.weight_decay': 0.00020647532078184235, 'optim.beta1': 0.9277538788253274, 'optim.beta2': 0.966414715806867, 'optim.eps': 8.540012051648017e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.08436730100395595, 'train.clip_grad': 1.0458476267518049, 'model.dropout': 0.1377889047803727, 'model.attn_dropout': 0.2997414769853587, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8707908170621106, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.2832762706100621, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8351566606547025, 'loss.cls.alpha': 0.5164851822318934, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 00:09:48,032] Trial 5820 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5821 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.155320681640039e-06
  Dropout: 0.28293192635910414
================================================================================

[I 2025-11-11 00:17:26,351] Trial 5821 pruned. Pruned at step 22 with metric 0.6121
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5822 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.989854714431678e-06
  Dropout: 0.38630067834663884
================================================================================

[I 2025-11-11 00:21:50,723] Trial 5822 pruned. Pruned at step 14 with metric 0.5801
[W 2025-11-11 00:21:51,371] The parameter `tok.doc_stride` in Trial#5823 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5823 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.32392279248809e-05
  Dropout: 0.004872370590431975
================================================================================

[I 2025-11-11 00:38:57,289] Trial 5823 finished with value: 0.6781789638932496 and parameters: {'seed': 40570, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.32392279248809e-05, 'optim.weight_decay': 5.8220876660270806e-05, 'optim.beta1': 0.8730503884652794, 'optim.beta2': 0.9852139723706463, 'optim.eps': 7.673383221091086e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13078492041436796, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8122579401936609, 'model.dropout': 0.004872370590431975, 'model.attn_dropout': 0.17426284087030008, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8902674393079031, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.24014606875598435, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.05510526496073129, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 5824 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 2.677563143701414e-05
  Dropout: 0.45708484914443503
================================================================================

[I 2025-11-11 00:41:48,728] Trial 5819 finished with value: 0.44594594594594594 and parameters: {'seed': 45180, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 0.00013322264724432731, 'optim.weight_decay': 7.546145427832482e-05, 'optim.beta1': 0.8466660679585137, 'optim.beta2': 0.9978849982512235, 'optim.eps': 5.977581235454963e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.15339580988734564, 'sched.cosine_cycles': 4, 'train.clip_grad': 1.0049037409237724, 'model.dropout': 0.3341565608530927, 'model.attn_dropout': 0.16739274178790925, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.9157563186217647, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.4925637385145538, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.637331662292016, 'loss.cls.alpha': 0.49883955412471015, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 00:41:49,386] The parameter `tok.doc_stride` in Trial#5825 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5825 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.097011881288988e-05
  Dropout: 0.3709803566889955
================================================================================

[I 2025-11-11 00:44:03,017] Trial 5824 pruned. Pruned at step 11 with metric 0.5878

================================================================================
TRIAL 5826 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 2.7180200773959548e-05
  Dropout: 0.43435202763443015
================================================================================

/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[I 2025-11-11 01:03:04,345] Trial 5825 pruned. Pruned at step 23 with metric 0.6535
[W 2025-11-11 01:03:05,006] The parameter `tok.doc_stride` in Trial#5827 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5827 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.919194845424443e-05
  Dropout: 0.36365536110174246
================================================================================

[I 2025-11-11 01:17:37,953] Trial 5827 pruned. Pruned at step 15 with metric 0.6343

================================================================================
TRIAL 5828 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 6.181068441323983e-05
  Dropout: 0.0939347148377244
================================================================================

[I 2025-11-11 01:21:30,463] Trial 5826 pruned. Pruned at step 27 with metric 0.4399
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5829 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.293251824682511e-05
  Dropout: 0.1115816534221746
================================================================================

[I 2025-11-11 01:27:26,032] Trial 5829 pruned. Pruned at step 8 with metric 0.5984
[I 2025-11-11 01:27:26,726] Trial 5830 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5831 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.2928720794296591e-05
  Dropout: 0.42777301795833855
================================================================================

[I 2025-11-11 01:31:22,116] Trial 5828 finished with value: 0.4273743016759777 and parameters: {'seed': 37287, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 6.181068441323983e-05, 'optim.weight_decay': 7.588422547646091e-05, 'optim.beta1': 0.9406761511777901, 'optim.beta2': 0.9748461888192577, 'optim.eps': 1.0489223496784466e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.10084054662746332, 'sched.poly_power': 0.9557900777792344, 'train.clip_grad': 1.2946256521240382, 'model.dropout': 0.0939347148377244, 'model.attn_dropout': 0.23098742462361369, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8358123288220884, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.3694125675194934, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.9693375380568443, 'loss.cls.alpha': 0.5328077313000037, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5832 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 3.0491363637805983e-05
  Dropout: 0.26297443925262964
================================================================================

[I 2025-11-11 01:45:41,202] Trial 5832 pruned. Pruned at step 13 with metric 0.6107
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5833 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.9339729058049017e-05
  Dropout: 0.07886444127750449
================================================================================

[I 2025-11-11 01:58:00,219] Trial 5833 pruned. Pruned at step 14 with metric 0.6079
[I 2025-11-11 01:58:00,902] Trial 5834 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-11 01:58:01,525] The parameter `tok.doc_stride` in Trial#5835 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5835 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0918926619847922e-05
  Dropout: 0.43976262144552836
================================================================================

[I 2025-11-11 02:04:01,941] Trial 5835 pruned. Pruned at step 14 with metric 0.6148
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5836 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.8402114747316484e-05
  Dropout: 0.39584971674258856
================================================================================

[I 2025-11-11 02:19:59,962] Trial 5836 pruned. Pruned at step 27 with metric 0.5827
[I 2025-11-11 02:20:00,644] Trial 5837 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-11 02:20:01,261] The parameter `tok.doc_stride` in Trial#5838 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5838 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.991894330795077e-05
  Dropout: 0.0429788180966187
================================================================================

[I 2025-11-11 02:20:26,481] Trial 5831 finished with value: 0.7106154714850367 and parameters: {'seed': 65235, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.2928720794296591e-05, 'optim.weight_decay': 0.002675242031859118, 'optim.beta1': 0.9074301770479363, 'optim.beta2': 0.9895200511404418, 'optim.eps': 1.7436391425333358e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.06459200577883277, 'train.clip_grad': 0.4834571983221979, 'model.dropout': 0.42777301795833855, 'model.attn_dropout': 0.02335301697545422, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8954142253575528, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.16678462638981714, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.03203171635491527, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 02:20:27,173] Trial 5839 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-11 02:20:27,800] The parameter `tok.doc_stride` in Trial#5840 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 37 (patience=20)

================================================================================
TRIAL 5840 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.8766388881239683e-05
  Dropout: 0.2884122631503876
================================================================================

[I 2025-11-11 02:22:59,382] Trial 5840 pruned. Pruned at step 9 with metric 0.6223
[W 2025-11-11 02:23:00,077] The parameter `tok.doc_stride` in Trial#5841 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5841 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 7.18360968495818e-06
  Dropout: 0.3600566747966759
================================================================================

[I 2025-11-11 02:25:38,264] Trial 5841 pruned. Pruned at step 9 with metric 0.6200
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5842 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 7.635464873363996e-06
  Dropout: 0.2170937146349839
================================================================================

[I 2025-11-11 02:32:20,719] Trial 5838 pruned. Pruned at step 9 with metric 0.5249

================================================================================
TRIAL 5843 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.645895016565948e-05
  Dropout: 0.007805310730535142
================================================================================

[I 2025-11-11 02:32:24,146] Trial 5842 pruned. Pruned at step 13 with metric 0.6608
[W 2025-11-11 02:32:24,942] The parameter `tok.doc_stride` in Trial#5844 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5844 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1372727741012186e-05
  Dropout: 0.16127557719353433
================================================================================

[I 2025-11-11 02:33:58,708] Trial 5843 pruned. Pruned at step 6 with metric 0.6277
[I 2025-11-11 02:33:59,404] Trial 5845 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5846 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.2931573267814892e-05
  Dropout: 0.47007489445812434
================================================================================

[I 2025-11-11 02:38:09,664] Trial 5844 pruned. Pruned at step 9 with metric 0.5443
[W 2025-11-11 02:38:10,335] The parameter `tok.doc_stride` in Trial#5847 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5847 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.595436248175332e-05
  Dropout: 0.3294744538918409
================================================================================

[I 2025-11-11 02:41:58,388] Trial 5847 pruned. Pruned at step 11 with metric 0.5769
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5848 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 3.297934837490816e-05
  Dropout: 0.23829734885511683
================================================================================

[I 2025-11-11 02:42:05,093] Trial 5848 pruned. OOM: microsoft/deberta-v3-base bs=16 len=384
[I 2025-11-11 02:42:05,302] Trial 5846 pruned. OOM: roberta-base bs=64 len=256
[I 2025-11-11 02:42:06,162] Trial 5850 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-11 02:42:06,388] The parameter `tok.doc_stride` in Trial#5849 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 02:42:06,534] Trial 5849 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-11 02:42:07,321] The parameter `tok.doc_stride` in Trial#5851 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 5848 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5846 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5851 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.181527944327514e-05
  Dropout: 0.414210381000144
================================================================================


================================================================================
TRIAL 5852 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 4.756794421873428e-05
  Dropout: 0.3441054955723853
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-11 02:50:44,131] Trial 5851 finished with value: 0.6483951135846546 and parameters: {'seed': 43879, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 4.181527944327514e-05, 'optim.weight_decay': 0.0010296386312364505, 'optim.beta1': 0.8651828061496918, 'optim.beta2': 0.9860176728270694, 'optim.eps': 1.8687704685893244e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.17638409253626733, 'sched.poly_power': 1.188042770065755, 'train.clip_grad': 1.0713854348990244, 'model.dropout': 0.414210381000144, 'model.attn_dropout': 0.14599946810197462, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8658583888006978, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.4827150037586336, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.663733150883383, 'loss.cls.alpha': 0.7427482129727108, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5853 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 7.383559529247545e-06
  Dropout: 0.47718186903480514
================================================================================

[I 2025-11-11 02:55:12,329] Trial 5853 pruned. Pruned at step 17 with metric 0.5884
[I 2025-11-11 02:55:13,041] Trial 5854 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-11 02:55:13,699] Trial 5855 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-11 02:55:14,356] Trial 5856 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5857 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 9.240007668606128e-05
  Dropout: 0.10523384180756995
================================================================================

[I 2025-11-11 03:11:01,742] Trial 5852 finished with value: 0.4489247311827957 and parameters: {'seed': 55050, 'model.name': 'roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 4.756794421873428e-05, 'optim.weight_decay': 0.00039418609861348254, 'optim.beta1': 0.8317505042520541, 'optim.beta2': 0.9832410188497188, 'optim.eps': 6.049210375637209e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.15589676938492136, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.2452895439601686, 'model.dropout': 0.3441054955723853, 'model.attn_dropout': 0.08646040218543953, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8711263779206988, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.48337397963657164, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7922800013515556, 'loss.cls.alpha': 0.4409916264937941, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 450 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 5858 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.4528081686295476e-05
  Dropout: 0.4280484058407722
================================================================================

[I 2025-11-11 03:11:10,657] Trial 5858 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352

[OOM] Trial 5858 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 50.56 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
[GPU RESET] Performing periodic GPU reset after 450 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 5859 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 7.678852355284462e-06
  Dropout: 0.3631883334923165
================================================================================

[I 2025-11-11 03:22:45,466] Trial 5857 finished with value: 0.4444444444444444 and parameters: {'seed': 46871, 'model.name': 'roberta-large', 'tok.max_length': 160, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 9.240007668606128e-05, 'optim.weight_decay': 0.0001594805457393673, 'optim.beta1': 0.8662013117905748, 'optim.beta2': 0.9814993888614446, 'optim.eps': 6.741750186201196e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.15779047057236237, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.8804981318749782, 'model.dropout': 0.10523384180756995, 'model.attn_dropout': 0.19464967691419346, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8912943797007147, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.26366312175474194, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.543593944617285, 'loss.cls.alpha': 0.2551308081646089, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 03:22:46,140] The parameter `tok.doc_stride` in Trial#5860 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5860 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.910047705701092e-05
  Dropout: 0.24557348373070714
================================================================================

[I 2025-11-11 03:27:06,260] Trial 5859 pruned. Pruned at step 13 with metric 0.5149
[I 2025-11-11 03:27:07,209] Trial 5861 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5862 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.3363889519410499e-05
  Dropout: 0.30904469632839
================================================================================

[I 2025-11-11 03:33:49,965] Trial 5862 pruned. Pruned at step 14 with metric 0.6372
[W 2025-11-11 03:33:50,611] The parameter `tok.doc_stride` in Trial#5863 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5863 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 8.624438214323797e-06
  Dropout: 0.19465107763349382
================================================================================

[I 2025-11-11 03:38:03,748] Trial 5860 pruned. Pruned at step 7 with metric 0.6270
[I 2025-11-11 03:38:04,570] Trial 5864 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-11 03:38:05,197] The parameter `tok.doc_stride` in Trial#5865 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5865 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.85566651719859e-05
  Dropout: 0.4941657920097353
================================================================================

[I 2025-11-11 03:46:44,103] Trial 5863 finished with value: 0.7359674269807637 and parameters: {'seed': 26540, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.624438214323797e-06, 'optim.weight_decay': 8.14943397931142e-06, 'optim.beta1': 0.9059430529224122, 'optim.beta2': 0.9665931669131537, 'optim.eps': 2.3794177665168994e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.004020877455148542, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.041783779889237, 'model.dropout': 0.19465107763349382, 'model.attn_dropout': 0.20933092787171723, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8542415151602948, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4636737199356726, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.16311956461456836, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 5866 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 4.855159331019727e-05
  Dropout: 0.09372617105003177
================================================================================

[I 2025-11-11 03:58:36,437] Trial 5865 pruned. Pruned at step 27 with metric 0.5945
[I 2025-11-11 03:58:37,155] Trial 5867 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[W 2025-11-11 03:58:37,779] The parameter `tok.doc_stride` in Trial#5868 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5868 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.4887854781011792e-05
  Dropout: 0.4327681646339924
================================================================================

[I 2025-11-11 04:08:49,870] Trial 5868 finished with value: 0.7328320802005013 and parameters: {'seed': 38272, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.4887854781011792e-05, 'optim.weight_decay': 0.05602489345698785, 'optim.beta1': 0.8791708965745059, 'optim.beta2': 0.9846582955958256, 'optim.eps': 1.7802064744290372e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1319596677931678, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.33663665969792744, 'model.dropout': 0.4327681646339924, 'model.attn_dropout': 0.185672010172587, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8673047520420002, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.04591574686382606, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.971940027916235, 'loss.cls.alpha': 0.6378684397448543, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 04:08:50,513] The parameter `tok.doc_stride` in Trial#5869 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5869 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.672037589924966e-05
  Dropout: 0.05377259483715134
================================================================================

[I 2025-11-11 04:13:55,046] Trial 5869 pruned. Pruned at step 12 with metric 0.5957
[I 2025-11-11 04:13:55,726] Trial 5870 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5871 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.002848754312608e-05
  Dropout: 0.28266727404760417
================================================================================

[I 2025-11-11 04:45:00,207] Trial 5871 finished with value: 0.6628934184863666 and parameters: {'seed': 60951, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.002848754312608e-05, 'optim.weight_decay': 0.09239186187096245, 'optim.beta1': 0.8896673395523887, 'optim.beta2': 0.9606110799652897, 'optim.eps': 1.3922207576128726e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.04390152115475263, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4891638963914142, 'model.dropout': 0.28266727404760417, 'model.attn_dropout': 0.28300006597052363, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.829102075552261, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.4169217333725724, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.164972301413199, 'loss.cls.alpha': 0.35058726135074847, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 5872 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 9.520295790507498e-06
  Dropout: 0.03160987633684223
================================================================================

[I 2025-11-11 04:49:23,762] Trial 5866 finished with value: 0.45910290237467016 and parameters: {'seed': 58578, 'model.name': 'roberta-large', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 4.855159331019727e-05, 'optim.weight_decay': 1.5994476652362651e-06, 'optim.beta1': 0.9135318571775215, 'optim.beta2': 0.9884383707338441, 'optim.eps': 5.3817664630270936e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.039725940680540986, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.3817535431736427, 'model.dropout': 0.09372617105003177, 'model.attn_dropout': 0.1288440823822233, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8477731301530128, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.2760297881025845, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.644999804526125, 'loss.cls.alpha': 0.25358898568493315, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 04:49:24,486] Trial 5873 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5874 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 4.238039320832187e-05
  Dropout: 0.4266521634822847
================================================================================

[I 2025-11-11 04:50:08,041] Trial 5872 pruned. Pruned at step 10 with metric 0.5866
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5875 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 2.6558539221954762e-05
  Dropout: 0.3504009059719971
================================================================================

[I 2025-11-11 04:59:46,023] Trial 5875 finished with value: 0.7114707952146375 and parameters: {'seed': 46654, 'model.name': 'roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.6558539221954762e-05, 'optim.weight_decay': 5.027088052168918e-05, 'optim.beta1': 0.8855680484862607, 'optim.beta2': 0.9755954124601598, 'optim.eps': 4.735197545572215e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.09676239350241131, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.0774260135147604, 'model.dropout': 0.3504009059719971, 'model.attn_dropout': 0.10594715133408625, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9421713706328212, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4553396577338673, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.807271004317211, 'loss.cls.alpha': 0.5199830336906571, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 04:59:46,671] The parameter `tok.doc_stride` in Trial#5876 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 5876 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 1.3440515995067119e-05
  Dropout: 0.2672621704118012
================================================================================

[I 2025-11-11 05:03:32,999] Trial 5874 finished with value: 0.6628934184863666 and parameters: {'seed': 23719, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 4.238039320832187e-05, 'optim.weight_decay': 4.500797056979648e-06, 'optim.beta1': 0.8830440979947403, 'optim.beta2': 0.9775697027465724, 'optim.eps': 3.028812569635453e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1441706704879138, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.7155893452909048, 'model.dropout': 0.4266521634822847, 'model.attn_dropout': 0.04778738638973108, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.9796923590643611, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.43721123348189267, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.1813533480491438, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 5877 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.033014707341148e-05
  Dropout: 0.05043673229313053
================================================================================

[I 2025-11-11 05:03:49,296] Trial 5877 pruned. OOM: bert-base-uncased bs=48 len=192
[I 2025-11-11 05:03:49,980] Trial 5878 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)

[OOM] Trial 5877 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 96 with grad_accum=2)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 114.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5879 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.0058893844799754e-05
  Dropout: 0.13552054686445467
================================================================================

[I 2025-11-11 05:21:06,854] Trial 5879 finished with value: 0.6483951135846546 and parameters: {'seed': 54818, 'model.name': 'bert-base-uncased', 'tok.max_length': 288, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.0058893844799754e-05, 'optim.weight_decay': 0.12086695219614695, 'optim.beta1': 0.8323484464830228, 'optim.beta2': 0.9759411814372367, 'optim.eps': 2.8813484020155637e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1416165404560712, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.10828821869249894, 'model.dropout': 0.13552054686445467, 'model.attn_dropout': 0.2392321214037816, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8384493708397802, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.035292445559673896, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.99309350654721, 'loss.cls.alpha': 0.6843218634319211, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 5880 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.9416772568192292e-05
  Dropout: 0.07518059024557654
================================================================================

[I 2025-11-11 05:41:33,913] Trial 5880 pruned. Pruned at step 27 with metric 0.6205

================================================================================
TRIAL 5881 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.6146103174756527e-05
  Dropout: 0.25378466936116145
================================================================================

[I 2025-11-11 05:48:05,176] Trial 5876 finished with value: 0.7318158032443747 and parameters: {'seed': 25057, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 1.3440515995067119e-05, 'optim.weight_decay': 0.00013557376948446787, 'optim.beta1': 0.8447097129336746, 'optim.beta2': 0.9757592908174462, 'optim.eps': 1.6665872489363625e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13545644785411454, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.4290192510068416, 'model.dropout': 0.2672621704118012, 'model.attn_dropout': 0.06415066105429147, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8965286019125389, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.4850286237622051, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.278670602719269, 'loss.cls.alpha': 0.5602303438859525, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 5882 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.795639138245447e-06
  Dropout: 0.35785223242026404
================================================================================

[I 2025-11-11 05:56:11,388] Trial 5882 finished with value: 0.6848643483318791 and parameters: {'seed': 45980, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 6.795639138245447e-06, 'optim.weight_decay': 0.002065509576982989, 'optim.beta1': 0.8606888154757221, 'optim.beta2': 0.9773565458235373, 'optim.eps': 1.9016812129247395e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.19993722483276277, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.29903905538626097, 'model.dropout': 0.35785223242026404, 'model.attn_dropout': 0.13125523481505058, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9253043065559843, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.37605907316683884, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.0596135456868847, 'loss.cls.alpha': 0.4101123160965276, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 05:56:12,082] Trial 5883 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-11 05:56:12,712] The parameter `tok.doc_stride` in Trial#5884 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 51 (patience=20)

================================================================================
TRIAL 5884 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.366836528642859e-06
  Dropout: 0.15768518035472126
================================================================================

[I 2025-11-11 05:58:46,072] Trial 5884 pruned. Pruned at step 10 with metric 0.5886
[I 2025-11-11 05:58:46,752] Trial 5885 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-11 05:58:47,437] Trial 5886 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-11 05:58:48,097] Trial 5887 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-11 05:58:48,759] Trial 5888 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-11 05:58:49,443] Trial 5889 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-11 05:58:50,142] Trial 5890 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5891 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 7.951604621105189e-06
  Dropout: 0.3913158660636377
================================================================================

[I 2025-11-11 06:05:32,829] Trial 5881 pruned. Pruned at step 27 with metric 0.6121
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5892 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 7.1681591656484785e-06
  Dropout: 0.31090124846617984
================================================================================

[I 2025-11-11 06:06:20,421] Trial 5891 pruned. Pruned at step 15 with metric 0.6285
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5893 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 7.946354632394773e-06
  Dropout: 0.18771661222968247
================================================================================

[I 2025-11-11 06:10:24,250] Trial 5893 pruned. Pruned at step 14 with metric 0.5451
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5894 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.1089145542674187e-05
  Dropout: 0.2570023439107062
================================================================================

[I 2025-11-11 06:15:46,229] Trial 5892 pruned. Pruned at step 15 with metric 0.5880
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5895 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 4.4247920316953917e-05
  Dropout: 0.2904957388446071
================================================================================

[I 2025-11-11 06:16:30,548] Trial 5894 pruned. Pruned at step 12 with metric 0.5769

================================================================================
TRIAL 5896 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.6424676930247775e-05
  Dropout: 0.21339304825783928
================================================================================

[I 2025-11-11 06:22:45,320] Trial 5895 finished with value: 0.6722046722046722 and parameters: {'seed': 49300, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 4.4247920316953917e-05, 'optim.weight_decay': 0.10105191952750067, 'optim.beta1': 0.8185200105988054, 'optim.beta2': 0.9718731449525255, 'optim.eps': 9.347072362074151e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.05122000177506676, 'sched.poly_power': 0.7226182617407412, 'train.clip_grad': 1.0244814246532252, 'model.dropout': 0.2904957388446071, 'model.attn_dropout': 0.054976139379887934, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9136675740199517, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.23622303208705903, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.042787763509866, 'loss.cls.alpha': 0.5903090692362042, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5897 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.9747886229691806e-06
  Dropout: 0.41733237062913997
================================================================================

[I 2025-11-11 06:31:57,070] Trial 5896 finished with value: 0.6963381446140067 and parameters: {'seed': 29780, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.6424676930247775e-05, 'optim.weight_decay': 0.044607315026521405, 'optim.beta1': 0.818043882874559, 'optim.beta2': 0.9690329790718695, 'optim.eps': 1.1746359421871763e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.058562832016759915, 'train.clip_grad': 1.4627803160336215, 'model.dropout': 0.21339304825783928, 'model.attn_dropout': 0.22683139952378278, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8333296249146034, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.25909921346229486, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.671348593857931, 'loss.cls.alpha': 0.5219820063380474, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 5898 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 2.8254766345631933e-05
  Dropout: 0.3074843506878743
================================================================================

[I 2025-11-11 06:41:01,708] Trial 5897 finished with value: 0.7498564455928797 and parameters: {'seed': 59922, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 5.9747886229691806e-06, 'optim.weight_decay': 0.044814427209149445, 'optim.beta1': 0.9206668136296519, 'optim.beta2': 0.9927806708059398, 'optim.eps': 3.0452976883890064e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.11847354433456705, 'sched.poly_power': 0.6173008465760119, 'train.clip_grad': 0.9318322192843231, 'model.dropout': 0.41733237062913997, 'model.attn_dropout': 0.15384264462344785, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9128923333113703, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.300618289119105, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.035395470544363566, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 06:41:02,392] The parameter `tok.doc_stride` in Trial#5899 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 06:41:02,453] Trial 5899 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5900 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 8.738867286780011e-06
  Dropout: 0.4017678437689335
================================================================================

[I 2025-11-11 06:49:55,388] Trial 5900 pruned. Pruned at step 12 with metric 0.5905

================================================================================
TRIAL 5901 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.0343278878973576e-05
  Dropout: 0.3335598383511819
================================================================================

[I 2025-11-11 06:52:22,831] Trial 5898 finished with value: 0.43213296398891965 and parameters: {'seed': 30919, 'model.name': 'roberta-large', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.8254766345631933e-05, 'optim.weight_decay': 1.1120248596790768e-05, 'optim.beta1': 0.8338491982323369, 'optim.beta2': 0.970100754816864, 'optim.eps': 1.3944115040054864e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.08759390200089631, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8204254576948793, 'model.dropout': 0.3074843506878743, 'model.attn_dropout': 0.01093895877754194, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.9087808951535378, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.20274987671186656, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.18397172123283867, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 06:52:23,535] Trial 5902 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5903 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.173839200367103e-05
  Dropout: 0.3042169699955003
================================================================================

[I 2025-11-11 06:58:58,868] Trial 5903 pruned. Pruned at step 14 with metric 0.5949
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5904 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.1754161590909705e-05
  Dropout: 0.3500990798100762
================================================================================

[I 2025-11-11 07:03:48,610] Trial 5904 pruned. Pruned at step 9 with metric 0.6343

================================================================================
TRIAL 5905 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 6.1082950635824605e-06
  Dropout: 0.4163384728975608
================================================================================

[I 2025-11-11 07:16:11,139] Trial 5905 pruned. Pruned at step 20 with metric 0.6257
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5906 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.178038662768798e-06
  Dropout: 0.45557584213663516
================================================================================

[I 2025-11-11 07:19:58,548] Trial 5906 pruned. Pruned at step 10 with metric 0.6506
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5907 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.567867643567647e-06
  Dropout: 0.3320671184743159
================================================================================

[I 2025-11-11 07:33:41,240] Trial 5901 pruned. Pruned at step 15 with metric 0.6189
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5908 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.859289904966997e-05
  Dropout: 0.17770195648794726
================================================================================

[I 2025-11-11 07:44:02,922] Trial 5907 finished with value: 0.7356398069341586 and parameters: {'seed': 63012, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 5.567867643567647e-06, 'optim.weight_decay': 0.00549058594423954, 'optim.beta1': 0.9402228966390654, 'optim.beta2': 0.9965181450266867, 'optim.eps': 9.676955621193976e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.14420730708022825, 'sched.poly_power': 0.6468598088311375, 'train.clip_grad': 0.696265448351304, 'model.dropout': 0.3320671184743159, 'model.attn_dropout': 0.21302405990524487, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8304676132537079, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.2890875763662232, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.03800793002548464, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 07:44:03,607] The parameter `tok.doc_stride` in Trial#5909 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 07:44:03,670] Trial 5909 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 56 (patience=20)

================================================================================
TRIAL 5910 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.65977613696572e-06
  Dropout: 0.39357072623327527
================================================================================

[I 2025-11-11 07:55:26,526] Trial 5910 finished with value: 0.6903209166943001 and parameters: {'seed': 65036, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 7.65977613696572e-06, 'optim.weight_decay': 0.08389148585130198, 'optim.beta1': 0.9050591024990683, 'optim.beta2': 0.9905231076926447, 'optim.eps': 3.64121828112006e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.09260152614877318, 'sched.poly_power': 1.2868424413788635, 'train.clip_grad': 0.5673796851231755, 'model.dropout': 0.39357072623327527, 'model.attn_dropout': 0.11714849375732578, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8417166527906165, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.2886984422545921, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.029990886770399267, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 07:55:27,249] Trial 5911 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-11 07:55:27,939] Trial 5912 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 48 (patience=20)

================================================================================
TRIAL 5913 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.673047677097121e-05
  Dropout: 0.101745030552805
================================================================================

[I 2025-11-11 07:57:07,988] Trial 5908 finished with value: 0.4444444444444444 and parameters: {'seed': 53155, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 5.859289904966997e-05, 'optim.weight_decay': 7.260314798471645e-06, 'optim.beta1': 0.8497817271575568, 'optim.beta2': 0.985787665645569, 'optim.eps': 5.3480440804356444e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1820204481644965, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.393000384787669, 'model.dropout': 0.17770195648794726, 'model.attn_dropout': 0.09752274233629192, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9745595103743651, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.49077548129994386, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.9894031005909785, 'loss.cls.alpha': 0.24359877813438505, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 07:57:08,703] Trial 5914 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5915 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.8232648414572043e-05
  Dropout: 0.4416367891562828
================================================================================

[I 2025-11-11 07:58:40,397] Trial 5913 pruned. Pruned at step 11 with metric 0.6095
[I 2025-11-11 07:58:41,118] Trial 5916 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5917 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.631758119240399e-06
  Dropout: 0.41999542054811323
================================================================================

[I 2025-11-11 08:07:30,770] Trial 5915 pruned. Pruned at step 12 with metric 0.6093

================================================================================
TRIAL 5918 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.4000648406634476e-05
  Dropout: 0.4587297794732962
================================================================================

[I 2025-11-11 08:22:51,945] Trial 5917 finished with value: 0.7099315461394318 and parameters: {'seed': 62250, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 6.631758119240399e-06, 'optim.weight_decay': 0.04796988799993442, 'optim.beta1': 0.9074240694086831, 'optim.beta2': 0.9958376099817138, 'optim.eps': 6.476418067639874e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1330945035497369, 'sched.poly_power': 0.6095965990304665, 'train.clip_grad': 1.1696285373780126, 'model.dropout': 0.41999542054811323, 'model.attn_dropout': 0.18514266974468432, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8698732369143388, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.34608530368333684, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.03844642912860688, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 47 (patience=20)

================================================================================
TRIAL 5919 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.341019165789161e-06
  Dropout: 0.009433534125486753
================================================================================

[I 2025-11-11 08:29:31,827] Trial 5918 finished with value: 0.6731793358434246 and parameters: {'seed': 33276, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.4000648406634476e-05, 'optim.weight_decay': 0.05196983196006447, 'optim.beta1': 0.829996113612337, 'optim.beta2': 0.9976742459932308, 'optim.eps': 4.8023240522789625e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1663944804427743, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.3482317182929361, 'model.dropout': 0.4587297794732962, 'model.attn_dropout': 0.16795736220323573, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8992712215514694, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.052316761342688985, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.68043190591698, 'loss.cls.alpha': 0.26308094737588816, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 08:29:32,511] The parameter `tok.doc_stride` in Trial#5920 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 43 (patience=20)

================================================================================
TRIAL 5920 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 3.676212296385344e-05
  Dropout: 0.4638364217648721
================================================================================

[I 2025-11-11 08:32:24,163] Trial 5919 pruned. Pruned at step 15 with metric 0.5480
[I 2025-11-11 08:32:24,872] Trial 5921 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5922 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 5.6832941055780114e-06
  Dropout: 0.4981639247409904
================================================================================

[I 2025-11-11 08:52:18,780] Trial 5922 finished with value: 0.6125082182774491 and parameters: {'seed': 63269, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.6832941055780114e-06, 'optim.weight_decay': 0.013714947242410454, 'optim.beta1': 0.9303808787886724, 'optim.beta2': 0.991702275817441, 'optim.eps': 5.56361154740549e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.08205646115862844, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.7830294539071034, 'model.dropout': 0.4981639247409904, 'model.attn_dropout': 0.2111405842057225, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9275645261054164, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.2578942953740978, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.03928184142836204, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 5923 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.815477131313737e-06
  Dropout: 0.0466026540665177
================================================================================

[I 2025-11-11 08:52:27,769] Trial 5920 pruned. Pruned at step 17 with metric 0.5847
[W 2025-11-11 08:52:28,466] The parameter `tok.doc_stride` in Trial#5924 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5924 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 8.147551951395776e-06
  Dropout: 0.4888264930448766
================================================================================

[I 2025-11-11 08:59:29,342] Trial 5923 pruned. Pruned at step 15 with metric 0.5884
[I 2025-11-11 08:59:30,069] Trial 5925 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-11 08:59:30,697] The parameter `tok.doc_stride` in Trial#5926 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5926 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 5.5065660572710894e-06
  Dropout: 0.39773534024595314
================================================================================

[I 2025-11-11 09:04:52,210] Trial 5924 pruned. Pruned at step 9 with metric 0.6117
[W 2025-11-11 09:04:52,902] The parameter `tok.doc_stride` in Trial#5927 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5927 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.262875692505906e-05
  Dropout: 0.24382293803711297
================================================================================

[I 2025-11-11 09:05:14,019] Trial 5926 pruned. Pruned at step 13 with metric 0.5742
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5928 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.829823224312042e-06
  Dropout: 0.21043096952122134
================================================================================

[I 2025-11-11 09:09:41,452] Trial 5927 pruned. Pruned at step 9 with metric 0.5839
[I 2025-11-11 09:09:42,154] Trial 5929 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-11 09:09:42,832] Trial 5930 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5931 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 4.2083822375896165e-05
  Dropout: 0.07847736683129589
================================================================================

[I 2025-11-11 09:11:04,003] Trial 5928 pruned. Pruned at step 14 with metric 0.5586
[I 2025-11-11 09:11:04,731] Trial 5932 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5933 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 1.0049050315738368e-05
  Dropout: 0.4576219501352548
================================================================================

[I 2025-11-11 09:11:11,532] Trial 5933 pruned. OOM: microsoft/deberta-v3-large bs=16 len=160
[I 2025-11-11 09:11:12,348] Trial 5934 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-11 09:11:12,970] The parameter `tok.doc_stride` in Trial#5935 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 5933 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 48.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5935 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 8.367850453170092e-06
  Dropout: 0.3425519154256361
================================================================================

[I 2025-11-11 09:17:55,276] Trial 5935 finished with value: 0.6826625386996904 and parameters: {'seed': 30595, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 8.367850453170092e-06, 'optim.weight_decay': 0.0018632605443247346, 'optim.beta1': 0.9131027915081378, 'optim.beta2': 0.9873765931951934, 'optim.eps': 1.495713715914695e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1970822040015668, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.863205603171957, 'model.dropout': 0.3425519154256361, 'model.attn_dropout': 0.18323725272340025, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9231009985371447, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.06274623917808701, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.484216316087169, 'loss.cls.alpha': 0.8509105658455863, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 5936 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.177779882223035e-06
  Dropout: 0.3510118632032886
================================================================================

[I 2025-11-11 09:29:55,984] Trial 5936 pruned. Pruned at step 15 with metric 0.6208
[I 2025-11-11 09:29:56,675] Trial 5937 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[W 2025-11-11 09:29:57,311] The parameter `tok.doc_stride` in Trial#5938 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5938 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.2049556473693262e-05
  Dropout: 0.006370249698344281
================================================================================

[I 2025-11-11 09:41:34,769] Trial 5938 finished with value: 0.6776729559748428 and parameters: {'seed': 55418, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.2049556473693262e-05, 'optim.weight_decay': 8.338206769168855e-06, 'optim.beta1': 0.9461099729021132, 'optim.beta2': 0.9754347603799559, 'optim.eps': 9.26161082907565e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.013535389027155727, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.4635740778486703, 'model.dropout': 0.006370249698344281, 'model.attn_dropout': 0.17647179799091064, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8066867481310327, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3642763443405955, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.671502667171605, 'loss.cls.alpha': 0.35557911318108565, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 09:41:35,412] The parameter `tok.doc_stride` in Trial#5939 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 5939 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.189145334814722e-06
  Dropout: 0.48688605579163213
================================================================================

[I 2025-11-11 09:55:45,673] Trial 5939 pruned. Pruned at step 27 with metric 0.5619
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5940 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.470119247085091e-05
  Dropout: 0.015261091193038222
================================================================================

[I 2025-11-11 10:08:59,660] Trial 5940 finished with value: 0.4383561643835616 and parameters: {'seed': 44449, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.470119247085091e-05, 'optim.weight_decay': 0.000738713072900792, 'optim.beta1': 0.8947790594433906, 'optim.beta2': 0.9946813459827307, 'optim.eps': 2.757944660563382e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.057383860218152075, 'sched.poly_power': 0.7480138181183166, 'train.clip_grad': 1.477870982099159, 'model.dropout': 0.015261091193038222, 'model.attn_dropout': 0.23060179305171494, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8319862171693408, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.3776902283385717, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.739025280780984, 'loss.cls.alpha': 0.8121800669801558, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 5941 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.202803215353479e-05
  Dropout: 0.3683625637940021
================================================================================

[I 2025-11-11 10:13:49,881] Trial 5941 pruned. Pruned at step 10 with metric 0.6192
[I 2025-11-11 10:13:50,575] Trial 5942 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-11 10:13:51,210] The parameter `tok.doc_stride` in Trial#5943 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5943 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 4.417334767382667e-05
  Dropout: 0.15242375480287454
================================================================================

[I 2025-11-11 10:23:08,058] Trial 5943 finished with value: 0.6123542388906398 and parameters: {'seed': 60523, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 4.417334767382667e-05, 'optim.weight_decay': 7.424306120418012e-06, 'optim.beta1': 0.9391340114816921, 'optim.beta2': 0.9937708222055553, 'optim.eps': 3.9552833380221094e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.07902842892265623, 'train.clip_grad': 1.1574751423121885, 'model.dropout': 0.15242375480287454, 'model.attn_dropout': 0.09017378298911984, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.809744312973449, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.24404544856617605, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.746738776519013, 'loss.cls.alpha': 0.33716082253689794, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 10:23:08,782] Trial 5944 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-11 10:23:09,405] The parameter `tok.doc_stride` in Trial#5945 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 5945 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.5325745827095067e-05
  Dropout: 0.010456234432093259
================================================================================

[I 2025-11-11 10:23:17,788] Trial 5931 pruned. OOM: microsoft/deberta-v3-large bs=8 len=256
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5931 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 34.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5946 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.6235861988115414e-05
  Dropout: 0.4418842222223651
================================================================================

[I 2025-11-11 10:32:02,625] Trial 5946 pruned. Pruned at step 9 with metric 0.6390

================================================================================
TRIAL 5947 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.5845444722453394e-05
  Dropout: 0.07859609802087103
================================================================================

[I 2025-11-11 10:37:22,074] Trial 5947 pruned. Pruned at step 10 with metric 0.6428

================================================================================
TRIAL 5948 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 2.4938855954841776e-05
  Dropout: 0.07084299417247604
================================================================================

[I 2025-11-11 11:06:12,909] Trial 5948 finished with value: 0.6474028207774338 and parameters: {'seed': 16951, 'model.name': 'bert-large-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.4938855954841776e-05, 'optim.weight_decay': 0.11367084001052456, 'optim.beta1': 0.863123626580846, 'optim.beta2': 0.9776823087695782, 'optim.eps': 6.45145063410674e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.13785964991955613, 'train.clip_grad': 0.2387636688401822, 'model.dropout': 0.07084299417247604, 'model.attn_dropout': 0.2317228725211835, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8358076797835546, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.13654547972961611, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.197467001208911, 'loss.cls.alpha': 0.6924443374033697, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 5949 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.084413244079794e-05
  Dropout: 0.37256898328410637
================================================================================

[I 2025-11-11 11:11:40,790] Trial 5949 finished with value: 0.6672077922077921 and parameters: {'seed': 63130, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.084413244079794e-05, 'optim.weight_decay': 0.14573991159326713, 'optim.beta1': 0.9125823619527688, 'optim.beta2': 0.9841367817018798, 'optim.eps': 1.890198533329808e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.10989221337988286, 'sched.poly_power': 0.5053725398284667, 'train.clip_grad': 0.6400399624624095, 'model.dropout': 0.37256898328410637, 'model.attn_dropout': 0.17049673435513502, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9097560623171405, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.17827530670085617, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.5026634054750474, 'loss.cls.alpha': 0.20497618951841856, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 11:11:41,455] The parameter `tok.doc_stride` in Trial#5950 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 11:11:41,517] Trial 5950 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 5951 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.235771761075197e-06
  Dropout: 0.32926456891015227
================================================================================

[I 2025-11-11 11:25:18,213] Trial 5951 pruned. Pruned at step 29 with metric 0.6650
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5952 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 2.7345573936997222e-05
  Dropout: 0.4408781169794344
================================================================================

[I 2025-11-11 11:38:55,593] Trial 5945 finished with value: 0.705799992999405 and parameters: {'seed': 40206, 'model.name': 'microsoft/deberta-v3-large', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.5325745827095067e-05, 'optim.weight_decay': 3.603280092625904e-05, 'optim.beta1': 0.8293792446128393, 'optim.beta2': 0.987949494900959, 'optim.eps': 1.3311480350667117e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.16152025507603576, 'sched.cosine_cycles': 1, 'train.clip_grad': 1.2640624741365305, 'model.dropout': 0.010456234432093259, 'model.attn_dropout': 0.09452267573610135, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8508618362974727, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.11790329061810192, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.042458381454752024, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 5953 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.690731613950256e-06
  Dropout: 0.3722759953118351
================================================================================

[I 2025-11-11 11:43:19,296] Trial 5953 pruned. Pruned at step 14 with metric 0.6083
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5954 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 1.0633051956875395e-05
  Dropout: 0.3810339967166908
================================================================================

[I 2025-11-11 11:58:49,915] Trial 5954 pruned. Pruned at step 9 with metric 0.6299

================================================================================
TRIAL 5955 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 9.964478176672773e-06
  Dropout: 0.3205358600646396
================================================================================

[I 2025-11-11 12:02:10,342] Trial 5955 pruned. Pruned at step 9 with metric 0.5962
[W 2025-11-11 12:02:11,061] The parameter `tok.doc_stride` in Trial#5956 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 12:02:11,126] Trial 5956 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5957 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.6157564362368078e-05
  Dropout: 0.08848261837020577
================================================================================

[I 2025-11-11 12:10:42,458] Trial 5957 finished with value: 0.7222723988439306 and parameters: {'seed': 63603, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.6157564362368078e-05, 'optim.weight_decay': 1.4852331125714368e-05, 'optim.beta1': 0.8987019729141166, 'optim.beta2': 0.9871790970725, 'optim.eps': 5.706017514062076e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.04159664341412495, 'train.clip_grad': 1.1117338412710183, 'model.dropout': 0.08848261837020577, 'model.attn_dropout': 0.22818653538498862, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.833477829259541, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.30218383435251883, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.641161554005668, 'loss.cls.alpha': 0.6239404813474327, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 12:10:43,125] The parameter `tok.doc_stride` in Trial#5958 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 5958 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.944769961733519e-05
  Dropout: 0.09113412749748544
================================================================================

[I 2025-11-11 12:13:20,933] Trial 5958 pruned. Pruned at step 11 with metric 0.5986
[I 2025-11-11 12:13:21,652] Trial 5959 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5960 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 7.772330778952676e-06
  Dropout: 0.41264535591029483
================================================================================

[I 2025-11-11 12:46:41,293] Trial 5960 pruned. Pruned at step 27 with metric 0.6242
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 5961 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 7.674656531711129e-06
  Dropout: 0.44179997276651956
================================================================================

[I 2025-11-11 12:53:51,468] Trial 5961 pruned. Pruned at step 26 with metric 0.5827

================================================================================
TRIAL 5962 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.0766770315377916e-05
  Dropout: 0.15860045474462234
================================================================================

[I 2025-11-11 12:57:43,658] Trial 5962 pruned. Pruned at step 20 with metric 0.5325

================================================================================
TRIAL 5963 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.8521352549933282e-05
  Dropout: 0.2337352170953857
================================================================================

[I 2025-11-11 12:57:49,126] Trial 5963 pruned. OOM: bert-large-uncased bs=16 len=320
[W 2025-11-11 12:57:49,804] The parameter `tok.doc_stride` in Trial#5964 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 12:57:49,866] Trial 5964 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[I 2025-11-11 12:57:50,530] Trial 5965 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)

[OOM] Trial 5963 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 16 (effective: 16 with grad_accum=1)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 46.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5966 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.5405371950590576e-05
  Dropout: 0.03479369426701166
================================================================================

[I 2025-11-11 12:57:56,945] Trial 5966 pruned. OOM: bert-base-uncased bs=48 len=384
[I 2025-11-11 12:57:57,663] Trial 5967 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-11 12:57:58,394] Trial 5968 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-11 12:57:59,003] Trial 5952 pruned. OOM: microsoft/deberta-v3-large bs=12 len=128
[W 2025-11-11 12:58:00,288] The parameter `tok.doc_stride` in Trial#5970 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 5966 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 48 (effective: 144 with grad_accum=3)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 82.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5952 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 60.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5969 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 5.021680911176894e-06
  Dropout: 0.4587486871943681
================================================================================


================================================================================
TRIAL 5970 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.023881838113031e-05
  Dropout: 0.29713275729535527
================================================================================

[I 2025-11-11 13:14:25,298] Trial 5969 pruned. Pruned at step 29 with metric 0.5512

================================================================================
TRIAL 5971 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 6.1343305968131104e-06
  Dropout: 0.30175013455194366
================================================================================

[I 2025-11-11 13:20:11,219] Trial 5971 pruned. Pruned at step 10 with metric 0.5383
[I 2025-11-11 13:20:11,911] Trial 5972 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 5973 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 9.647909477374548e-06
  Dropout: 0.4581039971009116
================================================================================

[I 2025-11-11 13:24:35,886] Trial 5970 pruned. Pruned at step 15 with metric 0.5750
[W 2025-11-11 13:24:36,809] The parameter `tok.doc_stride` in Trial#5974 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5974 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.922693135160036e-05
  Dropout: 0.15245502128595906
================================================================================

[I 2025-11-11 13:35:09,957] Trial 5973 pruned. Pruned at step 27 with metric 0.6107
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5975 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 1.1040576544732438e-05
  Dropout: 0.39336505767688507
================================================================================

[I 2025-11-11 13:36:40,054] Trial 5974 pruned. Pruned at step 11 with metric 0.6356
[I 2025-11-11 13:36:40,777] Trial 5976 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
[W 2025-11-11 13:36:41,410] The parameter `tok.doc_stride` in Trial#5977 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 5977 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.573770235311192e-06
  Dropout: 0.23413551911653102
================================================================================

[I 2025-11-11 13:54:07,029] Trial 5975 finished with value: 0.7005550686532283 and parameters: {'seed': 53787, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.1040576544732438e-05, 'optim.weight_decay': 0.0011057901002707858, 'optim.beta1': 0.8394650753975917, 'optim.beta2': 0.962538843180789, 'optim.eps': 9.120638547015065e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.009365119587711903, 'sched.poly_power': 0.8321599560687886, 'train.clip_grad': 1.2992329698555989, 'model.dropout': 0.39336505767688507, 'model.attn_dropout': 0.21774944227954635, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9921293072330821, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'gelu', 'head.dropout': 0.35423802474297994, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.993875437578063, 'loss.cls.alpha': 0.3361194413901115, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 5978 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 8.136256952775822e-06
  Dropout: 0.4587749923372176
================================================================================

[I 2025-11-11 13:58:28,227] Trial 5978 pruned. Pruned at step 14 with metric 0.6231

================================================================================
TRIAL 5979 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.920314979208375e-06
  Dropout: 0.4387792990409636
================================================================================

[I 2025-11-11 14:12:12,108] Trial 5979 finished with value: 0.7625115848007414 and parameters: {'seed': 56903, 'model.name': 'bert-base-uncased', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.920314979208375e-06, 'optim.weight_decay': 2.1938271779252876e-06, 'optim.beta1': 0.8969812376749794, 'optim.beta2': 0.9774819994387638, 'optim.eps': 1.0250844876594081e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.144782401742035, 'train.clip_grad': 1.0642625762862907, 'model.dropout': 0.4387792990409636, 'model.attn_dropout': 0.18960711571537198, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9548042474197856, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4883321842334747, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.61887178640122, 'loss.cls.alpha': 0.32311292673317044, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 5980 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.445082990453187e-06
  Dropout: 0.19946087962024528
================================================================================

[I 2025-11-11 14:23:34,744] Trial 5980 pruned. Pruned at step 28 with metric 0.6254
[I 2025-11-11 14:23:35,442] Trial 5981 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 5982 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 1.963269906595603e-05
  Dropout: 0.334070496902277
================================================================================

[I 2025-11-11 14:23:42,891] Trial 5982 pruned. OOM: microsoft/deberta-v3-large bs=16 len=320
[I 2025-11-11 14:23:43,098] Trial 5977 pruned. OOM: bert-base-uncased bs=8 len=128

[OOM] Trial 5977 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 70.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5982 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 70.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5983 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 8.855566647785581e-06
  Dropout: 0.25846554086108237
================================================================================


================================================================================
TRIAL 5984 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.926675079315573e-05
  Dropout: 0.4410810669291246
================================================================================

[I 2025-11-11 14:23:49,260] Trial 5983 pruned. OOM: bert-base-uncased bs=64 len=320
[I 2025-11-11 14:23:49,935] Trial 5984 pruned. OOM: bert-base-uncased bs=64 len=352
[W 2025-11-11 14:23:51,136] The parameter `tok.doc_stride` in Trial#5986 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 14:23:51,227] Trial 5986 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

[OOM] Trial 5983 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 72.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5984 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 272.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5985 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.379322204965985e-06
  Dropout: 0.4886293457713273
================================================================================


================================================================================
TRIAL 5987 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 8.000599011710376e-06
  Dropout: 0.10828320256149199
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-11 14:24:00,671] Trial 5985 pruned. OOM: bert-base-uncased bs=64 len=352
[I 2025-11-11 14:24:00,882] Trial 5987 pruned. OOM: roberta-base bs=48 len=224
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 5987 exceeded GPU memory:
  Model: roberta-base
  Batch size: 48 (effective: 288 with grad_accum=6)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 120.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 5985 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 5988 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 3.579140190901108e-05
  Dropout: 0.39380075265409176
================================================================================


================================================================================
TRIAL 5989 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 5.115451570143432e-06
  Dropout: 0.4591473803003266
================================================================================

/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [64,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [65,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [66,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [67,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [68,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [69,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [70,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [71,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [72,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [73,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [74,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [75,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [76,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [77,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [78,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [79,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [80,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [81,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [82,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [83,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [84,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [85,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [86,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [87,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [88,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [89,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [90,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [91,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [92,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [93,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [94,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
/pytorch/aten/src/ATen/native/cuda/IndexKernelUtils.cu:16: vectorized_gather_kernel: block: [62,0,0], thread: [95,0,0] Assertion `ind >=0 && ind < ind_dim_size && "vectorized gather kernel index out of bounds"` failed.
[W 2025-11-11 14:24:03,973] Trial 5988 failed with parameters: {'seed': 18734, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.579140190901108e-05, 'optim.weight_decay': 0.13025735205718392, 'optim.beta1': 0.8888703749589535, 'optim.beta2': 0.9576063334049286, 'optim.eps': 9.40037308614277e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04420577516606099, 'sched.poly_power': 1.266290279169785, 'train.clip_grad': 1.4960490369048332, 'model.dropout': 0.39380075265409176, 'model.attn_dropout': 0.21653128386205908, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8790816186306439, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.3019862617608503, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.054121443583345315, 'loss.cls.balance': 'effective_num'} because of the following error: RuntimeError('CUDA context corrupted (health check failed after cleanup). Process must restart.').
Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1081, in _obj
    res = run_training_eval(cfg, {"on_epoch": _cb}, trial_number=trial.number)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 815, in run_training_eval
    logits = model(input_ids, attention_mask)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/src/Project/Criteria/models/model.py", line 119, in forward
    outputs = self.encoder(
              ^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 798, in forward
    embedding_output = self.embeddings(
                       ^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 111, in forward
    token_type_embeddings = self.token_type_embeddings(token_type_ids)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 192, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/functional.py", line 2542, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1198, in _obj
    raise RuntimeError(
RuntimeError: CUDA context corrupted (health check failed after cleanup). Process must restart.
[W 2025-11-11 14:24:03,981] Trial 5988 failed with value None.
[W 2025-11-11 14:24:04,185] Trial 5989 failed with parameters: {'seed': 58636, 'model.name': 'bert-base-uncased', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.115451570143432e-06, 'optim.weight_decay': 0.0021133282749080783, 'optim.beta1': 0.9119427046207796, 'optim.beta2': 0.9952661609076823, 'optim.eps': 3.996851132813231e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.07505127202213382, 'sched.poly_power': 1.2594581065098545, 'train.clip_grad': 0.6822411118816311, 'model.dropout': 0.4591473803003266, 'model.attn_dropout': 0.10558186714163403, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9070148500970809, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.4145099910660778, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.048013485822456334, 'loss.cls.balance': 'weighted'} because of the following error: RuntimeError('CUDA context corrupted (health check failed after cleanup). Process must restart.').
Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1081, in _obj
    res = run_training_eval(cfg, {"on_epoch": _cb}, trial_number=trial.number)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 745, in run_training_eval
    model = safe_to_device(model, device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 732, in safe_to_device
    model = model.to(target_device)
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
           ^^^^^
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1198, in _obj
    raise RuntimeError(
RuntimeError: CUDA context corrupted (health check failed after cleanup). Process must restart.
[W 2025-11-11 14:24:04,186] Trial 5989 failed with value None.
Warning: Error during model cleanup: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Error during CUDA cleanup: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


[CUDA ERROR] Trial 5989 encountered CUDA error (consecutive failures: 2):
  Error Type: AcceleratorError
  Model: bert-base-uncased
  Batch size: 48
  Learning rate: 5.115451570143432e-06
  Dropout: 0.4591473803003266
  Error: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


  This trial will be pruned. HPO will continue.


[CUDA ERROR] Trial 5988 encountered CUDA error (consecutive failures: 2):
  Error Type: AcceleratorError
  Model: roberta-base
  Batch size: 8
  Learning rate: 3.579140190901108e-05
  Dropout: 0.39380075265409176
  Error: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


  This trial will be pruned. HPO will continue.


================================================================================
FATAL: CUDA health check failed after error cleanup!
CUDA context is corrupted and cannot be recovered.
Raising fatal error to trigger process restart...
================================================================================


================================================================================
FATAL: CUDA health check failed after error cleanup!
CUDA context is corrupted and cannot be recovered.
Raising fatal error to trigger process restart...
================================================================================

Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1081, in _obj
    res = run_training_eval(cfg, {"on_epoch": _cb}, trial_number=trial.number)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 815, in run_training_eval
    logits = model(input_ids, attention_mask)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/src/Project/Criteria/models/model.py", line 119, in forward
    outputs = self.encoder(
              ^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 798, in forward
    embedding_output = self.embeddings(
                       ^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 111, in forward
    token_type_embeddings = self.token_type_embeddings(token_type_ids)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 192, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/torch/nn/functional.py", line 2542, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1624, in <module>
    main()
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1573, in main
    study.optimize(
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 100, in _optimize
    f.result()
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/concurrent/futures/thread.py", line 59, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 258, in _run_trial
    raise func_err
  File "/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py", line 1198, in _obj
    raise RuntimeError(
RuntimeError: CUDA context corrupted (health check failed after cleanup). Process must restart.
[W1111 14:24:25.691547381 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
make[1]: *** [Makefile:369: tune-criteria-supermax] Error 1
make[1]: Leaving directory '/media/user/SSD1/YuNing/NoAug_Criteria_Evidence'
make: *** [Makefile:414: tune-all-supermax] Error 2
