[0;34m======================================================================
  Running Super-Max HPO for ALL Architectures Sequentially
======================================================================[0m 

[0;33mSequence: Criteria â†’ Evidence â†’ Share â†’ Joint[0m 
[0;33mTotal trials: ~19,000 (5000+8000+3000+3000)[0m 
[0;33mEstimated time: ~120-180 hours with PAR=2 (reduced for GPU stability)[0m 

[0;32mStarting...[0m 

[0;34m[1/4] Running Criteria (5000 trials)...[0m 
make[1]: Entering directory '/media/user/SSD1/YuNing/NoAug_Criteria_Evidence'
[0;34mRunning SUPER-MAX HPO for Criteria...[0m 
Trials: 5000 | Parallel: 2 (reduced 4â†’3â†’2 for stability) | Epochs: 100 | Patience: 20
PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python HPO_EPOCHS=100 HPO_PATIENCE=20 poetry run python scripts/tune_max.py \
	--agent criteria --study noaug-criteria-supermax \
	--n-trials 5000 --parallel 2 \
	--outdir ./_runs
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.
  warnings.warn(
/media/user/SSD1/YuNing/NoAug_Criteria_Evidence/scripts/tune_max.py:975: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return PatientPruner(hb, patience=4)  # More patient (was 2)
[I 2025-11-11 14:27:27,807] Using an existing study with name 'noaug-criteria-supermax' instead of creating a new one.
[W 2025-11-11 14:27:36,072] The parameter `tok.doc_stride` in Trial#6001 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 14:27:36,375] Trial 6001 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-11 14:27:37,068] Trial 6003 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
âœ“ Configuration validation passed
  Agent: criteria
  Epochs: 100 | Patience: 20
  Output: ./_runs
[HPO] agent=criteria epochs=100 storage=sqlite:////media/user/SSD1/YuNing/NoAug_Criteria_Evidence/_optuna/noaug.db
[HPO] Study 'noaug-criteria-supermax' is compatible. Resuming optimization.

================================================================================
TRIAL 6002 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.1883550424656543e-05
  Dropout: 0.24723756342853526
================================================================================


================================================================================
TRIAL 6004 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 7.727462659193721e-06
  Dropout: 0.33078548843609373
================================================================================

[I 2025-11-11 14:27:46,402] Trial 6002 pruned. OOM: roberta-base bs=64 len=352

[OOM] Trial 6002 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 384 with grad_accum=6)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 84.00 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6005 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.117697502243294e-06
  Dropout: 0.4910337656224462
================================================================================

[I 2025-11-11 14:54:36,108] Trial 6005 pruned. Pruned at step 16 with metric 0.5371
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6006 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 7.969187186621962e-06
  Dropout: 0.4374026910210898
================================================================================

[I 2025-11-11 15:01:41,935] Trial 6004 pruned. Pruned at step 27 with metric 0.6184
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6007 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 9.592993185222372e-06
  Dropout: 0.48507834350377216
================================================================================

[I 2025-11-11 15:10:01,660] Trial 6007 pruned. Pruned at step 27 with metric 0.6139
[I 2025-11-11 15:10:02,289] Trial 6008 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-11 15:10:02,883] Trial 6009 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6010 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.3518707159133995e-05
  Dropout: 0.29147986403713366
================================================================================

[I 2025-11-11 15:52:05,113] Trial 6006 pruned. Pruned at step 11 with metric 0.6602

================================================================================
TRIAL 6011 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.563666740148097e-06
  Dropout: 0.3624017530676502
================================================================================

[I 2025-11-11 15:52:09,071] Trial 6011 pruned. OOM: bert-base-uncased bs=64 len=384
[I 2025-11-11 15:52:09,727] Trial 6012 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

[OOM] Trial 6011 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 104.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6013 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.057106100922452e-06
  Dropout: 0.49890571356747493
================================================================================

[I 2025-11-11 15:52:14,990] Trial 6013 pruned. OOM: bert-base-uncased bs=64 len=384
[W 2025-11-11 15:52:15,591] The parameter `tok.doc_stride` in Trial#6014 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 15:52:16,507] Trial 6010 pruned. OOM: roberta-large bs=16 len=256

[OOM] Trial 6013 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 66.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6014 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.398300002550026e-05
  Dropout: 0.2796692981410876
================================================================================


[OOM] Trial 6010 exceeded GPU memory:
  Model: roberta-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 66.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6015 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.784046224510621e-06
  Dropout: 0.2685708578785675
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-11 15:56:35,873] Trial 6014 pruned. Pruned at step 8 with metric 0.5688

================================================================================
TRIAL 6016 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 8.075348684822223e-06
  Dropout: 0.39934255600465224
================================================================================

[I 2025-11-11 15:58:07,997] Trial 6015 pruned. Pruned at step 14 with metric 0.6273

================================================================================
TRIAL 6017 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.1470310993523511e-05
  Dropout: 0.31931374883524116
================================================================================

[I 2025-11-11 16:27:50,079] Trial 6016 finished with value: 0.6916871752802844 and parameters: {'seed': 50797, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 8.075348684822223e-06, 'optim.weight_decay': 0.016940423377664948, 'optim.beta1': 0.9059881038875328, 'optim.beta2': 0.988878187620897, 'optim.eps': 1.494847752215699e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1476420880323286, 'sched.poly_power': 1.2161509574761524, 'train.clip_grad': 0.8149865034138997, 'model.dropout': 0.39934255600465224, 'model.attn_dropout': 0.1624785121581404, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8345840028911358, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.45860898415865603, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.05783709086903353, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 6018 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.0146216974917079e-05
  Dropout: 0.4118886069461748
================================================================================

[I 2025-11-11 16:52:00,615] Trial 6018 pruned. Pruned at step 12 with metric 0.5383

================================================================================
TRIAL 6019 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.9095782673169244e-05
  Dropout: 0.4262782004214597
================================================================================

[I 2025-11-11 16:57:34,799] Trial 6017 pruned. Pruned at step 27 with metric 0.6297
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6020 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.7584992875683413e-05
  Dropout: 0.118352013796325
================================================================================

[I 2025-11-11 17:01:10,405] Trial 6019 pruned. Pruned at step 11 with metric 0.6032

================================================================================
TRIAL 6021 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.7663530806651302e-05
  Dropout: 0.022583310226952
================================================================================

[I 2025-11-11 17:05:26,917] Trial 6021 pruned. Pruned at step 13 with metric 0.5731
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6022 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.100850714897161e-06
  Dropout: 0.46880833536142885
================================================================================

[I 2025-11-11 17:05:46,657] Trial 6020 pruned. Pruned at step 6 with metric 0.5929
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6023 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.298916790425995e-05
  Dropout: 0.4384916973086218
================================================================================

[I 2025-11-11 17:18:38,065] Trial 6023 finished with value: 0.6421395869701262 and parameters: {'seed': 56733, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 3.298916790425995e-05, 'optim.weight_decay': 0.03954311537026546, 'optim.beta1': 0.898942874023576, 'optim.beta2': 0.9561659581141263, 'optim.eps': 3.515104456933632e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.05170918130571238, 'sched.poly_power': 1.0964963731679753, 'train.clip_grad': 1.1204924343979352, 'model.dropout': 0.4384916973086218, 'model.attn_dropout': 0.1850398570890756, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8298858799488943, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.21093459991433447, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.0578190041347035, 'loss.cls.alpha': 0.24856446307707455, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 6024 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 4.5381118176564114e-05
  Dropout: 0.11814050992082381
================================================================================

[I 2025-11-11 17:20:34,573] Trial 6022 finished with value: 0.6496445173639596 and parameters: {'seed': 12108, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 32, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 6.100850714897161e-06, 'optim.weight_decay': 0.005930006405819174, 'optim.beta1': 0.9213033953435604, 'optim.beta2': 0.9903685811165357, 'optim.eps': 6.699682110407801e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.12739541030991972, 'train.clip_grad': 1.271955615036415, 'model.dropout': 0.46880833536142885, 'model.attn_dropout': 0.14858673208919324, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9516700956404713, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.3981612403968685, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.050718027791632474, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 33 (patience=20)

================================================================================
TRIAL 6025 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 7.70336313862841e-06
  Dropout: 0.4843860911341309
================================================================================

[I 2025-11-11 17:25:57,878] Trial 6025 pruned. Pruned at step 10 with metric 0.5984

================================================================================
TRIAL 6026 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.436779086035072e-05
  Dropout: 0.4522413441523483
================================================================================

[I 2025-11-11 17:37:31,727] Trial 6026 pruned. Pruned at step 27 with metric 0.6252
[I 2025-11-11 17:37:32,345] Trial 6027 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-11 17:37:32,908] The parameter `tok.doc_stride` in Trial#6028 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6028 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 2.9039487662076005e-05
  Dropout: 0.11289995083068716
================================================================================

[I 2025-11-11 17:37:59,286] Trial 6024 pruned. Pruned at step 27 with metric 0.6067
[I 2025-11-11 17:37:59,918] Trial 6029 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-11 17:38:00,506] Trial 6030 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-11 17:38:01,135] Trial 6031 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6032 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 8.408148762305764e-06
  Dropout: 0.019811816692438873
================================================================================

[I 2025-11-11 17:43:32,388] Trial 6032 pruned. Pruned at step 12 with metric 0.5993

================================================================================
TRIAL 6033 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 7.067432280690919e-06
  Dropout: 0.22139227112051274
================================================================================

[I 2025-11-11 17:43:35,663] Trial 6033 pruned. OOM: bert-base-uncased bs=64 len=384
[W 2025-11-11 17:43:36,255] The parameter `tok.doc_stride` in Trial#6034 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 6033 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 54.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6034 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 8.41137408928927e-06
  Dropout: 0.3649635880351871
================================================================================

[I 2025-11-11 17:48:30,737] Trial 6034 pruned. Pruned at step 9 with metric 0.6555
[I 2025-11-11 17:48:31,367] Trial 6035 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6036 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 7.5859194132708766e-06
  Dropout: 0.24854430726481358
================================================================================

[I 2025-11-11 18:01:56,608] Trial 6036 pruned. Pruned at step 27 with metric 0.6207
[I 2025-11-11 18:01:57,230] Trial 6037 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6038 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.786596800058086e-05
  Dropout: 0.14921028557615834
================================================================================

[I 2025-11-11 18:08:12,891] Trial 6038 pruned. Pruned at step 10 with metric 0.6207
[I 2025-11-11 18:08:13,514] Trial 6039 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-11 18:08:14,123] Trial 6040 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-11 18:08:14,698] The parameter `tok.doc_stride` in Trial#6041 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6041 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 6.70718924477514e-06
  Dropout: 0.44460376192707185
================================================================================

[I 2025-11-11 18:11:46,467] Trial 6041 pruned. Pruned at step 10 with metric 0.5349
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6042 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 8.790026885216979e-06
  Dropout: 0.4400202636534155
================================================================================

[I 2025-11-11 18:11:55,470] Trial 6028 pruned. OOM: microsoft/deberta-v3-large bs=16 len=128
[I 2025-11-11 18:11:55,720] Trial 6042 pruned. OOM: roberta-large bs=16 len=320
[W 2025-11-11 18:11:56,449] The parameter `tok.doc_stride` in Trial#6043 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 18:11:56,765] Trial 6044 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-11 18:11:57,374] The parameter `tok.doc_stride` in Trial#6045 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 18:11:57,435] Trial 6045 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-11 18:11:58,026] The parameter `tok.doc_stride` in Trial#6046 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 6028 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 48.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6042 exceeded GPU memory:
  Model: roberta-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 48.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6043 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.1104834453523217e-05
  Dropout: 0.2991248835292657
================================================================================


================================================================================
TRIAL 6046 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.453996389313873e-06
  Dropout: 0.37460344446853694
================================================================================

[I 2025-11-11 18:13:43,525] Trial 6043 pruned. Pruned at step 6 with metric 0.6208
[I 2025-11-11 18:13:44,164] Trial 6047 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6048 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.1351674459626447e-05
  Dropout: 0.23484797604448235
================================================================================

[I 2025-11-11 18:17:07,179] Trial 6046 pruned. Pruned at step 9 with metric 0.5393
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6049 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.3207880798927095e-05
  Dropout: 0.16527758611490095
================================================================================

[I 2025-11-11 18:24:16,879] Trial 6048 pruned. Pruned at step 11 with metric 0.6472
[W 2025-11-11 18:24:17,583] The parameter `tok.doc_stride` in Trial#6050 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 18:24:17,639] Trial 6050 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6051 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.6916799958676783e-05
  Dropout: 0.04030834014290467
================================================================================

[I 2025-11-11 18:27:34,819] Trial 6049 pruned. Pruned at step 11 with metric 0.6038
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6052 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.011822838668219e-05
  Dropout: 0.31588984160838796
================================================================================

[I 2025-11-11 18:32:43,184] Trial 6051 finished with value: 0.7201512128530925 and parameters: {'seed': 59761, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 32, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 1.6916799958676783e-05, 'optim.weight_decay': 0.0344538827242205, 'optim.beta1': 0.9019375595522284, 'optim.beta2': 0.9798148667497689, 'optim.eps': 1.094874494277963e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.07539342842717929, 'train.clip_grad': 1.199259579384, 'model.dropout': 0.04030834014290467, 'model.attn_dropout': 0.21545657833449006, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8050099974616142, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.4501787576851544, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.02933239381785, 'loss.cls.alpha': 0.266574195235368, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 30 (patience=20)

================================================================================
TRIAL 6053 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.751515535518829e-05
  Dropout: 0.06186734632899109
================================================================================

[I 2025-11-11 18:53:06,034] Trial 6053 finished with value: 0.6738636363636363 and parameters: {'seed': 63236, 'model.name': 'bert-base-uncased', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.751515535518829e-05, 'optim.weight_decay': 1.0131638182485647e-06, 'optim.beta1': 0.9056613602286633, 'optim.beta2': 0.9937271128308428, 'optim.eps': 1.444871943083138e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.05238993774672421, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.3518349753379417, 'model.dropout': 0.06186734632899109, 'model.attn_dropout': 0.25045455167768405, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8473882070795509, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.4251435731586716, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.67445541769819, 'loss.cls.alpha': 0.31811144943983766, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 42 (patience=20)

================================================================================
TRIAL 6054 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.7182002642690855e-05
  Dropout: 0.26256348754011677
================================================================================

[I 2025-11-11 19:06:24,291] Trial 6054 finished with value: 0.6846035498517768 and parameters: {'seed': 37987, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.7182002642690855e-05, 'optim.weight_decay': 0.10454717343478115, 'optim.beta1': 0.8887602468366456, 'optim.beta2': 0.9960626377078, 'optim.eps': 1.1005042704555117e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.15862462132791313, 'train.clip_grad': 0.5238060329370543, 'model.dropout': 0.26256348754011677, 'model.attn_dropout': 0.2991457384653334, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8267287948816543, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.09089399136989534, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.641105332510728, 'loss.cls.alpha': 0.18972932706008358, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-11 19:06:24,915] Trial 6055 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 6056 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 2.4696970120522287e-05
  Dropout: 0.14820819070951835
================================================================================

[I 2025-11-11 19:07:40,074] Trial 6052 pruned. Pruned at step 27 with metric 0.5842
[W 2025-11-11 19:07:40,693] The parameter `tok.doc_stride` in Trial#6057 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-11 19:07:40,751] Trial 6057 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-11 19:07:41,357] Trial 6058 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6059 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 7.818057075165846e-06
  Dropout: 0.40574399389534166
================================================================================

[I 2025-11-11 19:24:16,966] Trial 6059 finished with value: 0.6537162162162162 and parameters: {'seed': 51322, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 7.818057075165846e-06, 'optim.weight_decay': 0.00044263139324185414, 'optim.beta1': 0.9422710281327212, 'optim.beta2': 0.9819315486417401, 'optim.eps': 3.269557089622316e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.05824823376376401, 'train.clip_grad': 0.35688288513415534, 'model.dropout': 0.40574399389534166, 'model.attn_dropout': 0.106808842444055, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8899289996140421, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.06049347945951181, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.018254489940609117, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 19:24:17,583] The parameter `tok.doc_stride` in Trial#6060 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 6060 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.0672967749751484e-05
  Dropout: 0.26290447803193934
================================================================================

[I 2025-11-11 19:38:43,824] Trial 6060 pruned. Pruned at step 14 with metric 0.6407
[I 2025-11-11 19:38:44,451] Trial 6061 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6062 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.99067425447408e-06
  Dropout: 0.4405261860268749
================================================================================

[I 2025-11-11 19:38:48,175] Trial 6062 pruned. OOM: bert-base-uncased bs=64 len=384
[I 2025-11-11 19:38:48,876] Trial 6063 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-11 19:38:51,641] Trial 6056 pruned. OOM: microsoft/deberta-v3-large bs=8 len=224

[OOM] Trial 6062 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 384 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 70.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6064 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 8.111976188758936e-06
  Dropout: 0.3400512250191379
================================================================================


[OOM] Trial 6056 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 16 with grad_accum=2)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 410.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.

[I 2025-11-11 19:38:52,757] Trial 6065 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-11 19:38:53,416] Trial 6066 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-11 19:38:54,015] Trial 6067 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-11 19:38:54,575] The parameter `tok.doc_stride` in Trial#6068 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6068 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 9.030397861995758e-06
  Dropout: 0.3305547930699101
================================================================================

[I 2025-11-11 19:42:50,418] Trial 6068 pruned. Pruned at step 16 with metric 0.5409
[I 2025-11-11 19:42:51,066] Trial 6069 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
[W 2025-11-11 19:42:51,640] The parameter `tok.doc_stride` in Trial#6070 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6070 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.199466389554329e-05
  Dropout: 0.14925217449240508
================================================================================

[I 2025-11-11 19:45:17,931] Trial 6064 pruned. Pruned at step 9 with metric 0.5338
[I 2025-11-11 19:45:18,591] Trial 6071 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6072 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.1535691396928695e-05
  Dropout: 0.39869203213657795
================================================================================

[I 2025-11-11 19:51:12,735] Trial 6072 pruned. Pruned at step 9 with metric 0.5666
[I 2025-11-11 19:51:13,367] Trial 6073 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6074 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.6641337241381073e-05
  Dropout: 0.10007525117697232
================================================================================

[I 2025-11-11 19:55:25,047] Trial 6074 pruned. Pruned at step 9 with metric 0.6232
[W 2025-11-11 19:55:25,633] The parameter `tok.doc_stride` in Trial#6075 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6075 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.4949138993727e-05
  Dropout: 0.133478576801283
================================================================================

[I 2025-11-11 19:56:05,016] Trial 6070 pruned. Pruned at step 14 with metric 0.5783

================================================================================
TRIAL 6076 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.692877763998411e-05
  Dropout: 0.28021220462438956
================================================================================

[I 2025-11-11 20:02:34,940] Trial 6076 pruned. Pruned at step 14 with metric 0.5763

================================================================================
TRIAL 6077 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.1126916824316832e-05
  Dropout: 0.16223674000149552
================================================================================

[I 2025-11-11 20:06:09,286] Trial 6077 pruned. Pruned at step 9 with metric 0.5819

================================================================================
TRIAL 6078 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.747120684147141e-05
  Dropout: 0.4646644855815541
================================================================================

[I 2025-11-11 20:12:07,788] Trial 6075 pruned. Pruned at step 9 with metric 0.5415
[I 2025-11-11 20:12:08,429] Trial 6079 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6080 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 1.2753494202660525e-05
  Dropout: 0.3919409617832137
================================================================================

[I 2025-11-11 20:19:53,949] Trial 6078 pruned. Pruned at step 11 with metric 0.6607
[I 2025-11-11 20:19:54,858] Trial 6081 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6082 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.691387649337047e-05
  Dropout: 0.03709617789124628
================================================================================

[I 2025-11-11 20:20:00,118] Trial 6082 pruned. OOM: roberta-base bs=64 len=384

[OOM] Trial 6082 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 280.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6083 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 9.897075146282335e-06
  Dropout: 0.2420836215634521
================================================================================

[I 2025-11-11 20:20:05,723] Trial 6080 pruned. OOM: roberta-base bs=32 len=352
[I 2025-11-11 20:20:05,888] Trial 6083 pruned. OOM: bert-base-uncased bs=64 len=352
[I 2025-11-11 20:20:06,759] Trial 6085 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-11 20:20:07,519] Trial 6086 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-11 20:20:08,178] The parameter `tok.doc_stride` in Trial#6087 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 6083 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 120.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6080 exceeded GPU memory:
  Model: roberta-base
  Batch size: 32 (effective: 128 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6084 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 5.682184598889383e-06
  Dropout: 0.44228658443040086
================================================================================


================================================================================
TRIAL 6087 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 6.804569362586872e-06
  Dropout: 0.026994758277996883
================================================================================

/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[I 2025-11-11 20:35:46,685] Trial 6087 pruned. Pruned at step 11 with metric 0.6560
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6088 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.4939480917292994e-06
  Dropout: 0.484700465115386
================================================================================

[I 2025-11-11 20:43:23,495] Trial 6088 pruned. Pruned at step 15 with metric 0.5486
[W 2025-11-11 20:43:24,098] The parameter `tok.doc_stride` in Trial#6089 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6089 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.0171317588252032e-05
  Dropout: 0.28520438790811453
================================================================================

[I 2025-11-11 20:55:41,921] Trial 6089 finished with value: 0.7103658536585367 and parameters: {'seed': 65249, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 3.0171317588252032e-05, 'optim.weight_decay': 0.008121927460396304, 'optim.beta1': 0.8303799964049849, 'optim.beta2': 0.9684279074767551, 'optim.eps': 1.7647316351680204e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.05305907899197567, 'train.clip_grad': 1.491186932376324, 'model.dropout': 0.28520438790811453, 'model.attn_dropout': 0.11031893560304173, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9524714570751658, 'head.pooling': 'attn', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.11148211013005438, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.665281581968978, 'loss.cls.alpha': 0.312674521636609, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 6090 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.2194874669204568e-05
  Dropout: 0.2400145518027929
================================================================================

[I 2025-11-11 21:01:09,645] Trial 6084 finished with value: 0.7231645861467013 and parameters: {'seed': 17529, 'model.name': 'roberta-large', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.682184598889383e-06, 'optim.weight_decay': 0.021720534798932333, 'optim.beta1': 0.9223123529331019, 'optim.beta2': 0.9975681911846377, 'optim.eps': 5.8223419904851214e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.12431371582879912, 'sched.poly_power': 1.0146334778677828, 'train.clip_grad': 1.0824213845240773, 'model.dropout': 0.44228658443040086, 'model.attn_dropout': 0.09676299359214094, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9446101948548116, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.17623241181648314, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.014916638578568085, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-11 21:01:10,265] The parameter `tok.doc_stride` in Trial#6091 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 6091 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.044400732984759e-06
  Dropout: 0.29930975302141694
================================================================================

[I 2025-11-11 21:08:48,249] Trial 6090 pruned. Pruned at step 18 with metric 0.5725
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6092 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 9.258056110289463e-06
  Dropout: 0.44367434677679746
================================================================================

[I 2025-11-11 21:08:56,370] Trial 6091 pruned. OOM: roberta-large bs=8 len=128
[I 2025-11-11 21:08:56,553] Trial 6092 pruned. OOM: microsoft/deberta-v3-large bs=16 len=288
[I 2025-11-11 21:08:57,549] Trial 6093 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 6091 exceeded GPU memory:
  Model: roberta-large
  Batch size: 8 (effective: 8 with grad_accum=1)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proces
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6092 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proces
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6094 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.3596227290123192e-05
  Dropout: 0.09279335509904652
================================================================================


================================================================================
TRIAL 6095 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 4.6837185549650994e-05
  Dropout: 0.04820951598794091
================================================================================

[I 2025-11-11 21:13:20,278] Trial 6095 pruned. Pruned at step 11 with metric 0.5666
[I 2025-11-11 21:13:20,939] Trial 6096 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-11 21:13:21,501] The parameter `tok.doc_stride` in Trial#6097 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6097 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.528113013930976e-06
  Dropout: 0.30518832228834947
================================================================================

[I 2025-11-11 21:21:15,800] Trial 6097 pruned. Pruned at step 10 with metric 0.6297

================================================================================
TRIAL 6098 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.3334195259755289e-05
  Dropout: 0.37111727059894556
================================================================================

[I 2025-11-11 21:33:47,471] Trial 6098 pruned. Pruned at step 27 with metric 0.6167
[I 2025-11-11 21:33:48,099] Trial 6099 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-11 21:33:48,661] The parameter `tok.doc_stride` in Trial#6100 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6100 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.1846498844141607e-05
  Dropout: 0.3910105743767254
================================================================================

[I 2025-11-11 21:39:35,041] Trial 6100 pruned. Pruned at step 7 with metric 0.5827
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6101 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.968980721019636e-05
  Dropout: 0.18557018591237345
================================================================================

[I 2025-11-11 21:40:30,255] Trial 6101 pruned. OOM: roberta-base bs=64 len=352
[I 2025-11-11 21:40:30,898] Trial 6102 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)

[OOM] Trial 6101 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 254.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6103 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.3008374214142334e-05
  Dropout: 0.42096585216933746
================================================================================

[I 2025-11-11 21:44:59,735] Trial 6103 pruned. Pruned at step 9 with metric 0.5451

================================================================================
TRIAL 6104 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 6.0165675278495806e-06
  Dropout: 0.21774323205332033
================================================================================

[I 2025-11-11 22:21:37,220] Trial 6104 finished with value: 0.7041847041847041 and parameters: {'seed': 63541, 'model.name': 'xlm-roberta-base', 'tok.max_length': 352, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.0165675278495806e-06, 'optim.weight_decay': 0.00057265604077649, 'optim.beta1': 0.808748694788097, 'optim.beta2': 0.958907005221383, 'optim.eps': 1.2662097047647541e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.14470262934282277, 'train.clip_grad': 0.5207118619117619, 'model.dropout': 0.21774323205332033, 'model.attn_dropout': 0.024268870040381425, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9431031989784615, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.24910991356568743, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.06460291986956103, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 36 (patience=20)

================================================================================
TRIAL 6105 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 6.736988672318843e-06
  Dropout: 0.252163582753163
================================================================================

[I 2025-11-11 22:35:02,868] Trial 6094 pruned. Pruned at step 29 with metric 0.6298
[I 2025-11-11 22:35:03,537] Trial 6106 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6107 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 8.608467949580298e-06
  Dropout: 0.3364940109569357
================================================================================

[I 2025-11-11 23:03:14,070] Trial 6107 pruned. Pruned at step 15 with metric 0.5208
[W 2025-11-11 23:03:14,665] The parameter `tok.doc_stride` in Trial#6108 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6108 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 8.465403100890757e-06
  Dropout: 0.44135517638523636
================================================================================

[I 2025-11-11 23:08:37,584] Trial 6108 pruned. Pruned at step 14 with metric 0.5878
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6109 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.0252926384534317e-05
  Dropout: 0.10189716049358516
================================================================================

[I 2025-11-11 23:20:11,056] Trial 6109 pruned. Pruned at step 12 with metric 0.5731
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6110 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 6.55849679740833e-06
  Dropout: 0.475923590566542
================================================================================

[I 2025-11-11 23:35:39,922] Trial 6105 pruned. Pruned at step 27 with metric 0.6285

================================================================================
TRIAL 6111 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.394048595686665e-05
  Dropout: 0.1993571794338284
================================================================================

[I 2025-11-11 23:37:59,896] Trial 6111 pruned. Pruned at step 8 with metric 0.5173
[I 2025-11-11 23:38:00,538] Trial 6112 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6113 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 5.980170529440093e-06
  Dropout: 0.3268538952888969
================================================================================

[I 2025-11-11 23:39:01,074] Trial 6110 pruned. Pruned at step 14 with metric 0.6042
[I 2025-11-11 23:39:01,734] Trial 6114 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-11 23:39:02,373] Trial 6115 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6116 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.1002487421837365e-05
  Dropout: 0.09673905462331994
================================================================================

[I 2025-11-12 00:02:47,007] Trial 6116 pruned. Pruned at step 27 with metric 0.6125
[W 2025-11-12 00:02:47,644] The parameter `tok.doc_stride` in Trial#6117 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6117 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 7.003692216218097e-06
  Dropout: 0.301839718282667
================================================================================

[I 2025-11-12 00:28:53,115] Trial 6113 finished with value: 0.7041847041847041 and parameters: {'seed': 6992, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.980170529440093e-06, 'optim.weight_decay': 0.00022006897229390307, 'optim.beta1': 0.8114175074638325, 'optim.beta2': 0.9586205701015575, 'optim.eps': 3.841188657552924e-09, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.12448435925192955, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.5313784309172719, 'model.dropout': 0.3268538952888969, 'model.attn_dropout': 0.016183925895258064, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8966658772958829, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.4868157929280357, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.01660900009725555, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 00:28:53,765] Trial 6118 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 51 (patience=20)

================================================================================
TRIAL 6119 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 3.3755788811334595e-05
  Dropout: 0.20220849366302684
================================================================================

[I 2025-11-12 00:40:52,791] Trial 6117 pruned. Pruned at step 12 with metric 0.6242
[I 2025-11-12 00:40:53,529] Trial 6120 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6121 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 9.244498678852159e-06
  Dropout: 0.016188595936365402
================================================================================

[I 2025-11-12 00:40:59,772] Trial 6121 pruned. OOM: bert-base-uncased bs=64 len=288
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 6121 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 138.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6122 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.688765858093929e-06
  Dropout: 0.43916910865936387
================================================================================

[I 2025-11-12 00:41:06,356] Trial 6122 pruned. OOM: roberta-base bs=48 len=352
[W 2025-11-12 00:41:07,030] The parameter `tok.doc_stride` in Trial#6123 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 00:41:07,997] Trial 6119 pruned. OOM: roberta-large bs=12 len=384
[I 2025-11-12 00:41:08,681] Trial 6124 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)

[OOM] Trial 6122 exceeded GPU memory:
  Model: roberta-base
  Batch size: 48 (effective: 48 with grad_accum=1)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 60.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6123 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.4178768029522566e-05
  Dropout: 0.4011285357290256
================================================================================


[OOM] Trial 6119 exceeded GPU memory:
  Model: roberta-large
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 60.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.

/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6125 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.0695109640095136e-05
  Dropout: 0.30250631961441254
================================================================================

[I 2025-11-12 01:07:27,116] Trial 6125 finished with value: 0.7028160760001234 and parameters: {'seed': 51963, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.0695109640095136e-05, 'optim.weight_decay': 2.722772485778334e-06, 'optim.beta1': 0.9047266546087127, 'optim.beta2': 0.9823635119795012, 'optim.eps': 1.7335253345610202e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.14260227693626917, 'train.clip_grad': 1.027877858092058, 'model.dropout': 0.30250631961441254, 'model.attn_dropout': 0.23307311420115617, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9762888831427669, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.45107056996400574, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.696474709506236, 'loss.cls.alpha': 0.29189296016819094, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-12 01:07:27,759] The parameter `tok.doc_stride` in Trial#6126 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 01:07:27,820] Trial 6126 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-12 01:07:28,437] Trial 6127 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-12 01:07:29,057] Trial 6128 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-12 01:07:29,680] Trial 6129 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-12 01:07:30,291] The parameter `tok.doc_stride` in Trial#6130 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 6130 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.8391177166764142e-05
  Dropout: 0.39010945120470925
================================================================================

[I 2025-11-12 01:08:13,639] Trial 6123 finished with value: 0.7099315461394318 and parameters: {'seed': 63360, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.4178768029522566e-05, 'optim.weight_decay': 4.369585554685951e-06, 'optim.beta1': 0.9009087848948855, 'optim.beta2': 0.9758386460455611, 'optim.eps': 3.4392539330368146e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.16808014483075662, 'train.clip_grad': 0.9769086853878579, 'model.dropout': 0.4011285357290256, 'model.attn_dropout': 0.22712948928114987, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8748264034129933, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'silu', 'head.dropout': 0.4184901283172326, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.9629358563390467, 'loss.cls.alpha': 0.29814203443766685, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6131 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.123901456391729e-05
  Dropout: 0.1595804057628516
================================================================================

[I 2025-11-12 01:09:56,356] Trial 6131 pruned. Pruned at step 9 with metric 0.5676
[I 2025-11-12 01:09:56,995] Trial 6132 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6133 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 3.585235661273205e-05
  Dropout: 0.1540079149703792
================================================================================

[I 2025-11-12 01:14:36,784] Trial 6133 pruned. Pruned at step 9 with metric 0.6036
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6134 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 6.090559735009527e-05
  Dropout: 0.41753847739854677
================================================================================

[I 2025-11-12 01:15:27,188] Trial 6130 pruned. Pruned at step 6 with metric 0.5823

================================================================================
TRIAL 6135 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.1809312803348085e-05
  Dropout: 0.4207193597485093
================================================================================

[I 2025-11-12 01:19:02,744] Trial 6135 pruned. Pruned at step 7 with metric 0.5945
[W 2025-11-12 01:19:03,370] The parameter `tok.doc_stride` in Trial#6136 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6136 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.756275759379717e-05
  Dropout: 0.3520344262153494
================================================================================

[I 2025-11-12 01:21:35,974] Trial 6136 pruned. Pruned at step 9 with metric 0.5857

================================================================================
TRIAL 6137 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.1787403799862905e-05
  Dropout: 0.4130466509215797
================================================================================

[I 2025-11-12 02:02:22,327] Trial 6134 finished with value: 0.43989071038251365 and parameters: {'seed': 13817, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 6.090559735009527e-05, 'optim.weight_decay': 0.034485849057500316, 'optim.beta1': 0.8195239052241361, 'optim.beta2': 0.9668273995277996, 'optim.eps': 3.449233351204642e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.0464109228900243, 'train.clip_grad': 1.3707947848465676, 'model.dropout': 0.41753847739854677, 'model.attn_dropout': 0.12519734887499723, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9680081211759853, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.09656282210271458, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.23705205214895, 'loss.cls.alpha': 0.362621236259947, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 6138 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.1029833175773326e-05
  Dropout: 0.08063127273147196
================================================================================

[I 2025-11-12 02:10:32,964] Trial 6138 pruned. Pruned at step 9 with metric 0.6250

================================================================================
TRIAL 6139 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 9.508185248075463e-06
  Dropout: 0.40723330235724436
================================================================================

[I 2025-11-12 02:18:09,625] Trial 6137 pruned. Pruned at step 28 with metric 0.5695
[I 2025-11-12 02:18:10,286] Trial 6140 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-12 02:18:10,846] The parameter `tok.doc_stride` in Trial#6141 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6141 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.9709714577240763e-05
  Dropout: 0.1354877409778461
================================================================================

[I 2025-11-12 02:22:34,368] Trial 6141 pruned. Pruned at step 11 with metric 0.5603
[I 2025-11-12 02:22:35,026] Trial 6142 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6143 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.975428025405017e-06
  Dropout: 0.3740938539085834
================================================================================

[I 2025-11-12 02:29:41,426] Trial 6143 pruned. Pruned at step 27 with metric 0.5923
[I 2025-11-12 02:29:42,075] Trial 6144 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-12 02:29:42,680] Trial 6145 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6146 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 8.423148498907756e-06
  Dropout: 0.15856624756094662
================================================================================

[I 2025-11-12 02:39:55,258] Trial 6146 pruned. Pruned at step 6 with metric 0.6159
[W 2025-11-12 02:39:55,919] The parameter `tok.doc_stride` in Trial#6147 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6147 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.349420641269493e-05
  Dropout: 0.45399153818418275
================================================================================

[I 2025-11-12 02:45:30,359] Trial 6147 pruned. Pruned at step 16 with metric 0.5371
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6148 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.6236254576589998e-05
  Dropout: 0.052790519622300525
================================================================================

[I 2025-11-12 02:51:14,550] Trial 6148 pruned. Pruned at step 9 with metric 0.6398
[I 2025-11-12 02:51:15,184] Trial 6149 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6150 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 9.684104508530237e-06
  Dropout: 0.25840897289655107
================================================================================

[I 2025-11-12 02:57:44,766] Trial 6150 pruned. Pruned at step 8 with metric 0.6197

================================================================================
TRIAL 6151 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.3025130059904425e-06
  Dropout: 0.4751248877852054
================================================================================

[I 2025-11-12 03:02:20,434] Trial 6151 pruned. Pruned at step 9 with metric 0.5688
[I 2025-11-12 03:02:21,072] Trial 6152 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6153 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 9.951574469182097e-06
  Dropout: 0.31941182834458265
================================================================================

[I 2025-11-12 03:12:50,921] Trial 6153 finished with value: 0.6825343388095879 and parameters: {'seed': 63373, 'model.name': 'bert-base-uncased', 'tok.max_length': 320, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 64, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.951574469182097e-06, 'optim.weight_decay': 4.813034900537204e-06, 'optim.beta1': 0.8912704827609058, 'optim.beta2': 0.9694700621319554, 'optim.eps': 2.2864935984224626e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.178106595582796, 'train.clip_grad': 0.7626354372543787, 'model.dropout': 0.31941182834458265, 'model.attn_dropout': 0.27572413863216316, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9771114374492347, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.4658891591502619, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.968357472851296, 'loss.cls.alpha': 0.26449014837334583, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 03:12:51,551] Trial 6154 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6155 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 9.371665283315256e-06
  Dropout: 0.36372494834797536
================================================================================

[I 2025-11-12 03:19:08,152] Trial 6139 finished with value: 0.6779160382101559 and parameters: {'seed': 58200, 'model.name': 'bert-large-uncased', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.508185248075463e-06, 'optim.weight_decay': 0.04513497882864128, 'optim.beta1': 0.9158956505316078, 'optim.beta2': 0.9830544828956183, 'optim.eps': 9.278511865897128e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18101092553873166, 'train.clip_grad': 0.7771206790036442, 'model.dropout': 0.40723330235724436, 'model.attn_dropout': 0.28548393736999644, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8924337767810743, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.10354951611989746, 'loss.cls.type': 'focal', 'loss.cls.gamma': 1.1246878366762116, 'loss.cls.alpha': 0.6171118124337011, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 03:19:08,819] Trial 6156 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6157 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.3689948205822591e-05
  Dropout: 0.34854583077757967
================================================================================

[I 2025-11-12 03:34:05,778] Trial 6157 pruned. Pruned at step 31 with metric 0.5968
[I 2025-11-12 03:34:06,430] Trial 6158 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6159 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1275633688074683e-05
  Dropout: 0.36848121424665226
================================================================================

[I 2025-11-12 03:34:24,315] Trial 6155 pruned. Pruned at step 11 with metric 0.6176
[I 2025-11-12 03:34:25,215] Trial 6160 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-12 03:34:25,839] Trial 6161 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6162 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.601607189638204e-05
  Dropout: 0.09040633757874923
================================================================================

[I 2025-11-12 03:39:04,205] Trial 6159 pruned. Pruned at step 7 with metric 0.5804

================================================================================
TRIAL 6163 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.888229150196337e-05
  Dropout: 0.08586304422111454
================================================================================

[I 2025-11-12 03:56:05,986] Trial 6162 pruned. Pruned at step 11 with metric 0.6082
[W 2025-11-12 03:56:06,617] The parameter `tok.doc_stride` in Trial#6164 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6164 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 5.003988612636323e-06
  Dropout: 0.3902621719318388
================================================================================

[I 2025-11-12 03:57:27,999] Trial 6163 pruned. Pruned at step 10 with metric 0.5676
[I 2025-11-12 03:57:28,908] Trial 6165 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6166 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.757415267159693e-06
  Dropout: 0.4508961365913612
================================================================================

[I 2025-11-12 04:11:26,431] Trial 6166 pruned. Pruned at step 19 with metric 0.5620
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6167 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 9.839907564284228e-06
  Dropout: 0.17011318470437173
================================================================================

[I 2025-11-12 04:15:22,304] Trial 6164 pruned. Pruned at step 14 with metric 0.5485

================================================================================
TRIAL 6168 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 6.974286000275366e-06
  Dropout: 0.2980796712592735
================================================================================

[I 2025-11-12 04:24:08,394] Trial 6167 pruned. Pruned at step 13 with metric 0.6341
[W 2025-11-12 04:24:09,033] The parameter `tok.doc_stride` in Trial#6169 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 04:24:09,093] Trial 6169 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6170 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.019686469179361e-05
  Dropout: 0.47766562272318364
================================================================================

[I 2025-11-12 04:27:18,721] Trial 6170 pruned. Pruned at step 11 with metric 0.6112
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6171 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.449937411783e-06
  Dropout: 0.37072117066026844
================================================================================

[I 2025-11-12 04:36:46,678] Trial 6171 pruned. Pruned at step 16 with metric 0.6207

================================================================================
TRIAL 6172 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.2419274505245252e-05
  Dropout: 0.4846672837398429
================================================================================

[I 2025-11-12 04:44:20,854] Trial 6168 pruned. Pruned at step 27 with metric 0.6687
[I 2025-11-12 04:44:21,763] Trial 6173 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-12 04:44:22,336] The parameter `tok.doc_stride` in Trial#6174 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 04:44:22,392] Trial 6174 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-12 04:44:22,965] The parameter `tok.doc_stride` in Trial#6175 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6175 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.991704385752944e-06
  Dropout: 0.11137189992627854
================================================================================

[I 2025-11-12 04:47:32,867] Trial 6172 pruned. Pruned at step 9 with metric 0.5629
[I 2025-11-12 04:47:33,543] Trial 6176 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6177 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0160015034171762e-05
  Dropout: 0.46374328601879133
================================================================================

[I 2025-11-12 04:53:08,194] Trial 6175 finished with value: 0.6717899455651617 and parameters: {'seed': 51792, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.991704385752944e-06, 'optim.weight_decay': 0.001091187839114736, 'optim.beta1': 0.8409176560643569, 'optim.beta2': 0.9690452807765219, 'optim.eps': 4.380615626324579e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.04994301634752649, 'train.clip_grad': 1.4992729403911342, 'model.dropout': 0.11137189992627854, 'model.attn_dropout': 0.1897819797695863, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8590787209471786, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.030414269833494288, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.265085910307707, 'loss.cls.alpha': 0.354911809897135, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 6178 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.398653283915741e-05
  Dropout: 0.22526583211958096
================================================================================

[I 2025-11-12 05:10:35,081] Trial 6177 finished with value: 0.6648777027235433 and parameters: {'seed': 49731, 'model.name': 'bert-base-uncased', 'tok.max_length': 288, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.0160015034171762e-05, 'optim.weight_decay': 1.31319297388883e-05, 'optim.beta1': 0.9216855091023869, 'optim.beta2': 0.9716458360693632, 'optim.eps': 1.309652158072705e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.13643022443875022, 'train.clip_grad': 0.9966837844956475, 'model.dropout': 0.46374328601879133, 'model.attn_dropout': 0.1171895130233262, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9274773974618775, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 256, 'head.activation': 'gelu', 'head.dropout': 0.39687090166592853, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.0406586336392357, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 31 (patience=20)

================================================================================
TRIAL 6179 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.225564930654298e-06
  Dropout: 0.10884773032013431
================================================================================

[I 2025-11-12 05:30:22,370] Trial 6179 pruned. Pruned at step 10 with metric 0.6343
[I 2025-11-12 05:30:23,016] Trial 6180 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6181 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 7.227453714477899e-06
  Dropout: 0.4328146931205986
================================================================================

[I 2025-11-12 05:41:49,114] Trial 6181 pruned. Pruned at step 9 with metric 0.6607
[W 2025-11-12 05:41:49,735] The parameter `tok.doc_stride` in Trial#6182 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6182 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 8.945419293471777e-06
  Dropout: 0.3958055921198828
================================================================================

[I 2025-11-12 05:46:58,967] Trial 6178 finished with value: 0.6272727272727272 and parameters: {'seed': 33791, 'model.name': 'bert-large-uncased', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.398653283915741e-05, 'optim.weight_decay': 5.0298202886419404e-06, 'optim.beta1': 0.8808354563910047, 'optim.beta2': 0.9891669233836736, 'optim.eps': 6.03167035888066e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1765499041451501, 'train.clip_grad': 0.6971180137914643, 'model.dropout': 0.22526583211958096, 'model.attn_dropout': 0.0389311073326778, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8137806637787113, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.41582857357648556, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.01859803581059895, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 05:46:59,634] Trial 6183 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6184 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 1.067245626718284e-05
  Dropout: 0.4336105931832239
================================================================================

[I 2025-11-12 05:49:20,588] Trial 6182 pruned. Pruned at step 13 with metric 0.6038

================================================================================
TRIAL 6185 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 5.509138245721141e-06
  Dropout: 0.34398807155289696
================================================================================

[I 2025-11-12 05:56:10,744] Trial 6184 pruned. Pruned at step 7 with metric 0.6125
[I 2025-11-12 05:56:11,409] Trial 6186 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6187 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.811974519489792e-05
  Dropout: 0.085845644852166
================================================================================

[I 2025-11-12 06:16:02,140] Trial 6187 pruned. Pruned at step 13 with metric 0.5780

================================================================================
TRIAL 6188 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3105200790246894e-05
  Dropout: 0.3199304579713257
================================================================================

[I 2025-11-12 06:25:16,714] Trial 6188 finished with value: 0.6903209166943001 and parameters: {'seed': 49339, 'model.name': 'bert-base-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.3105200790246894e-05, 'optim.weight_decay': 8.93733318422477e-06, 'optim.beta1': 0.8081099081915771, 'optim.beta2': 0.9728554894098673, 'optim.eps': 4.8406868404646534e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.11309470799490703, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.8662584046927264, 'model.dropout': 0.3199304579713257, 'model.attn_dropout': 0.0429521209203057, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9277535822744535, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.3531080808917316, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.02201443650541125, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-12 06:25:17,319] The parameter `tok.doc_stride` in Trial#6189 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 06:25:17,380] Trial 6189 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[W 2025-11-12 06:25:17,957] The parameter `tok.doc_stride` in Trial#6190 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 6190 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.73514329066969e-06
  Dropout: 0.41828148724349057
================================================================================

[I 2025-11-12 06:29:59,719] Trial 6185 pruned. Pruned at step 17 with metric 0.6476
[W 2025-11-12 06:30:00,368] The parameter `tok.doc_stride` in Trial#6191 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6191 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 4.5787830763990844e-05
  Dropout: 0.0008451206766622171
================================================================================

[I 2025-11-12 06:37:27,093] Trial 6191 pruned. Pruned at step 9 with metric 0.6527
[I 2025-11-12 06:37:27,753] Trial 6192 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-12 06:37:28,351] The parameter `tok.doc_stride` in Trial#6193 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6193 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 2.290282574316082e-05
  Dropout: 0.33846616452611566
================================================================================

[I 2025-11-12 06:47:25,284] Trial 6190 finished with value: 0.7304995617879053 and parameters: {'seed': 57156, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.73514329066969e-06, 'optim.weight_decay': 0.194155493089009, 'optim.beta1': 0.914668800029517, 'optim.beta2': 0.9933305230935701, 'optim.eps': 5.4109744725756976e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.13966050123250656, 'train.clip_grad': 0.7109265933860112, 'model.dropout': 0.41828148724349057, 'model.attn_dropout': 0.09195997693053412, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9539861335962321, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.2918298926718918, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.360796140194977, 'loss.cls.alpha': 0.2328342217339539, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 52 (patience=20)

================================================================================
TRIAL 6194 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.2837229821528285e-05
  Dropout: 0.3114089928877997
================================================================================

[I 2025-11-12 07:07:20,048] Trial 6194 finished with value: 0.7090322580645161 and parameters: {'seed': 44732, 'model.name': 'roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.2837229821528285e-05, 'optim.weight_decay': 1.6769760022407515e-05, 'optim.beta1': 0.8358677691964141, 'optim.beta2': 0.973473789252912, 'optim.eps': 8.090783677538624e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.14662428619293905, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.0969063195113486, 'model.dropout': 0.3114089928877997, 'model.attn_dropout': 0.047453602230943935, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8996045793896172, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.44621216753816906, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.0764648324251187, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-12 07:07:20,671] The parameter `tok.doc_stride` in Trial#6195 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 43 (patience=20)

================================================================================
TRIAL 6195 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 16
  Learning rate: 2.230283425467587e-05
  Dropout: 0.13122374363825345
================================================================================

[I 2025-11-12 07:20:55,876] Trial 6193 pruned. Pruned at step 7 with metric 0.4489
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6196 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.668184785823147e-06
  Dropout: 0.3899372284363064
================================================================================

[I 2025-11-12 07:29:45,281] Trial 6195 pruned. Pruned at step 27 with metric 0.5941
[I 2025-11-12 07:29:45,942] Trial 6197 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6198 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.3614693816032938e-05
  Dropout: 0.40219308935826925
================================================================================

[I 2025-11-12 07:35:59,199] Trial 6198 pruned. Pruned at step 27 with metric 0.6321
[W 2025-11-12 07:35:59,808] The parameter `tok.doc_stride` in Trial#6199 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6199 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.2469501850695807e-05
  Dropout: 0.07409570070392124
================================================================================

[I 2025-11-12 07:41:23,510] Trial 6199 pruned. Pruned at step 10 with metric 0.5325
[I 2025-11-12 07:41:24,181] Trial 6200 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6201 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.46402219288742e-05
  Dropout: 0.2719905409329858
================================================================================

[I 2025-11-12 07:49:11,673] Trial 6201 pruned. Pruned at step 18 with metric 0.5283
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6202 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.3749527942595738e-05
  Dropout: 0.49722935670820706
================================================================================

[I 2025-11-12 07:51:39,126] Trial 6202 pruned. Pruned at step 9 with metric 0.6151
[I 2025-11-12 07:51:39,760] Trial 6203 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6204 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.397994477725275e-05
  Dropout: 0.0021871391904506174
================================================================================

[I 2025-11-12 07:55:39,470] Trial 6204 pruned. Pruned at step 10 with metric 0.6112
[I 2025-11-12 07:55:40,106] Trial 6205 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-12 07:55:40,685] The parameter `tok.doc_stride` in Trial#6206 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6206 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.5601892487298792e-05
  Dropout: 0.1062243829404262
================================================================================

[I 2025-11-12 07:58:04,584] Trial 6206 pruned. Pruned at step 9 with metric 0.5208
[I 2025-11-12 07:58:05,247] Trial 6207 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-12 07:58:05,831] The parameter `tok.doc_stride` in Trial#6208 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6208 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.2488660364235767e-05
  Dropout: 0.48127935389791415
================================================================================

[I 2025-11-12 08:01:26,326] Trial 6208 pruned. Pruned at step 13 with metric 0.6038

================================================================================
TRIAL 6209 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.5902312561882922e-05
  Dropout: 0.054236228637137236
================================================================================

[I 2025-11-12 08:03:27,186] Trial 6196 pruned. Pruned at step 10 with metric 0.6264
[W 2025-11-12 08:03:27,891] The parameter `tok.doc_stride` in Trial#6210 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6210 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 6.772000038011032e-06
  Dropout: 0.22469951124359888
================================================================================

[I 2025-11-12 08:06:28,083] Trial 6209 pruned. Pruned at step 10 with metric 0.5089
[I 2025-11-12 08:06:28,739] Trial 6211 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6212 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.3715001439654214e-05
  Dropout: 0.3540318738300243
================================================================================

[I 2025-11-12 08:19:36,212] Trial 6212 pruned. Pruned at step 27 with metric 0.6462
[W 2025-11-12 08:19:36,845] The parameter `tok.doc_stride` in Trial#6213 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6213 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3258520908297168e-05
  Dropout: 0.4702070503278011
================================================================================

[I 2025-11-12 08:22:23,573] Trial 6213 pruned. Pruned at step 10 with metric 0.5349
[I 2025-11-12 08:22:24,217] Trial 6214 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-12 08:22:24,842] Trial 6215 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-12 08:22:25,460] Trial 6216 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6217 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.2055867215784382e-05
  Dropout: 0.03346112470659417
================================================================================

[I 2025-11-12 08:24:10,201] Trial 6217 pruned. Pruned at step 10 with metric 0.5923
[I 2025-11-12 08:24:10,854] Trial 6218 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6219 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3139267940592677e-05
  Dropout: 0.4746958299512181
================================================================================

[I 2025-11-12 08:29:29,980] Trial 6219 pruned. Pruned at step 18 with metric 0.5243
[I 2025-11-12 08:29:30,637] Trial 6220 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-12 08:29:31,256] Trial 6221 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-12 08:29:31,872] Trial 6222 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6223 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.4098036460482977e-05
  Dropout: 0.4375421820146054
================================================================================

[I 2025-11-12 08:29:43,819] Trial 6210 pruned. Pruned at step 10 with metric 0.6618
[I 2025-11-12 08:29:44,494] Trial 6224 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6225 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.926870988914354e-05
  Dropout: 0.2543702718194021
================================================================================

[I 2025-11-12 08:39:15,255] Trial 6225 pruned. Pruned at step 9 with metric 0.6388
[I 2025-11-12 08:39:15,927] Trial 6226 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-12 08:39:16,541] Trial 6227 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6228 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.3724569666877123e-05
  Dropout: 0.46906416476857477
================================================================================

[I 2025-11-12 08:54:25,885] Trial 6223 pruned. Pruned at step 27 with metric 0.5659
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6229 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.6615977255358717e-05
  Dropout: 0.20319365844981632
================================================================================

[I 2025-11-12 08:54:32,202] Trial 6229 pruned. OOM: roberta-base bs=64 len=256
[I 2025-11-12 08:54:33,833] Trial 6228 pruned. OOM: microsoft/deberta-v3-large bs=12 len=160
[W 2025-11-12 08:54:34,536] The parameter `tok.doc_stride` in Trial#6231 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 6229 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 256 with grad_accum=4)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 82.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6230 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 3.168825747479916e-05
  Dropout: 0.18727578278882492
================================================================================


[OOM] Trial 6228 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 24 with grad_accum=2)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 42.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6231 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.567514574860314e-05
  Dropout: 0.21943943714359776
================================================================================

[I 2025-11-12 08:59:59,164] Trial 6231 pruned. Pruned at step 9 with metric 0.5780
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6232 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.1374608397857265e-05
  Dropout: 0.03150736099205435
================================================================================

[I 2025-11-12 09:03:13,493] Trial 6232 pruned. Pruned at step 7 with metric 0.5688
[I 2025-11-12 09:03:14,141] Trial 6233 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6234 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.3172661003813964e-05
  Dropout: 0.4298755258301874
================================================================================

[I 2025-11-12 09:07:56,466] Trial 6234 pruned. Pruned at step 9 with metric 0.5512
[W 2025-11-12 09:07:57,114] The parameter `tok.doc_stride` in Trial#6235 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6235 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 8.729353845847699e-06
  Dropout: 0.21392827500556302
================================================================================

[I 2025-11-12 09:11:26,537] Trial 6235 pruned. Pruned at step 7 with metric 0.5603
[W 2025-11-12 09:11:27,157] The parameter `tok.doc_stride` in Trial#6236 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6236 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.174676401307825e-05
  Dropout: 0.04428480134799729
================================================================================

[I 2025-11-12 09:15:48,510] Trial 6236 pruned. Pruned at step 9 with metric 0.5788
[W 2025-11-12 09:15:49,124] The parameter `tok.doc_stride` in Trial#6237 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6237 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.695361071670857e-06
  Dropout: 0.45116047690755057
================================================================================

[I 2025-11-12 09:23:38,206] Trial 6237 pruned. Pruned at step 17 with metric 0.6232
[I 2025-11-12 09:23:38,852] Trial 6238 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-12 09:23:39,448] The parameter `tok.doc_stride` in Trial#6239 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6239 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3696553120166411e-05
  Dropout: 0.004190150603247872
================================================================================

[I 2025-11-12 09:23:48,271] Trial 6230 finished with value: 0.7197156138911676 and parameters: {'seed': 46348, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 3.168825747479916e-05, 'optim.weight_decay': 1.5483045773924459e-06, 'optim.beta1': 0.8426200640398589, 'optim.beta2': 0.9960663741026863, 'optim.eps': 9.519984378198666e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.18386030527207656, 'sched.poly_power': 0.8435137479694824, 'train.clip_grad': 0.7332867304545849, 'model.dropout': 0.18727578278882492, 'model.attn_dropout': 0.14378058649992656, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9553972004885306, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.45368426059050093, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.317159114656341, 'loss.cls.alpha': 0.34017755853358544, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 6240 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.3114925510061032e-05
  Dropout: 0.32504948584761834
================================================================================

[I 2025-11-12 09:29:28,427] Trial 6239 pruned. Pruned at step 10 with metric 0.5314

================================================================================
TRIAL 6241 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.5259109100820554e-05
  Dropout: 0.19254693516522112
================================================================================

[I 2025-11-12 09:38:59,684] Trial 6241 pruned. Pruned at step 16 with metric 0.5211
[W 2025-11-12 09:39:00,312] The parameter `tok.doc_stride` in Trial#6242 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6242 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.209803640869803e-06
  Dropout: 0.3937538292944111
================================================================================

[I 2025-11-12 09:42:03,371] Trial 6240 pruned. Pruned at step 32 with metric 0.6148

================================================================================
TRIAL 6243 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 6.871569275245363e-06
  Dropout: 0.4484542143148365
================================================================================

[I 2025-11-12 09:52:21,063] Trial 6243 pruned. Pruned at step 7 with metric 0.5508
[I 2025-11-12 09:52:21,724] Trial 6244 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6245 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.1460010066032652e-05
  Dropout: 0.37156751985943925
================================================================================

[I 2025-11-12 09:52:26,947] Trial 6242 pruned. Pruned at step 10 with metric 0.6083

================================================================================
TRIAL 6246 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 3.113997287223044e-05
  Dropout: 0.4846919109560733
================================================================================

[I 2025-11-12 09:54:52,008] Trial 6245 pruned. Pruned at step 9 with metric 0.6082
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6247 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.3752029021758388e-05
  Dropout: 0.06188885621787721
================================================================================

[I 2025-11-12 10:18:14,908] Trial 6246 finished with value: 0.45187165775401067 and parameters: {'seed': 42909, 'model.name': 'xlm-roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 3.113997287223044e-05, 'optim.weight_decay': 0.04943247312572445, 'optim.beta1': 0.8164839078064489, 'optim.beta2': 0.9529531778698912, 'optim.eps': 4.0246420767825073e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.04247717697113171, 'sched.poly_power': 1.3313205292370056, 'train.clip_grad': 1.26713158277218, 'model.dropout': 0.4846919109560733, 'model.attn_dropout': 0.15881039170975186, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9923828367434233, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.1588557963690157, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8133707731504787, 'loss.cls.alpha': 0.1747937656394944, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 6248 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 16
  Learning rate: 9.762727811010536e-06
  Dropout: 0.13618444946761377
================================================================================

[I 2025-11-12 10:18:22,755] Trial 6248 pruned. OOM: microsoft/deberta-v3-large bs=16 len=160
[I 2025-11-12 10:18:22,926] Trial 6247 pruned. OOM: microsoft/deberta-v3-large bs=12 len=192

[OOM] Trial 6247 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 36 with grad_accum=3)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 32.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6248 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 16 (effective: 32 with grad_accum=2)
  Max length: 160
  Error: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 32.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6250 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.122191576785085e-05
  Dropout: 0.01733126809807121
================================================================================


================================================================================
TRIAL 6249 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.598908247094523e-05
  Dropout: 0.2849866076457614
================================================================================

[I 2025-11-12 10:29:07,169] Trial 6249 pruned. Pruned at step 10 with metric 0.5486
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6251 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.202047098907504e-06
  Dropout: 0.4288611534891236
================================================================================

[I 2025-11-12 10:31:33,857] Trial 6250 pruned. Pruned at step 10 with metric 0.6215
[W 2025-11-12 10:31:34,515] The parameter `tok.doc_stride` in Trial#6252 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6252 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.527882680718955e-06
  Dropout: 0.4825606064574055
================================================================================

[I 2025-11-12 10:37:17,207] Trial 6251 pruned. Pruned at step 20 with metric 0.6002

================================================================================
TRIAL 6253 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 4.2164683321577136e-05
  Dropout: 0.22778650720677548
================================================================================

[I 2025-11-12 10:41:25,231] Trial 6253 pruned. Pruned at step 9 with metric 0.6250
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6254 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 5.6319571027921275e-06
  Dropout: 0.3568842882073904
================================================================================

[I 2025-11-12 10:47:34,667] Trial 6252 pruned. Pruned at step 11 with metric 0.6197
[W 2025-11-12 10:47:35,543] The parameter `tok.doc_stride` in Trial#6255 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6255 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 32
  Learning rate: 1.8468782905735843e-05
  Dropout: 0.15888606142424355
================================================================================

[I 2025-11-12 10:53:46,301] Trial 6255 pruned. Pruned at step 9 with metric 0.5444

================================================================================
TRIAL 6256 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.904103154555575e-05
  Dropout: 0.04357705659754106
================================================================================

[I 2025-11-12 10:58:30,988] Trial 6254 pruned. Pruned at step 9 with metric 0.5461
[I 2025-11-12 10:58:31,729] Trial 6257 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-12 10:58:32,366] Trial 6258 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6259 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.690333718128008e-06
  Dropout: 0.3198700904058339
================================================================================

[I 2025-11-12 11:02:16,423] Trial 6256 finished with value: 0.6826625386996904 and parameters: {'seed': 41681, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 1.904103154555575e-05, 'optim.weight_decay': 0.007158375305097732, 'optim.beta1': 0.9149418488161811, 'optim.beta2': 0.9832132264113836, 'optim.eps': 2.2394258448070754e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0671656053889225, 'sched.cosine_cycles': 4, 'train.clip_grad': 0.5488783649136955, 'model.dropout': 0.04357705659754106, 'model.attn_dropout': 0.17555657762805257, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8949642786836589, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.18768326287607834, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.988398478599311, 'loss.cls.alpha': 0.41671416077659235, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 6260 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.429800511011616e-05
  Dropout: 0.3350034377190984
================================================================================

[I 2025-11-12 11:02:47,410] Trial 6259 pruned. Pruned at step 11 with metric 0.6428
[I 2025-11-12 11:02:48,081] Trial 6261 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6262 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.2650744554934234e-05
  Dropout: 0.040134216981577875
================================================================================

[I 2025-11-12 11:16:49,626] Trial 6262 pruned. Pruned at step 27 with metric 0.5746
[I 2025-11-12 11:16:50,312] Trial 6263 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6264 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.011654836095446e-05
  Dropout: 0.46463339859777897
================================================================================

[I 2025-11-12 11:27:39,045] Trial 6264 pruned. Pruned at step 27 with metric 0.6508

================================================================================
TRIAL 6265 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.434953139165194e-05
  Dropout: 0.033522490181043224
================================================================================

[I 2025-11-12 11:33:48,544] Trial 6265 pruned. Pruned at step 9 with metric 0.6082
[W 2025-11-12 11:33:49,162] The parameter `tok.doc_stride` in Trial#6266 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6266 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 5.191970191321377e-06
  Dropout: 0.15117348530949815
================================================================================

[I 2025-11-12 11:36:00,640] Trial 6260 finished with value: 0.6107215892491966 and parameters: {'seed': 14342, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 32, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.429800511011616e-05, 'optim.weight_decay': 0.07819677156667174, 'optim.beta1': 0.8514657393824926, 'optim.beta2': 0.9765730804780022, 'optim.eps': 1.0873105997061365e-07, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.15010302160257744, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.5090384617588832, 'model.dropout': 0.3350034377190984, 'model.attn_dropout': 0.13469954996483222, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8022972508336298, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.09696115301216202, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.576038037154802, 'loss.cls.alpha': 0.3277208509912073, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 6267 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.0655913644569775e-06
  Dropout: 0.3363072833503817
================================================================================

[I 2025-11-12 11:43:28,348] Trial 6267 pruned. Pruned at step 13 with metric 0.5804
[I 2025-11-12 11:43:29,010] Trial 6268 pruned. Pruned: Large model with bsz=24, accum=8 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-12 11:43:29,632] Trial 6269 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6270 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.1457421427445362e-05
  Dropout: 0.2572551209814606
================================================================================

[I 2025-11-12 11:46:15,974] Trial 6270 pruned. Pruned at step 11 with metric 0.6151
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6271 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.471251226839684e-05
  Dropout: 0.3776679349618375
================================================================================

[I 2025-11-12 11:53:13,344] Trial 6271 finished with value: 0.7054302422723475 and parameters: {'seed': 5167, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.471251226839684e-05, 'optim.weight_decay': 0.0005613302460541697, 'optim.beta1': 0.8937403483593236, 'optim.beta2': 0.9950942885365827, 'optim.eps': 1.3855022620208753e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.1107921100194392, 'train.clip_grad': 1.0853091137346198, 'model.dropout': 0.3776679349618375, 'model.attn_dropout': 0.08224440454048881, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8800270142759467, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.338327440990574, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1820881542135312, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 37 (patience=20)

================================================================================
TRIAL 6272 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 9.965245565621233e-06
  Dropout: 0.33211813501121
================================================================================

[I 2025-11-12 11:58:22,347] Trial 6272 pruned. Pruned at step 11 with metric 0.5557
[I 2025-11-12 11:58:23,002] Trial 6273 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-12 11:58:23,585] The parameter `tok.doc_stride` in Trial#6274 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6274 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.297208635901845e-06
  Dropout: 0.15021401377517718
================================================================================

[I 2025-11-12 12:11:57,761] Trial 6274 pruned. Pruned at step 45 with metric 0.6009
[W 2025-11-12 12:11:58,393] The parameter `tok.doc_stride` in Trial#6275 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6275 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.481096511311674e-05
  Dropout: 0.16446177417254693
================================================================================

[I 2025-11-12 12:14:51,087] Trial 6275 pruned. Pruned at step 10 with metric 0.6254
[I 2025-11-12 12:14:51,774] Trial 6276 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-12 12:14:52,408] Trial 6277 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-12 12:14:53,044] Trial 6278 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6279 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.1844206850782115e-05
  Dropout: 0.16023084643255758
================================================================================

[I 2025-11-12 12:19:16,032] Trial 6279 pruned. Pruned at step 6 with metric 0.5486
[W 2025-11-12 12:19:16,691] The parameter `tok.doc_stride` in Trial#6280 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6280 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.819670425748774e-05
  Dropout: 0.43821580386521664
================================================================================

[I 2025-11-12 12:30:09,821] Trial 6266 finished with value: 0.657129639514608 and parameters: {'seed': 4365, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 5.191970191321377e-06, 'optim.weight_decay': 0.00012865479508931667, 'optim.beta1': 0.8134089011254357, 'optim.beta2': 0.9914835725985315, 'optim.eps': 1.4938859409746263e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.19933244197509042, 'train.clip_grad': 0.8301412317743124, 'model.dropout': 0.15117348530949815, 'model.attn_dropout': 0.0564519382494561, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8240723955604047, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'silu', 'head.dropout': 0.48746668616236427, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.519729463902323, 'loss.cls.alpha': 0.1680007733647632, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 12:30:10,504] Trial 6281 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-12 12:30:11,090] The parameter `tok.doc_stride` in Trial#6282 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 44 (patience=20)

================================================================================
TRIAL 6282 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 2.769586569667644e-05
  Dropout: 0.2968324958214393
================================================================================

[I 2025-11-12 12:38:10,987] Trial 6282 pruned. Pruned at step 27 with metric 0.6254
[W 2025-11-12 12:38:11,620] The parameter `tok.doc_stride` in Trial#6283 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6283 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.5616617744819575e-05
  Dropout: 0.47450027095574937
================================================================================

[I 2025-11-12 12:43:50,518] Trial 6280 finished with value: 0.7051773729626079 and parameters: {'seed': 51063, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.819670425748774e-05, 'optim.weight_decay': 0.10198418155107299, 'optim.beta1': 0.8335316460980448, 'optim.beta2': 0.9759576913414942, 'optim.eps': 2.413816177406976e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.14423859587920523, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.5834477143213435, 'model.dropout': 0.43821580386521664, 'model.attn_dropout': 0.12478672567864452, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.885834659704032, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.016647650122244936, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.677111670946869, 'loss.cls.alpha': 0.5230002520874235, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-12 12:43:51,161] The parameter `tok.doc_stride` in Trial#6284 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 56 (patience=20)

================================================================================
TRIAL 6284 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 2.3347613847283946e-05
  Dropout: 0.4148149938743524
================================================================================

[I 2025-11-12 12:45:13,604] Trial 6283 pruned. Pruned at step 27 with metric 0.5240
[W 2025-11-12 12:45:14,289] The parameter `tok.doc_stride` in Trial#6285 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6285 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.4137475977688985e-06
  Dropout: 0.47987351266469175
================================================================================

[I 2025-11-12 12:49:14,676] Trial 6284 pruned. Pruned at step 27 with metric 0.6095
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6286 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 6.615520860183074e-06
  Dropout: 0.4530000258471861
================================================================================

[I 2025-11-12 12:52:23,479] Trial 6285 pruned. Pruned at step 8 with metric 0.5652
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6287 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.354545206350203e-05
  Dropout: 0.04187553223199519
================================================================================

[I 2025-11-12 12:56:22,788] Trial 6287 pruned. Pruned at step 9 with metric 0.6216
[I 2025-11-12 12:56:23,435] Trial 6288 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6289 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.249457952979972e-06
  Dropout: 0.49294395824808473
================================================================================

[I 2025-11-12 13:03:41,343] Trial 6289 pruned. Pruned at step 19 with metric 0.5725
[I 2025-11-12 13:03:41,996] Trial 6290 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6291 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 5.2516263011081325e-06
  Dropout: 0.45514238822109726
================================================================================

[I 2025-11-12 13:03:50,021] Trial 6291 pruned. OOM: roberta-base bs=64 len=384
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 6291 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 512 with grad_accum=8)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 98.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6292 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 9.426217344796043e-06
  Dropout: 0.4197816091312135
================================================================================

[I 2025-11-12 13:03:54,628] Trial 6292 pruned. OOM: roberta-base bs=64 len=384

[OOM] Trial 6292 exceeded GPU memory:
  Model: roberta-base
  Batch size: 64 (effective: 128 with grad_accum=2)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 116.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proc
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6293 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 7.905277915893e-06
  Dropout: 0.4665896652993628
================================================================================

[I 2025-11-12 13:18:36,181] Trial 6286 pruned. Pruned at step 12 with metric 0.5248
[W 2025-11-12 13:18:36,917] The parameter `tok.doc_stride` in Trial#6294 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 13:18:36,976] Trial 6294 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6295 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 5.137855911367883e-06
  Dropout: 0.32970966077382763
================================================================================

[I 2025-11-12 13:19:49,164] Trial 6293 finished with value: 0.7528333735230287 and parameters: {'seed': 10761, 'model.name': 'xlm-roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 7.905277915893e-06, 'optim.weight_decay': 3.164046509764634e-06, 'optim.beta1': 0.8930349896194496, 'optim.beta2': 0.9759785813011277, 'optim.eps': 5.158993204102459e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.197786344061161, 'train.clip_grad': 1.3562647966024124, 'model.dropout': 0.4665896652993628, 'model.attn_dropout': 0.2064617669567387, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9646352959908319, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.49388278267860825, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.030089331758345123, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6296 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.4040799199345863e-05
  Dropout: 0.47374317597568677
================================================================================

[I 2025-11-12 13:29:37,901] Trial 6296 pruned. Pruned at step 12 with metric 0.6474

================================================================================
TRIAL 6297 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 5.919921759340343e-06
  Dropout: 0.380164954286874
================================================================================

[I 2025-11-12 13:29:43,275] Trial 6295 pruned. Pruned at step 15 with metric 0.6038
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6298 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 8.948680548905005e-06
  Dropout: 0.4668961775691549
================================================================================

[I 2025-11-12 13:44:24,707] Trial 6297 finished with value: 0.6762848119437266 and parameters: {'seed': 14262, 'model.name': 'xlm-roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.919921759340343e-06, 'optim.weight_decay': 0.0047011260137457865, 'optim.beta1': 0.8720452741378116, 'optim.beta2': 0.9697623068160507, 'optim.eps': 1.8307999147039467e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1221183708085071, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.25390543367996193, 'model.dropout': 0.380164954286874, 'model.attn_dropout': 0.028240741413732114, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9557110835890396, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.1672429801918344, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.026750662705905467, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 29 (patience=20)

================================================================================
TRIAL 6299 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 7.355583350711571e-06
  Dropout: 0.44692045974539096
================================================================================

[I 2025-11-12 13:48:33,469] Trial 6299 pruned. Pruned at step 15 with metric 0.5371

================================================================================
TRIAL 6300 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 7.605567890624721e-05
  Dropout: 0.004603067060292676
================================================================================

[I 2025-11-12 14:00:22,061] Trial 6300 finished with value: 0.42896935933147634 and parameters: {'seed': 879, 'model.name': 'xlm-roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 7.605567890624721e-05, 'optim.weight_decay': 3.608107813987908e-05, 'optim.beta1': 0.872932755035088, 'optim.beta2': 0.9870300274314988, 'optim.eps': 3.550491212659187e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.1611879533354953, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.6532874884273451, 'model.dropout': 0.004603067060292676, 'model.attn_dropout': 0.19175769959893324, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8509948425691758, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'gelu', 'head.dropout': 0.4468897431310245, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.001967129647724157, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 6301 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 5.760098300901295e-06
  Dropout: 0.43838045113321583
================================================================================

[I 2025-11-12 14:05:45,565] Trial 6301 pruned. Pruned at step 9 with metric 0.6042

================================================================================
TRIAL 6302 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 7.5743357135988395e-06
  Dropout: 0.38358670463934463
================================================================================

[I 2025-11-12 14:35:31,864] Trial 6298 pruned. Pruned at step 27 with metric 0.5884
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6303 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 2.7666927333298993e-05
  Dropout: 0.49228850616265224
================================================================================

[I 2025-11-12 14:35:43,735] Trial 6303 pruned. OOM: microsoft/deberta-v3-large bs=12 len=192
[I 2025-11-12 14:35:43,986] Trial 6302 pruned. OOM: bert-large-uncased bs=16 len=224
[W 2025-11-12 14:35:44,768] The parameter `tok.doc_stride` in Trial#6305 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 14:35:44,833] Trial 6304 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

[OOM] Trial 6303 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 192
  Error: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6302 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 16 (effective: 16 with grad_accum=1)
  Max length: 224
  Error: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 40.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6305 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.7909218973328243e-05
  Dropout: 0.4948313669653565
================================================================================


================================================================================
TRIAL 6306 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.3532190263785726e-05
  Dropout: 0.4098148982380045
================================================================================

[I 2025-11-12 14:40:18,851] Trial 6305 pruned. Pruned at step 7 with metric 0.5375

================================================================================
TRIAL 6307 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 6.863793622140742e-06
  Dropout: 0.45414689096964755
================================================================================

[I 2025-11-12 14:43:22,651] Trial 6306 pruned. Pruned at step 11 with metric 0.6252
[I 2025-11-12 14:43:23,584] Trial 6308 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-12 14:43:24,218] Trial 6309 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6310 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 8.83209050511766e-06
  Dropout: 0.4237980393095346
================================================================================

[I 2025-11-12 14:51:52,822] Trial 6310 pruned. Pruned at step 9 with metric 0.6231

================================================================================
TRIAL 6311 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 5.504468943185355e-05
  Dropout: 0.3014827770990648
================================================================================

[I 2025-11-12 15:01:50,656] Trial 6307 pruned. Pruned at step 17 with metric 0.6027
[I 2025-11-12 15:01:51,582] Trial 6312 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6313 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.0398629921763933e-05
  Dropout: 0.47192749415542007
================================================================================

[I 2025-11-12 15:12:58,856] Trial 6313 pruned. Pruned at step 8 with metric 0.6107

================================================================================
TRIAL 6314 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.4000572420193483e-05
  Dropout: 0.42345885804034056
================================================================================

[I 2025-11-12 15:30:16,576] Trial 6311 finished with value: 0.4474393530997305 and parameters: {'seed': 57738, 'model.name': 'bert-large-uncased', 'tok.max_length': 320, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.504468943185355e-05, 'optim.weight_decay': 3.74644516836092e-06, 'optim.beta1': 0.8520179568957054, 'optim.beta2': 0.9855030912122714, 'optim.eps': 2.1629646784160562e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.17863573002698616, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.9045366574827105, 'model.dropout': 0.3014827770990648, 'model.attn_dropout': 0.19132746608401696, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8907535585622482, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.3742505635674187, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.9539083783861275, 'loss.cls.alpha': 0.7287267532223277, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 15:30:17,263] Trial 6315 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-12 15:30:17,851] The parameter `tok.doc_stride` in Trial#6316 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 6316 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.949613740625948e-05
  Dropout: 0.056812144365018996
================================================================================

[I 2025-11-12 15:39:32,859] Trial 6316 finished with value: 0.6942521702550767 and parameters: {'seed': 37698, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 3.949613740625948e-05, 'optim.weight_decay': 0.0001163440430865101, 'optim.beta1': 0.8199113980968095, 'optim.beta2': 0.9824081977249514, 'optim.eps': 9.561022872659357e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.10643397662069375, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.3577134860099902, 'model.dropout': 0.056812144365018996, 'model.attn_dropout': 0.15047540125908915, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9251859844776554, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'gelu', 'head.dropout': 0.08337180954145539, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.03380530341479468, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 34 (patience=20)

================================================================================
TRIAL 6317 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.2439426860730494e-05
  Dropout: 0.4161003827753782
================================================================================

[I 2025-11-12 15:47:16,133] Trial 6317 pruned. Pruned at step 27 with metric 0.6338
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6318 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.051427294378216e-06
  Dropout: 0.36199857816811065
================================================================================

[I 2025-11-12 15:54:14,712] Trial 6314 pruned. Pruned at step 12 with metric 0.5731
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6319 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 12
  Learning rate: 7.636298405937214e-06
  Dropout: 0.4957946906094838
================================================================================

[I 2025-11-12 15:59:47,938] Trial 6318 pruned. Pruned at step 22 with metric 0.6081
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6320 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.848069314029055e-05
  Dropout: 0.25260398077902557
================================================================================

[I 2025-11-12 16:20:17,235] Trial 6319 pruned. Pruned at step 13 with metric 0.6616

================================================================================
TRIAL 6321 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 2.4985170823668198e-05
  Dropout: 0.48574483761184517
================================================================================

[I 2025-11-12 16:24:37,641] Trial 6320 pruned. Pruned at step 11 with metric 0.6107

================================================================================
TRIAL 6322 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 3.08680070798871e-05
  Dropout: 0.1708319645668538
================================================================================

[I 2025-11-12 16:29:34,128] Trial 6321 pruned. Pruned at step 10 with metric 0.6117
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6323 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 3.2627253407227335e-05
  Dropout: 0.44694719443820113
================================================================================

[I 2025-11-12 16:38:27,104] Trial 6322 pruned. Pruned at step 17 with metric 0.6082
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6324 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.183006687635383e-06
  Dropout: 0.45726380587828486
================================================================================

[I 2025-11-12 16:38:48,189] Trial 6323 pruned. Pruned at step 12 with metric 0.6037
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6325 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 2.0857745486460585e-05
  Dropout: 0.3646831044081479
================================================================================

[I 2025-11-12 16:45:05,831] Trial 6325 pruned. Pruned at step 7 with metric 0.5945
[I 2025-11-12 16:45:06,518] Trial 6326 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6327 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 2.4210367839241238e-05
  Dropout: 0.3767166630491231
================================================================================

[I 2025-11-12 16:59:36,175] Trial 6324 pruned. Pruned at step 30 with metric 0.6383

================================================================================
TRIAL 6328 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.0750521404009951e-05
  Dropout: 0.34683966119235166
================================================================================

[I 2025-11-12 17:05:35,046] Trial 6328 pruned. Pruned at step 7 with metric 0.5734
[W 2025-11-12 17:05:35,668] The parameter `tok.doc_stride` in Trial#6329 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6329 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.1272739011572614e-06
  Dropout: 0.44030713580756153
================================================================================

[I 2025-11-12 17:11:29,461] Trial 6327 pruned. Pruned at step 12 with metric 0.5629
[I 2025-11-12 17:11:30,156] Trial 6330 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[W 2025-11-12 17:11:30,756] The parameter `tok.doc_stride` in Trial#6331 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6331 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.57737049151898e-05
  Dropout: 0.46425617147931575
================================================================================

[I 2025-11-12 17:13:24,403] Trial 6329 pruned. Pruned at step 10 with metric 0.5656
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6332 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 1.2272257913890068e-05
  Dropout: 0.08994804333794088
================================================================================

[I 2025-11-12 17:22:57,581] Trial 6331 pruned. Pruned at step 9 with metric 0.5917

================================================================================
TRIAL 6333 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 5.815165994092416e-06
  Dropout: 0.440098442919492
================================================================================

[I 2025-11-12 17:28:04,778] Trial 6332 pruned. Pruned at step 14 with metric 0.6643
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6334 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 8.103233000133719e-06
  Dropout: 0.42972890161759125
================================================================================

[I 2025-11-12 17:53:34,512] Trial 6333 finished with value: 0.6215384615384616 and parameters: {'seed': 6212, 'model.name': 'xlm-roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.815165994092416e-06, 'optim.weight_decay': 0.001553375802304464, 'optim.beta1': 0.90465741240584, 'optim.beta2': 0.980301695129157, 'optim.eps': 3.058455021838909e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.0676095449609892, 'train.clip_grad': 1.3693663529302598, 'model.dropout': 0.440098442919492, 'model.attn_dropout': 0.1806332632427264, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9938753439789757, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.33303317561072276, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.996332891047549, 'loss.cls.alpha': 0.46239807722042375, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 38 (patience=20)

================================================================================
TRIAL 6335 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 4.460266537362171e-05
  Dropout: 0.36586442015854576
================================================================================

[I 2025-11-12 17:55:08,475] Trial 6334 pruned. Pruned at step 27 with metric 0.6056
[I 2025-11-12 17:55:09,233] Trial 6336 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6337 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 1.6729233204171826e-05
  Dropout: 0.4564180693902149
================================================================================

[I 2025-11-12 18:09:23,833] Trial 6335 pruned. Pruned at step 13 with metric 0.6167

================================================================================
TRIAL 6338 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 3.1621098407884314e-05
  Dropout: 0.020533865631453374
================================================================================

[I 2025-11-12 18:12:52,275] Trial 6338 pruned. Pruned at step 7 with metric 0.5656
[I 2025-11-12 18:12:52,936] Trial 6339 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[I 2025-11-12 18:12:53,571] Trial 6340 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-12 18:12:54,205] Trial 6341 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-12 18:12:54,830] Trial 6342 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6343 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 7.4245747592799855e-06
  Dropout: 0.3572506592306345
================================================================================

[I 2025-11-12 18:15:50,035] Trial 6337 finished with value: 0.6868055555555556 and parameters: {'seed': 46982, 'model.name': 'bert-base-uncased', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 16, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.6729233204171826e-05, 'optim.weight_decay': 0.0024787265026936962, 'optim.beta1': 0.9348524378859953, 'optim.beta2': 0.9991739543606922, 'optim.eps': 9.638198437465743e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1454191218913113, 'train.clip_grad': 0.5253140129479988, 'model.dropout': 0.4564180693902149, 'model.attn_dropout': 0.105453646962515, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.981294783213614, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.01905234975522882, 'loss.cls.type': 'focal', 'loss.cls.gamma': 2.4654287207092564, 'loss.cls.alpha': 0.49643776644928245, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 18:15:50,719] Trial 6344 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-12 18:15:51,305] The parameter `tok.doc_stride` in Trial#6345 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 18:15:51,365] Trial 6345 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-12 18:15:51,990] Trial 6346 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 6347 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.919540110208473e-05
  Dropout: 0.4615201961944358
================================================================================

[I 2025-11-12 18:24:37,918] Trial 6347 pruned. Pruned at step 27 with metric 0.6472
[I 2025-11-12 18:24:38,581] Trial 6348 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-12 18:24:39,213] Trial 6349 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
[W 2025-11-12 18:24:39,818] The parameter `tok.doc_stride` in Trial#6350 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6350 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.320186291210966e-06
  Dropout: 0.28306489032622334
================================================================================

[I 2025-11-12 18:29:13,301] Trial 6343 pruned. Pruned at step 11 with metric 0.6121
[W 2025-11-12 18:29:14,193] The parameter `tok.doc_stride` in Trial#6351 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6351 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 1.2274355584119288e-05
  Dropout: 0.46752120346128173
================================================================================

[I 2025-11-12 18:50:18,610] Trial 6351 pruned. Pruned at step 9 with metric 0.5827
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6352 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 6.860423204426261e-06
  Dropout: 0.4605697760765968
================================================================================

[I 2025-11-12 18:57:15,233] Trial 6352 pruned. Pruned at step 10 with metric 0.6139

================================================================================
TRIAL 6353 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.1320537142635521e-05
  Dropout: 0.3938253566628366
================================================================================

[I 2025-11-12 19:02:45,818] Trial 6350 pruned. Pruned at step 31 with metric 0.5927
[I 2025-11-12 19:02:46,514] Trial 6354 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-12 19:02:47,150] Trial 6355 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-12 19:02:47,788] Trial 6356 pruned. Pruned: Large model with bsz=64, accum=8 (effective_batch=512) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6357 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.7382272411737255e-05
  Dropout: 0.17229897093320817
================================================================================

[I 2025-11-12 19:07:18,811] Trial 6357 pruned. Pruned at step 9 with metric 0.6407
[W 2025-11-12 19:07:19,446] The parameter `tok.doc_stride` in Trial#6358 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6358 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.7583014259134063e-05
  Dropout: 0.12755649919538786
================================================================================

[I 2025-11-12 19:09:16,849] Trial 6358 pruned. Pruned at step 9 with metric 0.6117
[W 2025-11-12 19:09:17,477] The parameter `tok.doc_stride` in Trial#6359 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-12 19:09:17,536] Trial 6359 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6360 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 6.928242167802158e-06
  Dropout: 0.38711128043500886
================================================================================

[I 2025-11-12 19:09:25,743] Trial 6353 pruned. OOM: xlm-roberta-base bs=12 len=256
[I 2025-11-12 19:09:27,038] Trial 6360 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384

[OOM] Trial 6353 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 256
  Error: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 30.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6360 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 30.50 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6361 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.1974073550130644e-05
  Dropout: 0.25755597289075305
================================================================================


================================================================================
TRIAL 6362 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 1.0325522519010793e-05
  Dropout: 0.42659073873816694
================================================================================

[I 2025-11-12 19:34:34,266] Trial 6362 pruned. Pruned at step 15 with metric 0.5941
[I 2025-11-12 19:34:34,959] Trial 6363 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6364 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.923127798465931e-05
  Dropout: 0.0066595779545523745
================================================================================

[I 2025-11-12 19:42:54,985] Trial 6364 pruned. Pruned at step 10 with metric 0.5859

================================================================================
TRIAL 6365 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.381847103248612e-05
  Dropout: 0.25374474912235095
================================================================================

[I 2025-11-12 19:53:22,607] Trial 6365 pruned. Pruned at step 21 with metric 0.5603

================================================================================
TRIAL 6366 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.389146254223436e-05
  Dropout: 0.34721035159744873
================================================================================

[I 2025-11-12 20:05:16,191] Trial 6366 finished with value: 0.700900323831892 and parameters: {'seed': 27771, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.389146254223436e-05, 'optim.weight_decay': 0.0018943369154435623, 'optim.beta1': 0.9211787742681204, 'optim.beta2': 0.9683600640384196, 'optim.eps': 3.0501255512055842e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.03194483992014988, 'train.clip_grad': 1.3348867809474043, 'model.dropout': 0.34721035159744873, 'model.attn_dropout': 0.26913843770852464, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9172937777035632, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4270120542927836, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.68489521376443, 'loss.cls.alpha': 0.4478315995386035, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 20:05:16,872] Trial 6367 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-12 20:05:17,551] Trial 6368 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-12 20:05:18,186] Trial 6369 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-12 20:05:18,826] Trial 6370 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
[I 2025-11-12 20:05:19,456] Trial 6371 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-12 20:05:20,090] Trial 6372 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[I 2025-11-12 20:05:20,736] Trial 6373 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 6374 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 6.287297561779603e-05
  Dropout: 0.4867085013504772
================================================================================

[I 2025-11-12 20:16:44,282] Trial 6374 finished with value: 0.43989071038251365 and parameters: {'seed': 25113, 'model.name': 'xlm-roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.287297561779603e-05, 'optim.weight_decay': 0.07212247143147027, 'optim.beta1': 0.9342998401607192, 'optim.beta2': 0.9757592939548515, 'optim.eps': 8.999019844783902e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.11675565443391062, 'sched.poly_power': 0.500144401736932, 'train.clip_grad': 0.9560276199561343, 'model.dropout': 0.4867085013504772, 'model.attn_dropout': 0.2759729012101075, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8751981895006236, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.3624257400689185, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.6832395427238485, 'loss.cls.alpha': 0.6004306516456664, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 6375 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.784528350633723e-05
  Dropout: 0.14288053151609986
================================================================================

[I 2025-11-12 20:21:41,147] Trial 6375 pruned. Pruned at step 8 with metric 0.5656
[W 2025-11-12 20:21:41,974] The parameter `tok.doc_stride` in Trial#6376 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6376 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 7.143552091782281e-06
  Dropout: 0.4465631734782197
================================================================================

[I 2025-11-12 20:23:57,541] Trial 6361 finished with value: 0.62558877246948 and parameters: {'seed': 64722, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 352, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.1974073550130644e-05, 'optim.weight_decay': 0.0026498973044989478, 'optim.beta1': 0.9008520460664662, 'optim.beta2': 0.9779345405121974, 'optim.eps': 3.462958206598675e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.092903097685266, 'train.clip_grad': 0.23496778314320033, 'model.dropout': 0.25755597289075305, 'model.attn_dropout': 0.059041592324393904, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8685653071908335, 'head.pooling': 'mean', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.210466094194793, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.151640577574463, 'loss.cls.alpha': 0.38270801826585743, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 23 (patience=20)

================================================================================
TRIAL 6377 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.093407087249372e-05
  Dropout: 0.27339063735148933
================================================================================

[I 2025-11-12 20:28:54,342] Trial 6376 pruned. Pruned at step 12 with metric 0.6372

================================================================================
TRIAL 6378 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 8.892091078525805e-06
  Dropout: 0.23626120863591366
================================================================================

[I 2025-11-12 20:45:19,111] Trial 6378 pruned. Pruned at step 11 with metric 0.6425

================================================================================
TRIAL 6379 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 9.840764406643607e-06
  Dropout: 0.46772474977730016
================================================================================

[I 2025-11-12 20:50:47,196] Trial 6377 pruned. Pruned at step 13 with metric 0.6425
[W 2025-11-12 20:50:47,875] The parameter `tok.doc_stride` in Trial#6380 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6380 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 7.446373543489361e-06
  Dropout: 0.42499064727022967
================================================================================

[I 2025-11-12 21:02:04,910] Trial 6380 pruned. Pruned at step 19 with metric 0.6207

================================================================================
TRIAL 6381 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 1.4531871474959552e-05
  Dropout: 0.38665327331169697
================================================================================

[I 2025-11-12 21:05:45,969] Trial 6379 finished with value: 0.6901699515745967 and parameters: {'seed': 13812, 'model.name': 'xlm-roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 9.840764406643607e-06, 'optim.weight_decay': 0.00033704452802018795, 'optim.beta1': 0.8755090983659407, 'optim.beta2': 0.9637013514557927, 'optim.eps': 1.7259457873088395e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.19271076553731487, 'train.clip_grad': 1.1350719974173644, 'model.dropout': 0.46772474977730016, 'model.attn_dropout': 0.24922173454342567, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9555227122067714, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.4278776964986381, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.04945966762062019, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 33 (patience=20)

================================================================================
TRIAL 6382 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 4.63633558785251e-05
  Dropout: 0.33874852600788474
================================================================================

[I 2025-11-12 21:09:14,011] Trial 6382 pruned. Pruned at step 11 with metric 0.5780
[I 2025-11-12 21:09:14,684] Trial 6383 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6384 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.070108404622658e-06
  Dropout: 0.47126564981153723
================================================================================

[I 2025-11-12 21:14:24,969] Trial 6384 pruned. Pruned at step 27 with metric 0.5474
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6385 - Configuration:
  Model: {'name': 'roberta-large'}
  Batch size: 8
  Learning rate: 8.368735254807469e-06
  Dropout: 0.0838985879978201
================================================================================

[I 2025-11-12 21:19:33,225] Trial 6381 pruned. Pruned at step 12 with metric 0.5639
[I 2025-11-12 21:19:33,917] Trial 6386 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
[W 2025-11-12 21:19:34,504] The parameter `tok.doc_stride` in Trial#6387 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6387 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 9.626048718295707e-06
  Dropout: 0.46992847845855185
================================================================================

[I 2025-11-12 21:25:07,395] Trial 6387 pruned. Pruned at step 11 with metric 0.5886

================================================================================
TRIAL 6388 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.3412490062652293e-05
  Dropout: 0.25550785130316794
================================================================================

[I 2025-11-12 21:44:45,263] Trial 6388 pruned. Pruned at step 14 with metric 0.6583
[I 2025-11-12 21:44:46,112] Trial 6389 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-12 21:44:46,738] Trial 6390 pruned. Pruned: Large model with bsz=32, accum=8 (effective_batch=256) likely causes OOM (24GB GPU limit)
[I 2025-11-12 21:44:47,373] Trial 6391 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)
[I 2025-11-12 21:44:48,006] Trial 6392 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6393 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.1907512213129551e-05
  Dropout: 0.3751923238377556
================================================================================

[I 2025-11-12 21:54:24,033] Trial 6385 finished with value: 0.6592720735388895 and parameters: {'seed': 50371, 'model.name': 'roberta-large', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 8.368735254807469e-06, 'optim.weight_decay': 7.976863101076754e-05, 'optim.beta1': 0.8645479605687056, 'optim.beta2': 0.9750440534003071, 'optim.eps': 3.8906456785970196e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.023285539375177276, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.1629170304529866, 'model.dropout': 0.0838985879978201, 'model.attn_dropout': 0.20073328945927524, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8018564505285625, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.36194688844433837, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.0003541638989898038, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-12 21:54:24,718] Trial 6394 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-12 21:54:25,347] Trial 6395 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6396 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 1.89051621718208e-05
  Dropout: 0.4077661424481572
================================================================================

[I 2025-11-12 22:04:04,985] Trial 6396 pruned. Pruned at step 10 with metric 0.6112
[W 2025-11-12 22:04:05,811] The parameter `tok.doc_stride` in Trial#6397 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6397 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 9.784960732367933e-06
  Dropout: 0.10252942937229112
================================================================================

[I 2025-11-12 22:10:53,906] Trial 6397 finished with value: 0.7041847041847041 and parameters: {'seed': 28149, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 80, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 9.784960732367933e-06, 'optim.weight_decay': 1.2050421757557055e-06, 'optim.beta1': 0.9470114425892859, 'optim.beta2': 0.9696038183657296, 'optim.eps': 1.771290488390055e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.01527716095734627, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.9209072216810746, 'model.dropout': 0.10252942937229112, 'model.attn_dropout': 0.20020043825938458, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8312111413491436, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.31082393402290986, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.032118593466890145, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 28 (patience=20)

================================================================================
TRIAL 6398 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.375803238861369e-05
  Dropout: 0.33275613313733426
================================================================================

[I 2025-11-12 22:16:04,152] Trial 6398 pruned. Pruned at step 6 with metric 0.5880
[W 2025-11-12 22:16:04,788] The parameter `tok.doc_stride` in Trial#6399 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6399 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 5.249144159835845e-06
  Dropout: 0.30618371432935865
================================================================================

[I 2025-11-12 22:20:49,582] Trial 6393 pruned. Pruned at step 30 with metric 0.6167
[I 2025-11-12 22:20:50,516] Trial 6400 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6401 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.269242139379267e-05
  Dropout: 0.0920746916566666
================================================================================

[I 2025-11-12 22:25:00,059] Trial 6401 pruned. Pruned at step 13 with metric 0.5905
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6402 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.1612330070932693e-05
  Dropout: 0.3629677207725489
================================================================================

[I 2025-11-12 22:25:29,471] Trial 6399 pruned. Pruned at step 9 with metric 0.5512
[I 2025-11-12 22:25:30,157] Trial 6403 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6404 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 1.4287410881778632e-05
  Dropout: 0.3821543889469489
================================================================================

[I 2025-11-12 22:40:22,378] Trial 6404 pruned. Pruned at step 11 with metric 0.5474
[I 2025-11-12 22:40:23,076] Trial 6405 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6406 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 1.0788483434063708e-05
  Dropout: 0.050431668425629295
================================================================================

[I 2025-11-12 22:57:12,980] Trial 6406 pruned. Pruned at step 32 with metric 0.6069
[I 2025-11-12 22:57:13,639] Trial 6407 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6408 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.0696888574360902e-05
  Dropout: 0.43280889863527533
================================================================================

[I 2025-11-12 23:05:14,465] Trial 6408 pruned. Pruned at step 14 with metric 0.5884

================================================================================
TRIAL 6409 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.64167963182128e-06
  Dropout: 0.02424871688999234
================================================================================

[I 2025-11-12 23:12:00,696] Trial 6409 pruned. Pruned at step 12 with metric 0.5823
[I 2025-11-12 23:12:01,363] Trial 6410 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-12 23:12:02,007] Trial 6411 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6412 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 4.809776184460042e-05
  Dropout: 0.02012571647293732
================================================================================

[I 2025-11-12 23:25:34,743] Trial 6412 pruned. Pruned at step 27 with metric 0.6124

================================================================================
TRIAL 6413 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 9.09927631240094e-06
  Dropout: 0.38090053087116166
================================================================================

[I 2025-11-12 23:30:32,253] Trial 6413 pruned. Pruned at step 9 with metric 0.5783
[I 2025-11-12 23:30:33,103] Trial 6414 pruned. Pruned: Large model with bsz=32, accum=2 (effective_batch=64) likely causes OOM (24GB GPU limit)
[I 2025-11-12 23:30:33,733] Trial 6415 pruned. Pruned: Large model with bsz=32, accum=3 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6416 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.2548000955706188e-05
  Dropout: 0.09080879292616165
================================================================================

[I 2025-11-12 23:43:28,604] Trial 6402 pruned. Pruned at step 27 with metric 0.6002
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6417 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 1.2957742003430017e-05
  Dropout: 0.3521582625784807
================================================================================

[I 2025-11-12 23:44:17,604] Trial 6416 pruned. Pruned at step 10 with metric 0.5603
[I 2025-11-12 23:44:18,380] Trial 6418 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6419 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 2.185463752598555e-05
  Dropout: 0.020187652769687793
================================================================================

[I 2025-11-12 23:52:03,188] Trial 6419 pruned. Pruned at step 7 with metric 0.6176
[I 2025-11-12 23:52:03,918] Trial 6420 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[W 2025-11-12 23:52:04,511] The parameter `tok.doc_stride` in Trial#6421 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6421 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 6.262646857474453e-06
  Dropout: 0.41079878555655086
================================================================================

[I 2025-11-12 23:57:07,015] Trial 6421 pruned. Pruned at step 17 with metric 0.5859

================================================================================
TRIAL 6422 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.1987126050067305e-05
  Dropout: 0.47495688318970836
================================================================================

[I 2025-11-13 00:01:29,553] Trial 6422 pruned. Pruned at step 23 with metric 0.5786
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6423 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 7.0859216728614215e-06
  Dropout: 0.4307695935635754
================================================================================

[I 2025-11-13 00:04:12,331] Trial 6417 pruned. Pruned at step 27 with metric 0.5957
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6424 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.549419621447441e-05
  Dropout: 0.4331502311492956
================================================================================

[I 2025-11-13 00:04:18,930] Trial 6424 pruned. OOM: microsoft/deberta-v3-large bs=12 len=352
[I 2025-11-13 00:04:19,803] Trial 6425 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
[I 2025-11-13 00:04:20,915] Trial 6423 pruned. OOM: microsoft/deberta-v3-base bs=16 len=128
[W 2025-11-13 00:04:22,020] The parameter `tok.doc_stride` in Trial#6427 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 6424 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 58.62 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6423 exceeded GPU memory:
  Model: microsoft/deberta-v3-base
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 66.62 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6426 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 4.631414991889382e-05
  Dropout: 0.4505987470648965
================================================================================


================================================================================
TRIAL 6427 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 8.610823129261228e-06
  Dropout: 0.49999072648283205
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-13 00:10:48,781] Trial 6427 pruned. Pruned at step 10 with metric 0.5725

================================================================================
TRIAL 6428 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 9.35013393488998e-06
  Dropout: 0.3689109557687261
================================================================================

[I 2025-11-13 00:18:55,321] Trial 6428 pruned. Pruned at step 15 with metric 0.6441
[I 2025-11-13 00:18:56,779] Trial 6429 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6430 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.7255486859902985e-05
  Dropout: 0.3782420579955852
================================================================================

[I 2025-11-13 00:21:51,352] Trial 6430 pruned. Pruned at step 10 with metric 0.5880
[W 2025-11-13 00:21:51,995] The parameter `tok.doc_stride` in Trial#6431 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6431 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.3950776972648505e-05
  Dropout: 0.11306770100532743
================================================================================

[I 2025-11-13 00:35:17,063] Trial 6431 finished with value: 0.4429347826086957 and parameters: {'seed': 31648, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.3950776972648505e-05, 'optim.weight_decay': 0.00016054592833888282, 'optim.beta1': 0.8826050899926144, 'optim.beta2': 0.9837755710673721, 'optim.eps': 2.7230336457601797e-09, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.08009287013304967, 'sched.poly_power': 0.5037147839726541, 'train.clip_grad': 1.4053539534901172, 'model.dropout': 0.11306770100532743, 'model.attn_dropout': 0.27759541898784423, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.8280741306832653, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.3308272324516782, 'loss.cls.type': 'focal', 'loss.cls.gamma': 2.2065695864333135, 'loss.cls.alpha': 0.7126357993702306, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-13 00:35:17,750] Trial 6432 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-13 00:35:18,345] The parameter `tok.doc_stride` in Trial#6433 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 6433 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.6922563443338764e-05
  Dropout: 0.07798462930339367
================================================================================

[I 2025-11-13 00:47:21,429] Trial 6426 finished with value: 0.6654726368159204 and parameters: {'seed': 37378, 'model.name': 'roberta-base', 'tok.max_length': 384, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 4.631414991889382e-05, 'optim.weight_decay': 0.0055620369040488395, 'optim.beta1': 0.9010591692365545, 'optim.beta2': 0.9577028646743556, 'optim.eps': 4.43886548291522e-07, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.07809042081611675, 'sched.poly_power': 1.1202442006791562, 'train.clip_grad': 0.6322283418819213, 'model.dropout': 0.4505987470648965, 'model.attn_dropout': 0.26249741328888, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8860502816796476, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'silu', 'head.dropout': 0.3384320613507622, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.060859564782453045, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 31 (patience=20)

================================================================================
TRIAL 6434 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 7.4744390666970065e-06
  Dropout: 0.4490615180694425
================================================================================

[I 2025-11-13 00:48:08,657] Trial 6433 finished with value: 0.6539667229322401 and parameters: {'seed': 62143, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.6922563443338764e-05, 'optim.weight_decay': 0.08679673772124495, 'optim.beta1': 0.8679679621818636, 'optim.beta2': 0.9928933245958184, 'optim.eps': 7.320390834761298e-07, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.18498252316395675, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.037894954542347814, 'model.dropout': 0.07798462930339367, 'model.attn_dropout': 0.21640433583786126, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.803731499323732, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.09066742622855022, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.995290034794127, 'loss.cls.alpha': 0.5134835551223964, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-13 00:48:09,350] Trial 6435 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 43 (patience=20)

================================================================================
TRIAL 6436 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 9.62884536496479e-06
  Dropout: 0.426277747871233
================================================================================

[I 2025-11-13 00:51:55,624] Trial 6436 pruned. Pruned at step 8 with metric 0.6002
[I 2025-11-13 00:51:56,325] Trial 6437 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6438 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.042565414618217e-05
  Dropout: 0.4052280766801504
================================================================================

[I 2025-11-13 00:55:39,226] Trial 6438 pruned. Pruned at step 15 with metric 0.5905

================================================================================
TRIAL 6439 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 6.246351527579041e-06
  Dropout: 0.40055186898370804
================================================================================

[I 2025-11-13 01:07:29,509] Trial 6439 pruned. Pruned at step 27 with metric 0.5884

================================================================================
TRIAL 6440 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.277106411736068e-05
  Dropout: 0.3519319491566758
================================================================================

[I 2025-11-13 01:08:58,565] Trial 6434 pruned. Pruned at step 14 with metric 0.5827
[I 2025-11-13 01:08:59,264] Trial 6441 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6442 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1579447211742236e-05
  Dropout: 0.44579005078977096
================================================================================

[I 2025-11-13 01:35:55,563] Trial 6442 pruned. Pruned at step 12 with metric 0.5742
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6443 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 6.013899934610862e-06
  Dropout: 0.44020779624552564
================================================================================

[I 2025-11-13 01:36:03,240] Trial 6440 pruned. OOM: bert-base-uncased bs=64 len=320
[I 2025-11-13 01:36:04,643] Trial 6443 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352

[OOM] Trial 6440 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 207.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6444 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.888166935816849e-06
  Dropout: 0.42709979068350795
================================================================================


[OOM] Trial 6443 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 25.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6445 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 1.1882165852069397e-05
  Dropout: 0.40939083284333494
================================================================================

[I 2025-11-13 01:36:14,589] Trial 6444 pruned. OOM: bert-base-uncased bs=12 len=384
[W 2025-11-13 01:36:15,272] The parameter `tok.doc_stride` in Trial#6446 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-13 01:36:15,674] Trial 6445 pruned. OOM: microsoft/deberta-v3-large bs=12 len=288
[I 2025-11-13 01:36:16,681] Trial 6447 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

[OOM] Trial 6444 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 65.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6445 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 12 with grad_accum=1)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 51.81 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6446 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.3258825894103323e-05
  Dropout: 0.2924650020637353
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6448 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.7770385175681763e-05
  Dropout: 0.3201265099687253
================================================================================

[I 2025-11-13 01:50:07,483] Trial 6448 finished with value: 0.6646341463414634 and parameters: {'seed': 61335, 'model.name': 'roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.7770385175681763e-05, 'optim.weight_decay': 0.03444836658208517, 'optim.beta1': 0.8753020893275213, 'optim.beta2': 0.998303388362131, 'optim.eps': 1.8993269439454967e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.18019965785302838, 'train.clip_grad': 0.7639328876070295, 'model.dropout': 0.3201265099687253, 'model.attn_dropout': 0.11769472783446508, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8655612931224299, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.008407708892589752, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.433600640749593, 'loss.cls.alpha': 0.7390787278665639, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 42 (patience=20)

================================================================================
TRIAL 6449 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 5.3167832373849245e-06
  Dropout: 0.4939478368518207
================================================================================

[I 2025-11-13 01:52:55,933] Trial 6446 pruned. Pruned at step 9 with metric 0.5603

================================================================================
TRIAL 6450 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 2.0952991659174487e-05
  Dropout: 0.42661543867532625
================================================================================

[I 2025-11-13 02:02:39,527] Trial 6450 finished with value: 0.6914487731457174 and parameters: {'seed': 51259, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 64, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.0952991659174487e-05, 'optim.weight_decay': 2.788334724996075e-05, 'optim.beta1': 0.8048811364477635, 'optim.beta2': 0.97416149903754, 'optim.eps': 2.2522721757608423e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.17601694291392986, 'sched.poly_power': 1.239465413208629, 'train.clip_grad': 0.8069903912371659, 'model.dropout': 0.42661543867532625, 'model.attn_dropout': 0.08133267823011867, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9760566528230805, 'head.pooling': 'cls', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.4118918871226379, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.057696119069805, 'loss.cls.alpha': 0.3353665840832916, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-13 02:02:40,214] Trial 6451 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6452 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.0686375445880588e-05
  Dropout: 0.47682203104841514
================================================================================

[I 2025-11-13 02:03:50,664] Trial 6449 pruned. Pruned at step 22 with metric 0.6077

================================================================================
TRIAL 6453 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 5.201621870495163e-06
  Dropout: 0.47144027216209333
================================================================================

[I 2025-11-13 02:11:03,633] Trial 6452 pruned. Pruned at step 9 with metric 0.6245

================================================================================
TRIAL 6454 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 5.6909564106634626e-06
  Dropout: 0.40181526807884227
================================================================================

[I 2025-11-13 02:16:20,592] Trial 6454 pruned. Pruned at step 17 with metric 0.6125
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6455 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.782860398793196e-05
  Dropout: 0.26346324480904015
================================================================================

[I 2025-11-13 02:20:41,049] Trial 6455 pruned. Pruned at step 10 with metric 0.5064
[I 2025-11-13 02:20:41,720] Trial 6456 pruned. Pruned: Large model with bsz=48, accum=4 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-13 02:20:42,351] Trial 6457 pruned. Pruned: Large model with bsz=32, accum=4 (effective_batch=128) likely causes OOM (24GB GPU limit)
[I 2025-11-13 02:20:42,995] Trial 6458 pruned. Pruned: Large model with bsz=24, accum=3 (effective_batch=72) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6459 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.19315758257849e-06
  Dropout: 0.4652514731020187
================================================================================

[I 2025-11-13 02:41:06,503] Trial 6459 pruned. Pruned at step 11 with metric 0.5769
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6460 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.9875409887259215e-06
  Dropout: 0.4643550248010022
================================================================================

[I 2025-11-13 02:51:53,432] Trial 6460 pruned. Pruned at step 27 with metric 0.6372
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6461 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 6.03224800400682e-06
  Dropout: 0.1441968934545667
================================================================================

[I 2025-11-13 03:03:34,724] Trial 6461 pruned. Pruned at step 27 with metric 0.6521
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6462 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 9.218180695164178e-06
  Dropout: 0.1397631706772531
================================================================================

[I 2025-11-13 03:03:43,400] Trial 6453 pruned. OOM: xlm-roberta-base bs=12 len=384
[I 2025-11-13 03:03:44,703] Trial 6462 pruned. OOM: microsoft/deberta-v3-large bs=8 len=352

[OOM] Trial 6453 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 65.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6462 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 32 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 77.88 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6463 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.0872768087369365e-05
  Dropout: 0.4743927373473986
================================================================================


================================================================================
TRIAL 6464 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 8.224030766725921e-06
  Dropout: 0.2454984664088878
================================================================================

[I 2025-11-13 03:11:14,356] Trial 6463 pruned. Pruned at step 11 with metric 0.5897
[I 2025-11-13 03:11:15,074] Trial 6465 pruned. Pruned: Large model with bsz=48, accum=1 (effective_batch=48) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6466 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.1135849242810935e-05
  Dropout: 0.37239784675218124
================================================================================

[I 2025-11-13 03:14:48,716] Trial 6464 pruned. Pruned at step 18 with metric 0.6337
[I 2025-11-13 03:14:49,417] Trial 6467 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-13 03:14:50,027] The parameter `tok.doc_stride` in Trial#6468 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6468 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.53428990975708e-05
  Dropout: 0.23840879840562137
================================================================================

[I 2025-11-13 03:24:37,786] Trial 6468 pruned. Pruned at step 23 with metric 0.6407

================================================================================
TRIAL 6469 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.741549289862285e-05
  Dropout: 0.09507900799416139
================================================================================

[I 2025-11-13 03:32:50,920] Trial 6469 pruned. Pruned at step 18 with metric 0.6167
[I 2025-11-13 03:32:51,789] Trial 6470 pruned. Pruned: Large model with bsz=64, accum=2 (effective_batch=128) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6471 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 5.622818109705536e-06
  Dropout: 0.1790017285817581
================================================================================

[I 2025-11-13 03:38:05,424] Trial 6466 finished with value: 0.6693548387096775 and parameters: {'seed': 60648, 'model.name': 'xlm-roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 2.1135849242810935e-05, 'optim.weight_decay': 0.0016746755621498558, 'optim.beta1': 0.849674256851376, 'optim.beta2': 0.9628129967446339, 'optim.eps': 2.0172850800560054e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.010315065718413622, 'sched.cosine_cycles': 3, 'train.clip_grad': 0.7683462842980849, 'model.dropout': 0.37239784675218124, 'model.attn_dropout': 0.10573641780598178, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9240523791904742, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 2048, 'head.activation': 'relu', 'head.dropout': 0.40062955592065913, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.988469660902423, 'loss.cls.alpha': 0.44354313038845, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6472 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.175076109564928e-06
  Dropout: 0.37135486848799587
================================================================================

[I 2025-11-13 04:01:00,102] Trial 6471 pruned. Pruned at step 10 with metric 0.6107

================================================================================
TRIAL 6473 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 1.2483074954625957e-05
  Dropout: 0.009091645360535645
================================================================================

[I 2025-11-13 04:05:24,912] Trial 6473 pruned. Pruned at step 11 with metric 0.5769
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6474 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 24
  Learning rate: 1.038592792354455e-05
  Dropout: 0.36689546187600974
================================================================================

[I 2025-11-13 04:09:35,442] Trial 6474 pruned. Pruned at step 10 with metric 0.6450
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6475 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 7.520893144607696e-06
  Dropout: 0.3691115726910158
================================================================================

[I 2025-11-13 04:17:05,557] Trial 6475 pruned. Pruned at step 9 with metric 0.6200

================================================================================
TRIAL 6476 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 2.640771185496085e-05
  Dropout: 0.4910107002602066
================================================================================

[I 2025-11-13 04:36:39,776] Trial 6472 finished with value: 0.6809049773755655 and parameters: {'seed': 3923, 'model.name': 'xlm-roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 5.175076109564928e-06, 'optim.weight_decay': 0.007059335974521292, 'optim.beta1': 0.8951725211854166, 'optim.beta2': 0.9970829891278672, 'optim.eps': 9.703414037666277e-09, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.1271235688835709, 'train.clip_grad': 0.9349345507251416, 'model.dropout': 0.37135486848799587, 'model.attn_dropout': 0.06289949742492663, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.9332014865909786, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.24116043118441008, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.0032439607536034754, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-13 04:36:40,463] Trial 6477 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 40 (patience=20)

================================================================================
TRIAL 6478 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 9.584036162131773e-06
  Dropout: 0.43260418980616444
================================================================================

[I 2025-11-13 04:48:51,322] Trial 6476 finished with value: 0.43370165745856354 and parameters: {'seed': 17197, 'model.name': 'bert-large-uncased', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.640771185496085e-05, 'optim.weight_decay': 5.969335359733996e-06, 'optim.beta1': 0.8891449977201017, 'optim.beta2': 0.9753501480170396, 'optim.eps': 6.4433512688049955e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.12449958487269223, 'train.clip_grad': 0.9913816015743858, 'model.dropout': 0.4910107002602066, 'model.attn_dropout': 0.17178931186957677, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8893216496386609, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.4763299344246597, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8074479386356055, 'loss.cls.alpha': 0.72232534145102, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 6479 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 3.612742463102223e-05
  Dropout: 0.04751163523177545
================================================================================

[I 2025-11-13 04:53:28,098] Trial 6479 pruned. Pruned at step 9 with metric 0.5839

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 6480 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.735877592634973e-06
  Dropout: 0.35351339050811
================================================================================

[I 2025-11-13 04:59:16,912] Trial 6480 pruned. Pruned at step 16 with metric 0.6134
[I 2025-11-13 04:59:17,771] Trial 6481 pruned. Pruned: Large model with bsz=48, accum=8 (effective_batch=384) likely causes OOM (24GB GPU limit)
[W 2025-11-13 04:59:18,564] The parameter `tok.doc_stride` in Trial#6482 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
[GPU RESET] Performing periodic GPU reset after 50 successful trials
This prevents cumulative memory fragmentation during long HPO runs
================================================================================

[GPU RESET] Complete. Continuing HPO...

================================================================================
TRIAL 6482 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.6360414115336527e-05
  Dropout: 0.044386319904733754
================================================================================

[I 2025-11-13 05:07:03,680] Trial 6478 finished with value: 0.739553924336533 and parameters: {'seed': 18565, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 9.584036162131773e-06, 'optim.weight_decay': 1.4802388430410186e-06, 'optim.beta1': 0.8467734494455753, 'optim.beta2': 0.9754128101196633, 'optim.eps': 1.5861072076475654e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.16944269419884533, 'train.clip_grad': 1.33123604037372, 'model.dropout': 0.43260418980616444, 'model.attn_dropout': 0.14570396763348115, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.981366739747517, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.4086308514258479, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.00014813803768013584, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-13 05:07:04,386] Trial 6483 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 6484 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 16
  Learning rate: 1.671977630136806e-05
  Dropout: 0.4609536339492365
================================================================================

[I 2025-11-13 05:17:24,455] Trial 6482 finished with value: 0.6816770186335404 and parameters: {'seed': 57879, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.6360414115336527e-05, 'optim.weight_decay': 0.004102311508184324, 'optim.beta1': 0.9180169805414385, 'optim.beta2': 0.9834862767375456, 'optim.eps': 1.2339436630011431e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.060536495431697326, 'sched.cosine_cycles': 1, 'train.clip_grad': 0.7999932337060333, 'model.dropout': 0.044386319904733754, 'model.attn_dropout': 0.18434579840785525, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8863559862084379, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.017641483778238314, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.4952747918880203, 'loss.cls.alpha': 0.7611385378003525, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 6485 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 1.5753324011307073e-05
  Dropout: 0.05566388158357688
================================================================================

[I 2025-11-13 05:19:54,114] Trial 6485 pruned. Pruned at step 8 with metric 0.6156

================================================================================
TRIAL 6486 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.3342090731048677e-05
  Dropout: 0.08052986191706905
================================================================================

[I 2025-11-13 05:48:02,386] Trial 6484 pruned. Pruned at step 27 with metric 0.6002
[I 2025-11-13 05:48:03,158] Trial 6487 pruned. Pruned: Large model with bsz=24, accum=2 (effective_batch=48) likely causes OOM (24GB GPU limit)
[W 2025-11-13 05:48:03,774] The parameter `tok.doc_stride` in Trial#6488 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6488 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.0780896149883035e-05
  Dropout: 0.3943832053052669
================================================================================

[I 2025-11-13 06:01:51,456] Trial 6486 finished with value: 0.6930056377730797 and parameters: {'seed': 38827, 'model.name': 'bert-large-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.3342090731048677e-05, 'optim.weight_decay': 8.00982511492101e-05, 'optim.beta1': 0.9081276092154693, 'optim.beta2': 0.9616297402222297, 'optim.eps': 4.133280004230311e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.012347253459434448, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.348932946672046, 'model.dropout': 0.08052986191706905, 'model.attn_dropout': 0.22811315439113455, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8449967421489444, 'head.pooling': 'attn', 'head.layers': 3, 'head.hidden': 512, 'head.activation': 'gelu', 'head.dropout': 0.38525990165961244, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.01790609697072882, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 6489 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.1884810539223633e-05
  Dropout: 0.23274248345350046
================================================================================

[I 2025-11-13 06:01:56,690] Trial 6488 pruned. Pruned at step 27 with metric 0.5780
[W 2025-11-13 06:01:57,373] The parameter `tok.doc_stride` in Trial#6490 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6490 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.1300466632141313e-05
  Dropout: 0.3426172808439536
================================================================================

[I 2025-11-13 06:12:15,634] Trial 6490 pruned. Pruned at step 9 with metric 0.6512
[I 2025-11-13 06:12:16,342] Trial 6491 pruned. Pruned: Large model with bsz=64, accum=4 (effective_batch=256) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6492 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 6.58227939532098e-06
  Dropout: 0.34133216839379793
================================================================================

[I 2025-11-13 07:33:42,704] Trial 6489 pruned. Pruned at step 27 with metric 0.6372

================================================================================
TRIAL 6493 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 6.337811362386686e-06
  Dropout: 0.15673857634469684
================================================================================

[I 2025-11-13 07:41:05,492] Trial 6493 pruned. Pruned at step 14 with metric 0.6290
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6494 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 2.2643111299745777e-05
  Dropout: 0.3696688049021441
================================================================================

[I 2025-11-13 08:06:52,276] Trial 6494 pruned. Pruned at step 7 with metric 0.5593
[W 2025-11-13 08:06:52,973] The parameter `tok.doc_stride` in Trial#6495 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6495 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 3.623764506418893e-05
  Dropout: 0.05364615070441235
================================================================================

[I 2025-11-13 08:21:23,235] Trial 6495 pruned. Pruned at step 27 with metric 0.5155
[W 2025-11-13 08:21:23,873] The parameter `tok.doc_stride` in Trial#6496 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6496 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 3.3184229746754566e-05
  Dropout: 0.3744737263737553
================================================================================

[I 2025-11-13 08:27:26,319] Trial 6496 pruned. Pruned at step 7 with metric 0.5375

================================================================================
TRIAL 6497 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 3.3375897320668424e-05
  Dropout: 0.35275235832898194
================================================================================

[I 2025-11-13 08:46:57,179] Trial 6497 finished with value: 0.43989071038251365 and parameters: {'seed': 53304, 'model.name': 'xlm-roberta-base', 'tok.max_length': 256, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 3.3375897320668424e-05, 'optim.weight_decay': 1.8441172290255747e-05, 'optim.beta1': 0.8202079975179992, 'optim.beta2': 0.9678744261379216, 'optim.eps': 1.7490379370938093e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1797220164998205, 'sched.cosine_cycles': 4, 'train.clip_grad': 1.1775943657607506, 'model.dropout': 0.35275235832898194, 'model.attn_dropout': 0.09600399865339164, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9576152403925864, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.475286176703783, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.94438082631073, 'loss.cls.alpha': 0.6432779554498493, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 6498 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 12
  Learning rate: 1.0594598077194825e-05
  Dropout: 0.4549014746596558
================================================================================

[I 2025-11-13 08:54:16,885] Trial 6492 finished with value: 0.6372265122265123 and parameters: {'seed': 37402, 'model.name': 'bert-large-uncased', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 6.58227939532098e-06, 'optim.weight_decay': 0.0647896192457517, 'optim.beta1': 0.9068084873024063, 'optim.beta2': 0.9529067096683069, 'optim.eps': 2.0947766949673526e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.06969675351943709, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4305502816243034, 'model.dropout': 0.34133216839379793, 'model.attn_dropout': 0.2694150477504965, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.878003672272718, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'gelu', 'head.dropout': 0.3800346935372187, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.8992230601452222, 'loss.cls.alpha': 0.36472194081890685, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-13 08:54:17,550] The parameter `tok.doc_stride` in Trial#6499 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-13 08:54:17,613] Trial 6499 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
EarlyStopping triggered at epoch 67 (patience=20)

================================================================================
TRIAL 6500 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 7.549650426116157e-06
  Dropout: 0.2631448241380788
================================================================================

[I 2025-11-13 09:06:51,176] Trial 6500 pruned. Pruned at step 16 with metric 0.6250

================================================================================
TRIAL 6501 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 64
  Learning rate: 4.308766262690412e-05
  Dropout: 0.23227775222398897
================================================================================

[I 2025-11-13 09:07:57,513] Trial 6498 pruned. Pruned at step 15 with metric 0.5945
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6502 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.3909124446754702e-05
  Dropout: 0.3076584185176115
================================================================================

[I 2025-11-13 09:08:06,319] Trial 6501 pruned. OOM: bert-base-uncased bs=64 len=128

[OOM] Trial 6501 exceeded GPU memory:
  Model: bert-base-uncased
  Batch size: 64 (effective: 192 with grad_accum=3)
  Max length: 128
  Error: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 76.06 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6503 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 5.536165482355923e-06
  Dropout: 0.04905620000393519
================================================================================

[I 2025-11-13 09:08:12,790] Trial 6502 pruned. OOM: microsoft/deberta-v3-large bs=8 len=384
[I 2025-11-13 09:08:13,050] Trial 6503 pruned. OOM: xlm-roberta-base bs=8 len=288
[I 2025-11-13 09:08:13,881] Trial 6504 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-13 09:08:14,271] The parameter `tok.doc_stride` in Trial#6505 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-13 09:08:14,995] Trial 6506 pruned. Pruned: Large model with bsz=48, accum=2 (effective_batch=96) likely causes OOM (24GB GPU limit)

[OOM] Trial 6502 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 48 with grad_accum=6)
  Max length: 384
  Error: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 36.06 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6503 exceeded GPU memory:
  Model: xlm-roberta-base
  Batch size: 8 (effective: 24 with grad_accum=3)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 496.06 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6505 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.426376409101895e-05
  Dropout: 0.4954648319796464
================================================================================


================================================================================
TRIAL 6507 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.426492624158682e-05
  Dropout: 0.15230909993057523
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-13 09:28:03,038] Trial 6507 finished with value: 0.6983277591973245 and parameters: {'seed': 63196, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 16, 'train.grad_accum': 8, 'optim.name': 'adamw', 'optim.lr': 3.426492624158682e-05, 'optim.weight_decay': 0.00026271336805241025, 'optim.beta1': 0.8254894549233408, 'optim.beta2': 0.9913831136880228, 'optim.eps': 8.232675120273768e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.09201644345233181, 'train.clip_grad': 1.2091357808321725, 'model.dropout': 0.15230909993057523, 'model.attn_dropout': 0.21258131969584493, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8480686310741357, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.22399532259789792, 'loss.cls.type': 'ce_label_smooth', 'loss.cls.label_smoothing': 0.013859213804992141, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 51 (patience=20)

================================================================================
TRIAL 6508 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 2.1869994372370435e-05
  Dropout: 0.48242713029307727
================================================================================

[I 2025-11-13 09:34:05,130] Trial 6505 finished with value: 0.6785727943707216 and parameters: {'seed': 55571, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 1.426376409101895e-05, 'optim.weight_decay': 0.0007452212763457826, 'optim.beta1': 0.8505823881505705, 'optim.beta2': 0.9904638177644913, 'optim.eps': 4.1441362901506784e-08, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.14414121448328937, 'train.clip_grad': 0.9453408866904199, 'model.dropout': 0.4954648319796464, 'model.attn_dropout': 0.11403668799517416, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.9458653395854728, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.2430702115061107, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.969469119279965, 'loss.cls.alpha': 0.35086201829886277, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-13 09:34:05,776] The parameter `tok.doc_stride` in Trial#6509 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 6509 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 8.670266037563601e-06
  Dropout: 0.3366898368256155
================================================================================

[I 2025-11-13 09:35:15,776] Trial 6508 pruned. Pruned at step 11 with metric 0.5944
[I 2025-11-13 09:35:16,711] Trial 6510 pruned. Pruned: Large model with bsz=12, accum=8 (effective_batch=96) likely causes OOM (24GB GPU limit)
[W 2025-11-13 09:35:17,318] The parameter `tok.doc_stride` in Trial#6511 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6511 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 16
  Learning rate: 9.537955254031602e-06
  Dropout: 0.4011610352794406
================================================================================

[I 2025-11-13 09:46:22,481] Trial 6509 finished with value: 0.6296965317919074 and parameters: {'seed': 62349, 'model.name': 'bert-base-uncased', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 24, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 8.670266037563601e-06, 'optim.weight_decay': 2.999339866221716e-05, 'optim.beta1': 0.8575767340542227, 'optim.beta2': 0.9845473305791062, 'optim.eps': 8.39810175265987e-09, 'sched.name': 'linear', 'sched.warmup_ratio': 0.15352302938596835, 'train.clip_grad': 1.0088432600335697, 'model.dropout': 0.3366898368256155, 'model.attn_dropout': 0.15288888753617771, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9349826164739984, 'head.pooling': 'mean', 'head.layers': 3, 'head.hidden': 384, 'head.activation': 'relu', 'head.dropout': 0.4155637079883209, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.410165926725061, 'loss.cls.alpha': 0.5839940592498748, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-13 09:46:23,197] Trial 6512 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 44 (patience=20)

================================================================================
TRIAL 6513 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 5.099961501388909e-06
  Dropout: 0.48830886017010966
================================================================================

[I 2025-11-13 09:46:54,262] Trial 6511 pruned. Pruned at step 14 with metric 0.5415

================================================================================
TRIAL 6514 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 1.1214027389892488e-05
  Dropout: 0.45100131667739785
================================================================================

[I 2025-11-13 09:55:58,869] Trial 6513 pruned. Pruned at step 16 with metric 0.5667
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6515 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 32
  Learning rate: 6.538816064817017e-06
  Dropout: 0.40191963772634315
================================================================================

[I 2025-11-13 10:01:20,315] Trial 6515 pruned. Pruned at step 9 with metric 0.6134
[I 2025-11-13 10:01:20,991] Trial 6516 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6517 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 2.10102537277943e-05
  Dropout: 0.057482491387593
================================================================================

[I 2025-11-13 10:21:23,477] Trial 6517 finished with value: 0.7054272134409196 and parameters: {'seed': 46549, 'model.name': 'roberta-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 12, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 2.10102537277943e-05, 'optim.weight_decay': 0.18875580353393126, 'optim.beta1': 0.8370857048033719, 'optim.beta2': 0.9831636479572107, 'optim.eps': 7.120740759991464e-07, 'sched.name': 'cosine', 'sched.warmup_ratio': 0.14268788559733664, 'train.clip_grad': 0.6008425411291142, 'model.dropout': 0.057482491387593, 'model.attn_dropout': 0.19074736191112357, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8712853482561971, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 512, 'head.activation': 'relu', 'head.dropout': 0.09614434645945882, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.304585193028629, 'loss.cls.alpha': 0.310439561244502, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 32 (patience=20)

================================================================================
TRIAL 6518 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 1.071169616050321e-05
  Dropout: 0.10153560200527703
================================================================================

[I 2025-11-13 10:30:39,209] Trial 6518 pruned. Pruned at step 11 with metric 0.5821
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6519 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 1.1221412524134566e-05
  Dropout: 0.38857111117595505
================================================================================

[I 2025-11-13 10:48:59,379] Trial 6519 pruned. Pruned at step 7 with metric 0.5900
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6520 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.147184589912024e-06
  Dropout: 0.34627283222065197
================================================================================

[I 2025-11-13 10:55:41,790] Trial 6514 finished with value: 0.6963371936439537 and parameters: {'seed': 58077, 'model.name': 'bert-large-uncased', 'tok.max_length': 384, 'tok.doc_stride': 48, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 1.1214027389892488e-05, 'optim.weight_decay': 3.1633281985079532e-06, 'optim.beta1': 0.8777705515260472, 'optim.beta2': 0.963151905275913, 'optim.eps': 7.345179807845047e-09, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.13099524806657703, 'sched.cosine_cycles': 3, 'train.clip_grad': 1.017023363217279, 'model.dropout': 0.45100131667739785, 'model.attn_dropout': 0.2305092789001451, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9935150750076281, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.47182978220200766, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.07321337450503608, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-13 10:55:42,458] The parameter `tok.doc_stride` in Trial#6521 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 6521 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 2.787771714827405e-05
  Dropout: 0.4677068165316934
================================================================================

[I 2025-11-13 11:18:24,012] Trial 6520 finished with value: 0.7540231507622812 and parameters: {'seed': 17130, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.147184589912024e-06, 'optim.weight_decay': 7.32559866916216e-05, 'optim.beta1': 0.864486549718811, 'optim.beta2': 0.9903812671530057, 'optim.eps': 2.119651078565999e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.18320850276232595, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.835161814737982, 'model.dropout': 0.34627283222065197, 'model.attn_dropout': 0.06348970212638387, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.8858330342774124, 'head.pooling': 'max', 'head.layers': 3, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.45287249022986853, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.238195681160796, 'loss.cls.alpha': 0.3983478389928671, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-13 11:18:24,693] The parameter `tok.doc_stride` in Trial#6522 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 54 (patience=20)

================================================================================
TRIAL 6522 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.3060526337864498e-05
  Dropout: 0.24622259672899732
================================================================================

[I 2025-11-13 11:23:17,236] Trial 6522 finished with value: 0.7205335101875392 and parameters: {'seed': 44239, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 24, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 1.3060526337864498e-05, 'optim.weight_decay': 1.2585025909849536e-05, 'optim.beta1': 0.8616450174137761, 'optim.beta2': 0.9807158700424536, 'optim.eps': 7.19342935475382e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.16541981617711288, 'sched.poly_power': 1.011644629068362, 'train.clip_grad': 1.0398655937685204, 'model.dropout': 0.24622259672899732, 'model.attn_dropout': 0.0625311656650145, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9848136254660275, 'head.pooling': 'cls', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.36256333427145626, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.012221913888311394, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 6523 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.432268662020531e-06
  Dropout: 0.4339558868280407
================================================================================

[I 2025-11-13 11:28:55,440] Trial 6521 finished with value: 0.6387260085231824 and parameters: {'seed': 46292, 'model.name': 'bert-base-uncased', 'tok.max_length': 128, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.787771714827405e-05, 'optim.weight_decay': 9.05441043074863e-05, 'optim.beta1': 0.8574224314851262, 'optim.beta2': 0.9667684150954131, 'optim.eps': 6.337503247726652e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.13376578646094192, 'train.clip_grad': 1.090140243699685, 'model.dropout': 0.4677068165316934, 'model.attn_dropout': 0.16806023249258997, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.9626878445133313, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'silu', 'head.dropout': 0.49314660085776424, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.037824419607184856, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 6524 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 6.979580555318505e-06
  Dropout: 0.33641048750134556
================================================================================

[I 2025-11-13 11:35:28,883] Trial 6524 pruned. Pruned at step 20 with metric 0.5962
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6525 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.117142742711598e-05
  Dropout: 0.4026482826865968
================================================================================

[I 2025-11-13 11:41:11,582] Trial 6523 finished with value: 0.6450020264544416 and parameters: {'seed': 64253, 'model.name': 'roberta-base', 'tok.max_length': 224, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 6, 'optim.name': 'adamw', 'optim.lr': 5.432268662020531e-06, 'optim.weight_decay': 1.0765004586321671e-06, 'optim.beta1': 0.8717889202252052, 'optim.beta2': 0.9823031573096761, 'optim.eps': 4.966313868152127e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.15293074886581925, 'train.clip_grad': 1.402561843638193, 'model.dropout': 0.4339558868280407, 'model.attn_dropout': 0.14153402236913243, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.981891624437882, 'head.pooling': 'mean', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.43100487626522715, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.009500893364614339, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 25 (patience=20)

================================================================================
TRIAL 6526 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.6833176205386226e-06
  Dropout: 0.34514529955999296
================================================================================

[I 2025-11-13 11:44:02,245] Trial 6525 pruned. Pruned at step 7 with metric 0.6038
[I 2025-11-13 11:44:02,955] Trial 6527 pruned. Pruned: Large model with bsz=12, accum=6 (effective_batch=72) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6528 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 6.577903801626064e-05
  Dropout: 0.010518837967162457
================================================================================

[I 2025-11-13 11:54:47,770] Trial 6528 pruned. Pruned at step 27 with metric 0.6002
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6529 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 7.510123449093908e-06
  Dropout: 0.4851683927494285
================================================================================

[I 2025-11-13 11:56:48,400] Trial 6526 pruned. Pruned at step 13 with metric 0.4761
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6530 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.7917102845049887e-05
  Dropout: 0.19730696395879588
================================================================================

[I 2025-11-13 12:06:08,798] Trial 6529 pruned. Pruned at step 10 with metric 0.6208
[I 2025-11-13 12:06:09,496] Trial 6531 pruned. Pruned: Large model with bsz=32, accum=6 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-13 12:06:10,169] Trial 6532 pruned. Pruned: Large model with bsz=64, accum=6 (effective_batch=384) likely causes OOM (24GB GPU limit)
[I 2025-11-13 12:06:10,851] Trial 6533 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6534 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 12
  Learning rate: 5.434928840145797e-05
  Dropout: 0.024568165998941155
================================================================================

[I 2025-11-13 12:25:28,197] Trial 6534 finished with value: 0.6762848119437266 and parameters: {'seed': 29399, 'model.name': 'bert-base-uncased', 'tok.max_length': 224, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 5.434928840145797e-05, 'optim.weight_decay': 0.0008126356766587998, 'optim.beta1': 0.921654140206791, 'optim.beta2': 0.9729940084318045, 'optim.eps': 4.991995189851633e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.019789081279459816, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.186768957461085, 'model.dropout': 0.024568165998941155, 'model.attn_dropout': 0.10509037768902316, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8135863067034419, 'head.pooling': 'attn', 'head.layers': 4, 'head.hidden': 2048, 'head.activation': 'silu', 'head.dropout': 0.23637204680065205, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.0262708346324901, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 26 (patience=20)

================================================================================
TRIAL 6535 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 7.829288592796522e-06
  Dropout: 0.32708481219225366
================================================================================

[I 2025-11-13 12:25:57,430] Trial 6530 finished with value: 0.7119437939110069 and parameters: {'seed': 65118, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 320, 'tok.doc_stride': 80, 'tok.use_fast': False, 'train.batch_size': 8, 'train.grad_accum': 3, 'optim.name': 'adamw', 'optim.lr': 2.7917102845049887e-05, 'optim.weight_decay': 9.839651960725884e-05, 'optim.beta1': 0.8585822322158311, 'optim.beta2': 0.9779073821582147, 'optim.eps': 2.8544187753105014e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.16278608275105338, 'sched.poly_power': 1.0630213395737576, 'train.clip_grad': 1.4758899979102449, 'model.dropout': 0.19730696395879588, 'model.attn_dropout': 0.048651650804025774, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 2, 'optim.layerwise_lr_decay': 0.9502479640979268, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.48003148848198857, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.013958600501692071, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 24 (patience=20)

================================================================================
TRIAL 6536 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 48
  Learning rate: 5.454762237679018e-06
  Dropout: 0.47462433070697874
================================================================================

[I 2025-11-13 12:36:57,331] Trial 6536 pruned. Pruned at step 24 with metric 0.6134

================================================================================
TRIAL 6537 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 16
  Learning rate: 1.5839603714247642e-05
  Dropout: 0.40185484839046803
================================================================================

[I 2025-11-13 12:50:22,439] Trial 6537 pruned. Pruned at step 27 with metric 0.6583
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6538 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 6.626976279160522e-06
  Dropout: 0.3295119725854351
================================================================================

[I 2025-11-13 13:05:59,227] Trial 6538 pruned. Pruned at step 27 with metric 0.5693

================================================================================
TRIAL 6539 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 5.255156504570535e-06
  Dropout: 0.46736021014712303
================================================================================

[I 2025-11-13 13:09:52,430] Trial 6539 pruned. Pruned at step 11 with metric 0.5729
[I 2025-11-13 13:09:53,130] Trial 6540 pruned. Pruned: Large model with bsz=32, accum=1 (effective_batch=32) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6541 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 12
  Learning rate: 6.8714428736897745e-06
  Dropout: 0.30173577711599564
================================================================================

[I 2025-11-13 13:22:04,881] Trial 6535 finished with value: 0.6871682590233545 and parameters: {'seed': 14523, 'model.name': 'roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 7.829288592796522e-06, 'optim.weight_decay': 2.953523297760051e-05, 'optim.beta1': 0.862839829687379, 'optim.beta2': 0.9886971836700592, 'optim.eps': 5.928000711655408e-08, 'sched.name': 'cosine_restart', 'sched.warmup_ratio': 0.1794118451607702, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9216696766871975, 'model.dropout': 0.32708481219225366, 'model.attn_dropout': 0.03345793573862993, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 6, 'optim.layerwise_lr_decay': 0.8438130671585744, 'head.pooling': 'max', 'head.layers': 2, 'head.hidden': 768, 'head.activation': 'gelu', 'head.dropout': 0.28863708216690753, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.325199682432537, 'loss.cls.alpha': 0.38547757223586854, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 35 (patience=20)

================================================================================
TRIAL 6542 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 1.4401910990553375e-05
  Dropout: 0.46041483721294085
================================================================================

[I 2025-11-13 13:25:33,962] Trial 6541 finished with value: 0.6973394799481756 and parameters: {'seed': 18842, 'model.name': 'xlm-roberta-base', 'tok.max_length': 320, 'tok.doc_stride': 64, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 6.8714428736897745e-06, 'optim.weight_decay': 0.03985094587701845, 'optim.beta1': 0.8894568406572887, 'optim.beta2': 0.9905426666312658, 'optim.eps': 2.827840415822309e-07, 'sched.name': 'linear', 'sched.warmup_ratio': 0.11692889659735226, 'train.clip_grad': 1.3987355230615037, 'model.dropout': 0.30173577711599564, 'model.attn_dropout': 0.19237284323318402, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 0, 'optim.layerwise_lr_decay': 0.8311506884291064, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1024, 'head.activation': 'relu', 'head.dropout': 0.1488088608927523, 'loss.cls.type': 'focal', 'loss.cls.gamma': 3.7615322220289444, 'loss.cls.alpha': 0.3938000028291385, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-13 13:25:34,648] Trial 6543 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 27 (patience=20)

================================================================================
TRIAL 6544 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 6.4664159853577125e-06
  Dropout: 0.35640078708926715
================================================================================

[I 2025-11-13 13:37:01,172] Trial 6542 pruned. Pruned at step 11 with metric 0.5732
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6545 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 9.86378308266158e-06
  Dropout: 0.3281080555744912
================================================================================

[I 2025-11-13 13:44:36,354] Trial 6544 pruned. Pruned at step 14 with metric 0.5696
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6546 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 8.356978527490757e-06
  Dropout: 0.3662112080332869
================================================================================

[I 2025-11-13 13:55:35,805] Trial 6546 pruned. Pruned at step 9 with metric 0.6264
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6547 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 7.855193887327302e-06
  Dropout: 0.36907623232567316
================================================================================

[I 2025-11-13 14:02:34,941] Trial 6547 finished with value: 0.7356398069341586 and parameters: {'seed': 37485, 'model.name': 'roberta-base', 'tok.max_length': 128, 'tok.doc_stride': 32, 'tok.use_fast': True, 'train.batch_size': 48, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 7.855193887327302e-06, 'optim.weight_decay': 1.0519546211300167e-06, 'optim.beta1': 0.8928723069364678, 'optim.beta2': 0.9880034476373788, 'optim.eps': 3.2804892018157707e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.07793452596450617, 'train.clip_grad': 0.9666273246001931, 'model.dropout': 0.36907623232567316, 'model.attn_dropout': 0.1637521905263181, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 5, 'optim.layerwise_lr_decay': 0.9311097484802582, 'head.pooling': 'cls', 'head.layers': 2, 'head.hidden': 256, 'head.activation': 'relu', 'head.dropout': 0.39196271406454336, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.018059778674933, 'loss.cls.alpha': 0.44481602635836803, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[I 2025-11-13 14:02:35,613] Trial 6548 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[W 2025-11-13 14:02:36,234] The parameter `tok.doc_stride` in Trial#6549 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
EarlyStopping triggered at epoch 39 (patience=20)

================================================================================
TRIAL 6549 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-base'}
  Batch size: 8
  Learning rate: 2.0881097571171092e-05
  Dropout: 0.036889420548361085
================================================================================

[I 2025-11-13 14:17:49,501] Trial 6545 finished with value: 0.7354838709677419 and parameters: {'seed': 21138, 'model.name': 'roberta-base', 'tok.max_length': 288, 'tok.doc_stride': 48, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 9.86378308266158e-06, 'optim.weight_decay': 0.035601195932660694, 'optim.beta1': 0.9347343292907581, 'optim.beta2': 0.9850791442354314, 'optim.eps': 4.328387906821481e-08, 'sched.name': 'polynomial', 'sched.warmup_ratio': 0.1457480706398685, 'sched.poly_power': 1.5111725056584788, 'train.clip_grad': 0.4533513151704616, 'model.dropout': 0.3281080555744912, 'model.attn_dropout': 0.09037883391676686, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8705829253412222, 'head.pooling': 'max', 'head.layers': 1, 'head.hidden': 1024, 'head.activation': 'silu', 'head.dropout': 0.278241904494117, 'loss.cls.type': 'ce', 'loss.cls.label_smoothing': 0.1998684583602757, 'loss.cls.balance': 'weighted'}. Best is trial 1951 with value: 0.7883870967741935.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
EarlyStopping triggered at epoch 44 (patience=20)

================================================================================
TRIAL 6550 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 5.1824659059836215e-06
  Dropout: 0.39882652951308684
================================================================================

[I 2025-11-13 14:26:17,259] Trial 6550 pruned. Pruned at step 11 with metric 0.6474
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6551 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 12
  Learning rate: 5.871548324301985e-06
  Dropout: 0.3193275187451717
================================================================================

[I 2025-11-13 14:26:24,879] Trial 6551 pruned. OOM: microsoft/deberta-v3-large bs=12 len=288
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

[OOM] Trial 6551 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 12 (effective: 48 with grad_accum=4)
  Max length: 288
  Error: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 104.06 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this pro
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6552 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 6.352749548577151e-06
  Dropout: 0.3411970302231211
================================================================================

[I 2025-11-13 14:35:14,688] Trial 6552 pruned. Pruned at step 24 with metric 0.5439
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6553 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 5.230860370286191e-06
  Dropout: 0.16617112796263062
================================================================================

[I 2025-11-13 14:40:50,058] Trial 6553 pruned. Pruned at step 12 with metric 0.6343
[I 2025-11-13 14:40:50,746] Trial 6554 pruned. Pruned: Large model with bsz=16, accum=8 (effective_batch=128) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6555 - Configuration:
  Model: {'name': 'xlm-roberta-base'}
  Batch size: 8
  Learning rate: 2.208435274677876e-05
  Dropout: 0.34354840728267355
================================================================================

[I 2025-11-13 14:51:34,978] Trial 6549 finished with value: 0.6055734856007944 and parameters: {'seed': 59023, 'model.name': 'microsoft/deberta-v3-base', 'tok.max_length': 160, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 4, 'optim.name': 'adamw', 'optim.lr': 2.0881097571171092e-05, 'optim.weight_decay': 0.002692337366264118, 'optim.beta1': 0.8690341313506176, 'optim.beta2': 0.9926545791963376, 'optim.eps': 3.4206635024539043e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.0872058798023398, 'sched.cosine_cycles': 2, 'train.clip_grad': 1.4273671246051516, 'model.dropout': 0.036889420548361085, 'model.attn_dropout': 0.22285667538422418, 'train.grad_checkpointing': False, 'train.freeze_encoder_layers': 3, 'optim.layerwise_lr_decay': 0.8090495459437942, 'head.pooling': 'max', 'head.layers': 4, 'head.hidden': 1536, 'head.activation': 'relu', 'head.dropout': 0.06792973558223875, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.421498120981236, 'loss.cls.alpha': 0.38161651434291793, 'loss.cls.balance': 'none'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 39 (patience=20)

================================================================================
TRIAL 6556 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 5.676178120300185e-06
  Dropout: 0.26938667325661114
================================================================================

[I 2025-11-13 15:07:40,106] Trial 6555 finished with value: 0.4368131868131868 and parameters: {'seed': 14532, 'model.name': 'xlm-roberta-base', 'tok.max_length': 192, 'tok.doc_stride': 64, 'tok.use_fast': True, 'train.batch_size': 8, 'train.grad_accum': 2, 'optim.name': 'adamw', 'optim.lr': 2.208435274677876e-05, 'optim.weight_decay': 0.021973167552883116, 'optim.beta1': 0.8258665159186882, 'optim.beta2': 0.9990940973988094, 'optim.eps': 1.8672344360643683e-08, 'sched.name': 'one_cycle', 'sched.warmup_ratio': 0.19627669236175857, 'sched.cosine_cycles': 2, 'train.clip_grad': 0.9094501327896494, 'model.dropout': 0.34354840728267355, 'model.attn_dropout': 0.18522795609574547, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 1, 'optim.layerwise_lr_decay': 0.8104448030498537, 'head.pooling': 'cls', 'head.layers': 1, 'head.hidden': 384, 'head.activation': 'silu', 'head.dropout': 0.0752012693327584, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.159733772005033, 'loss.cls.alpha': 0.4312511825666261, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
[W 2025-11-13 15:07:40,767] The parameter `tok.doc_stride` in Trial#6557 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
EarlyStopping triggered at epoch 21 (patience=20)

================================================================================
TRIAL 6557 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.3293683577918179e-05
  Dropout: 0.35125113741383124
================================================================================

[I 2025-11-13 15:12:29,685] Trial 6556 pruned. Pruned at step 15 with metric 0.5450
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6558 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 12
  Learning rate: 5.1180071459434386e-05
  Dropout: 0.03797679137786873
================================================================================

[I 2025-11-13 15:22:47,639] Trial 6557 pruned. Pruned at step 10 with metric 0.5521

================================================================================
TRIAL 6559 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 8
  Learning rate: 6.0689656796877945e-06
  Dropout: 0.4038056192177214
================================================================================

[I 2025-11-13 15:24:02,784] Trial 6558 pruned. Pruned at step 27 with metric 0.5439
[I 2025-11-13 15:24:03,506] Trial 6560 pruned. Pruned: Large model with bsz=24, accum=1 (effective_batch=24) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6561 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 2.054011432631904e-05
  Dropout: 0.12837446181092138
================================================================================

[I 2025-11-13 15:27:50,734] Trial 6561 pruned. Pruned at step 10 with metric 0.5986
[I 2025-11-13 15:27:51,443] Trial 6562 pruned. Pruned: Large model with bsz=64, accum=1 (effective_batch=64) likely causes OOM (24GB GPU limit)

================================================================================
TRIAL 6563 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 12
  Learning rate: 9.33653167988034e-06
  Dropout: 0.318116730894652
================================================================================

[I 2025-11-13 16:07:24,415] Trial 6559 pruned. Pruned at step 13 with metric 0.6079
[I 2025-11-13 16:07:25,118] Trial 6564 pruned. Pruned: Large model with bsz=64, accum=3 (effective_batch=192) likely causes OOM (24GB GPU limit)
[I 2025-11-13 16:07:25,760] Trial 6565 pruned. Pruned: Large model with bsz=16, accum=6 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6566 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 48
  Learning rate: 1.122967075172651e-05
  Dropout: 0.3009223568783051
================================================================================

[I 2025-11-13 16:21:19,076] Trial 6566 pruned. Pruned at step 27 with metric 0.6279

================================================================================
TRIAL 6567 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 5.368682381961636e-06
  Dropout: 0.4418753122013363
================================================================================

[I 2025-11-13 16:26:30,581] Trial 6563 finished with value: 0.6654726368159204 and parameters: {'seed': 13780, 'model.name': 'bert-large-uncased', 'tok.max_length': 192, 'tok.doc_stride': 96, 'tok.use_fast': False, 'train.batch_size': 12, 'train.grad_accum': 1, 'optim.name': 'adamw', 'optim.lr': 9.33653167988034e-06, 'optim.weight_decay': 9.572476661019919e-06, 'optim.beta1': 0.8422539152039629, 'optim.beta2': 0.9819504779105188, 'optim.eps': 1.8734093170077242e-08, 'sched.name': 'linear', 'sched.warmup_ratio': 0.18496305321719056, 'train.clip_grad': 1.048911924286668, 'model.dropout': 0.318116730894652, 'model.attn_dropout': 0.061794811896292684, 'train.grad_checkpointing': True, 'train.freeze_encoder_layers': 4, 'optim.layerwise_lr_decay': 0.8550658493001403, 'head.pooling': 'mean', 'head.layers': 4, 'head.hidden': 768, 'head.activation': 'relu', 'head.dropout': 0.3706088400448111, 'loss.cls.type': 'focal', 'loss.cls.gamma': 4.355556970937855, 'loss.cls.alpha': 0.5026848598763674, 'loss.cls.balance': 'effective_num'}. Best is trial 1951 with value: 0.7883870967741935.
EarlyStopping triggered at epoch 44 (patience=20)

================================================================================
TRIAL 6568 - Configuration:
  Model: {'name': 'bert-large-uncased'}
  Batch size: 16
  Learning rate: 5.827378268348772e-06
  Dropout: 0.37055839023849385
================================================================================

[I 2025-11-13 16:40:14,576] Trial 6568 pruned. Pruned at step 10 with metric 0.5763
[I 2025-11-13 16:40:15,246] Trial 6569 pruned. Pruned: Large model with bsz=48, accum=3 (effective_batch=144) likely causes OOM (24GB GPU limit)
[I 2025-11-13 16:40:15,901] Trial 6570 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
/home/user/miniforge3/envs/llmhe/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

================================================================================
TRIAL 6571 - Configuration:
  Model: {'name': 'microsoft/deberta-v3-large'}
  Batch size: 8
  Learning rate: 1.4655073613454092e-05
  Dropout: 0.22715695442504585
================================================================================

[I 2025-11-13 16:40:24,181] Trial 6567 pruned. OOM: bert-large-uncased bs=16 len=352
[I 2025-11-13 16:40:24,387] Trial 6571 pruned. OOM: microsoft/deberta-v3-large bs=8 len=320
[I 2025-11-13 16:40:25,434] Trial 6572 pruned. Pruned: Large model with bsz=48, accum=6 (effective_batch=288) likely causes OOM (24GB GPU limit)
[W 2025-11-13 16:40:25,858] The parameter `tok.doc_stride` in Trial#6573 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

[OOM] Trial 6567 exceeded GPU memory:
  Model: bert-large-uncased
  Batch size: 16 (effective: 64 with grad_accum=4)
  Max length: 352
  Error: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 66.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


[OOM] Trial 6571 exceeded GPU memory:
  Model: microsoft/deberta-v3-large
  Batch size: 8 (effective: 16 with grad_accum=2)
  Max length: 320
  Error: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 27.69 MiB is free. Process 2565 has 258.00 MiB memory in use. Including non-PyTorch memory, this proce
  Pruning trial to allow Optuna to learn memory constraints.


================================================================================
TRIAL 6573 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 8
  Learning rate: 1.9484354979119018e-05
  Dropout: 0.45953726072952605
================================================================================


================================================================================
TRIAL 6574 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 16
  Learning rate: 3.38050416771126e-05
  Dropout: 0.19947223326489705
================================================================================

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-13 16:44:32,977] Trial 6573 pruned. Pruned at step 7 with metric 0.5866
[W 2025-11-13 16:44:33,645] The parameter `tok.doc_stride` in Trial#6575 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.

================================================================================
TRIAL 6575 - Configuration:
  Model: {'name': 'bert-base-uncased'}
  Batch size: 24
  Learning rate: 1.1938443349715016e-05
  Dropout: 0.37711744625063415
================================================================================

[I 2025-11-13 16:45:36,550] Trial 6574 pruned. Pruned at step 9 with metric 0.6200
[W 2025-11-13 16:45:37,228] The parameter `tok.doc_stride` in Trial#6576 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-13 16:45:37,288] Trial 6576 pruned. Pruned: Large model with bsz=24, accum=6 (effective_batch=144) likely causes OOM (24GB GPU limit)
[W 2025-11-13 16:45:37,898] The parameter `tok.doc_stride` in Trial#6577 is sampled independently using `RandomSampler` instead of `TPESampler`, potentially degrading the optimization performance. This fallback happend because dynamic search space is not supported for `multivariate=True`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler` if this independent sampling is intended behavior.
[I 2025-11-13 16:45:37,959] Trial 6577 pruned. Pruned: Large model with bsz=24, accum=4 (effective_batch=96) likely causes OOM (24GB GPU limit)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6578 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 8
  Learning rate: 2.6401064801293397e-05
  Dropout: 0.0848411839328339
================================================================================

[I 2025-11-13 16:47:09,330] Trial 6575 pruned. Pruned at step 10 with metric 0.6252
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

================================================================================
TRIAL 6579 - Configuration:
  Model: {'name': 'roberta-base'}
  Batch size: 64
  Learning rate: 1.8845550117229012e-05
  Dropout: 0.3915509589061079
================================================================================

Terminated
make[1]: *** [Makefile:369: tune-criteria-supermax] Error 143
make[1]: Leaving directory '/media/user/SSD1/YuNing/NoAug_Criteria_Evidence'
make: *** [Makefile:414: tune-all-supermax] Error 2
