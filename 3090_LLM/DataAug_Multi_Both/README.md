# DataAug Multi Both - Mental Health NLP

Deep learning framework for multi-task mental health criteria detection with hyperparameter optimization.

## ğŸ¯ Overview

This project implements a dual-agent NLP system for mental health text analysis:
- **Criteria Matching**: Multi-label classification of mental health criteria
- **Evidence Binding**: Span extraction for evidence identification

**Key Features:**
- 15 pre-trained transformer models (BERT, DeBERTa, RoBERTa, etc.)
- Comprehensive HPO with Optuna (97-100 hyperparameters)
- PEFT methods (LoRA, Adapters, IA3)
- Advanced optimization (AdamW, Lion, Adafactor)
- MLflow experiment tracking
- Automated checkpoint management

---

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
conda activate llmhe  # or your environment
pip install tiktoken --upgrade
pip install --upgrade optuna "sqlalchemy>=2.0.0"
pip install -e .

# 2. Test HPO (3 trials, ~15 minutes)
make hpo-test

# 3. View results
make mlflow-ui
# Open http://localhost:5000 in browser

# 4. Run full HPO (optional, 50 trials, 4-8 hours)
make hpo
```

---

## ğŸ“ Project Structure

```
DataAug_Multi_Both/
â”œâ”€â”€ src/dataaug_multi_both/
â”‚   â”œâ”€â”€ cli/                    # Command-line interfaces
â”‚   â”‚   â”œâ”€â”€ train.py           # Training & HPO entry point
â”‚   â”‚   â””â”€â”€ evaluate_study.py  # Evaluation tools
â”‚   â”œâ”€â”€ data/                   # Data loading & augmentation
â”‚   â”‚   â”œâ”€â”€ dataset.py         # Dataset classes
â”‚   â”‚   â”œâ”€â”€ dataset_loader.py  # HuggingFace dataset loading
â”‚   â”‚   â””â”€â”€ augmentation.py    # Data augmentation
â”‚   â”œâ”€â”€ models/                 # Model architectures
â”‚   â”‚   â”œâ”€â”€ multi_task_model.py      # Main multi-task model
â”‚   â”‚   â”œâ”€â”€ encoders/hf_encoder.py   # HuggingFace encoder wrapper
â”‚   â”‚   â””â”€â”€ heads/                    # Task-specific heads
â”‚   â”‚       â”œâ”€â”€ criteria_matching.py # Classification head
â”‚   â”‚       â””â”€â”€ evidence_binding.py  # Span extraction head
â”‚   â”œâ”€â”€ training/               # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Training loop
â”‚   â”‚   â”œâ”€â”€ losses.py          # Loss functions
â”‚   â”‚   â””â”€â”€ checkpoint_manager.py  # Checkpoint management
â”‚   â”œâ”€â”€ hpo/                    # Hyperparameter optimization
â”‚   â”‚   â”œâ”€â”€ search_space.py    # Optuna search space (97-100 params)
â”‚   â”‚   â”œâ”€â”€ trial_executor.py  # Trial execution
â”‚   â”‚   â””â”€â”€ metrics_buffer.py  # Metrics tracking
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ mlflow_setup.py    # MLflow configuration
â”‚       â”œâ”€â”€ logging.py         # Logging utilities
â”‚       â””â”€â”€ storage_monitor.py # Storage management
â”œâ”€â”€ configs/                    # Hydra configurations
â”œâ”€â”€ experiments/                # Experiment outputs (auto-generated)
â”‚   â”œâ”€â”€ hpo_production.db      # Optuna study database
â”‚   â”œâ”€â”€ mlflow_db/             # MLflow tracking database
â”‚   â””â”€â”€ trial_*/               # Trial checkpoints & logs
â”œâ”€â”€ tests/                      # Unit & integration tests
â”œâ”€â”€ Makefile                    # Build automation
â””â”€â”€ README.md                   # This file
```

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.10+
- CUDA 11.8+ (for GPU training)
- 16GB+ RAM (32GB+ recommended for production HPO)
- 50GB+ disk space for experiments

### Setup

```bash
# Using conda (recommended)
conda create -n mental_health_nlp python=3.10
conda activate mental_health_nlp

# Install dependencies
pip install tiktoken --upgrade
pip install --upgrade optuna "sqlalchemy>=2.0.0"
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -e .

# Verify installation
python -c "import tiktoken, optuna, torch; print('âœ… All dependencies installed')"
```

---

## ğŸ¯ Usage

### Hyperparameter Optimization

**Quick Test (3 trials, ~15 min):**
```bash
make hpo-test
```

**Default Run (50 trials, 4-8 hours):**
```bash
make hpo
```

**Production Run (500 trials, 40-80 hours):**
```bash
make hpo-production
```

### View Results

```bash
# MLflow UI
make mlflow-ui  # http://localhost:5000

# CLI results
make hpo-results

# Search space info
make hpo-info
```

### Evaluation

```bash
make evaluate ARGS='--study-db experiments/hpo_production.db --study-name mental_health_hpo_production'
```

---

## ğŸ—ï¸ Model Architecture

### Multi-Task Model

```
Input Text â†’ [Transformer Encoder] â†’ [Pooling]
                                          â”œâ†’ [Criteria Head] â†’ Multi-Label Classification
                                          â””â†’ [Evidence Head] â†’ Span Extraction
```

**Backbone Models (15):**
- BERT (3 variants), DeBERTa (2), SpanBERT (2)
- XLM-RoBERTa (3), ELECTRA (1), Longformer (2)
- BioBERT (1), ClinicalBERT (1)

**Pooling:** CLS, Mean, Attention, Scalar Mix

**Criteria Head:** Linear, MLP, GLU, Multi-Sample Dropout

**Evidence Head:** Linear, MLP, Biaffine, BIO-CRF, Sentence Reranker

---

## ğŸ”¬ HPO Search Space

**Total Parameters:** ~97-100

| Category | Options |
|----------|---------|
| **PEFT** | LoRA, LoRA+, AdaLoRA, Pfeiffer, Houlsby, Compacter, IA3 |
| **Optimizers** | AdamW, Adafactor, Lion, Adam |
| **Schedulers** | Linear, Cosine, Cosine Restart, One Cycle |
| **Losses** | CE, Focal, BCE, Weighted BCE, Adaptive Focal, Hybrid |
| **Adversarial** | FGM, PGD |
| **Adaptation** | DAPT, TAPT |
| **Augmentation** | NLPAug, TextAttack |

---

## ğŸ”„ Reproducibility

### Deterministic Training
- Fixed random seeds: 42, 1337, 2025
- CUDA deterministic algorithms enabled
- `CUBLAS_WORKSPACE_CONFIG=:4096:8` (auto-set)

### Checkpoints

**Auto-saved:**
```
experiments/trial_<uuid>/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ checkpoint_epoch0001.pt           # Model state
â”‚   â””â”€â”€ checkpoint_epoch0001.pt.meta.json # Metadata
â”œâ”€â”€ logs/train.log
â””â”€â”€ config.json
```

**Contents:** Model, optimizer, scheduler states, metrics, random states

---

## ğŸ“Š Results

### Metrics
- **Primary:** F1 Score (macro-averaged)
- **Criteria:** Precision, Recall, F1 (per-class & macro)
- **Evidence:** Exact Match, Token F1, Character F1
- **Training:** Loss, gradient norm, LR

### Export

```bash
# JSON summary
python -m dataaug_multi_both.cli.evaluate_study \
    --study-db experiments/hpo_production.db \
    --study-name mental_health_hpo_production \
    --output results.json

# Best config
python -c "
import optuna, json
study = optuna.load_study(study_name='mental_health_hpo_production', storage='sqlite:///experiments/hpo_production.db')
with open('best_config.json', 'w') as f:
    json.dump(study.best_params, f, indent=2)
"
```

---

## ğŸ› ï¸ Development

```bash
# Code quality
make format  # Format code
make lint    # Run linters
make check   # All checks

# Testing
make test           # All tests
make test-unit      # Unit only
make test-coverage  # With coverage

# Maintenance
make clean      # Remove caches
make clean-all  # Remove all (incl. experiments)

# Help
make help  # Show all commands
```

---

## ğŸ› Troubleshooting

**tiktoken error:**
```bash
pip install tiktoken --upgrade
```

**CUDA OOM:**
- Reduce batch_size in search space
- Enable gradient checkpointing
- Use fp16/bf16

**Study not found:**
```bash
ls experiments/*.db  # Check DB exists
make hpo-results     # Verify study name
```

---

## ğŸ“ˆ Monitoring

### Real-time

```bash
# Trial logs
tail -f experiments/trial_*/logs/train.log

# GPU usage
watch -n 1 nvidia-smi

# Study progress
make hpo-results
```

### Analysis

```python
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances

study = optuna.load_study(
    study_name='mental_health_hpo_production',
    storage='sqlite:///experiments/hpo_production.db'
)

# Plot history
fig = plot_optimization_history(study)
fig.write_html('history.html')

# Plot importances
fig = plot_param_importances(study)
fig.write_html('importances.html')
```

---

## ğŸ“š Dataset

**REDSM5:** Mental Health Spanish Dataset
- Source: `irlab-udc/redsm5` (HuggingFace)
- Splits: train, validation, test
- Tasks: Criteria classification + evidence extraction

---

## âœ… Status

**Version:** 0.1.0
**Updated:** 2025-10-11
**Status:** âœ… Production Ready

**Verified:**
- âœ… Dependencies installed
- âœ… HPO running successfully
- âœ… Progress tracking working
- âœ… Checkpointing functional
- âœ… MLflow enabled
- âœ… Reproducible

---

## ğŸ“„ License

Academic research project. Cite appropriately if used.

---

**Quick Commands:**
```bash
make help           # Show all commands
make hpo-test       # Test HPO (3 trials)
make hpo            # Run HPO (50 trials)
make hpo-production # Production HPO (500 trials)
make mlflow-ui      # View results
make hpo-results    # CLI results
```
