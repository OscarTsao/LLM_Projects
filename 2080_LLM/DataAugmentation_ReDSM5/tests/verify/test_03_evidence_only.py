"""Verify augmentation modifies ONLY evidence spans."""
import pytest
from tests.verify_utils import load_fixture, run_cli
import pandas as pd


@pytest.fixture(scope="session")
def generated_dataset_path(tmp_path_factory):
    """
    Session-scoped fixture that generates a minimal dataset once for all tests.
    Uses fast CPU methods to keep test suite quick.
    """
    outdir = tmp_path_factory.mktemp("evidence_test_data")
    
    # Run with two fast CPU methods
    run_cli(
        "--input", "tests/fixtures/mini_annotations.csv",
        "--output-root", str(outdir),
        "--combo-mode", "singletons",
        "--variants-per-sample", "2",
        "--seed", "123",
        "--methods-yaml", "conf/augment_methods.yaml",
        "--num-proc", "1",
    )
    
    # Find generated dataset
    datasets = list(outdir.rglob("dataset.parquet"))
    if not datasets:
        pytest.skip("No datasets generated by fixture")
    
    return datasets[0]


def test_non_evidence_unchanged(generated_dataset_path):
    """Core property: everything except evidence span is byte-identical."""
    df_aug = pd.read_parquet(generated_dataset_path)
    df_orig = load_fixture()
    
    # Track how many rows we actually tested
    tested_count = 0
    
    # For each augmented row, verify non-evidence region is unchanged
    for _, row in df_aug.iterrows():
        # Find original row by post_id
        orig_row = df_orig[df_orig["post_id"] == row.get("post_id", "")]
        if orig_row.empty:
            continue
        
        orig_text = orig_row.iloc[0]["post_text"]
        aug_text = row["post_text"]
        evidence_aug = row["evidence"]
        evidence_orig = row.get("evidence_original", orig_row.iloc[0]["evidence"])
        
        # Reconstruct original by replacing augmented evidence with original
        reconstructed = aug_text.replace(evidence_aug, evidence_orig, 1)
        assert reconstructed == orig_text, (
            f"Non-evidence region changed for {row.get('post_id')}\n"
            f"Original: {orig_text!r}\n"
            f"Reconstructed: {reconstructed!r}"
        )
        tested_count += 1
    
    # Ensure we actually tested some rows
    assert tested_count > 0, "No augmented rows were tested"


def test_fuzzy_match_evidence_only(generated_dataset_path):
    """
    For cases where exact string replacement doesn't work (whitespace, 
    case changes), use fuzzy matching to verify only evidence changed.
    """
    df_aug = pd.read_parquet(generated_dataset_path)
    df_orig = load_fixture()
    
    tested_count = 0
    
    for _, row in df_aug.iterrows():
        orig_row = df_orig[df_orig["post_id"] == row.get("post_id", "")]
        if orig_row.empty:
            continue
        
        orig_text = orig_row.iloc[0]["post_text"]
        aug_text = row["post_text"]
        evidence_orig = row.get("evidence_original", orig_row.iloc[0]["evidence"])
        evidence_aug = row["evidence"]
        
        # Try exact replacement first
        if evidence_aug in aug_text:
            reconstructed = aug_text.replace(evidence_aug, evidence_orig, 1)
            if reconstructed == orig_text:
                tested_count += 1
                continue
        
        # Fall back to fuzzy matching
        # Find the longest common prefix and suffix
        prefix_len = 0
        suffix_len = 0
        
        for i, (orig_char, aug_char) in enumerate(zip(orig_text, aug_text)):
            if orig_char == aug_char:
                prefix_len = i + 1
            else:
                break
        
        for i in range(1, min(len(orig_text), len(aug_text)) + 1):
            if orig_text[-i] == aug_text[-i]:
                suffix_len = i
            else:
                break
        
        # Verify the changed region overlaps with evidence
        if prefix_len + suffix_len < len(orig_text):
            # There's a changed middle section
            orig_middle = orig_text[prefix_len:len(orig_text) - suffix_len if suffix_len > 0 else len(orig_text)]
            aug_middle = aug_text[prefix_len:len(aug_text) - suffix_len if suffix_len > 0 else len(aug_text)]
            
            # Evidence should be in the changed region
            evidence_in_orig = evidence_orig in orig_middle
            evidence_in_aug = evidence_aug in aug_middle
            
            assert evidence_in_orig or evidence_in_aug, (
                f"Evidence not found in changed region for {row.get('post_id')}\n"
                f"Original middle: {orig_middle!r}\n"
                f"Augmented middle: {aug_middle!r}\n"
                f"Evidence orig: {evidence_orig!r}\n"
                f"Evidence aug: {evidence_aug!r}"
            )
        
        tested_count += 1
    
    assert tested_count > 0, "No rows were tested with fuzzy matching"


def test_evidence_actually_changed(generated_dataset_path):
    """Verify that augmentation actually changes the evidence (not a no-op)."""
    df_aug = pd.read_parquet(generated_dataset_path)
    df_orig = load_fixture()
    
    changed_count = 0
    total_count = 0
    
    for _, row in df_aug.iterrows():
        orig_row = df_orig[df_orig["post_id"] == row.get("post_id", "")]
        if orig_row.empty:
            continue
        
        evidence_orig = row.get("evidence_original", orig_row.iloc[0]["evidence"])
        evidence_aug = row["evidence"]
        
        total_count += 1
        if evidence_orig != evidence_aug:
            changed_count += 1
    
    assert total_count > 0, "No rows found for testing"
    # At least 50% of augmentations should actually change the evidence
    change_rate = changed_count / total_count
    assert change_rate >= 0.5, (
        f"Too few evidence changes: {changed_count}/{total_count} ({change_rate:.1%})"
    )
