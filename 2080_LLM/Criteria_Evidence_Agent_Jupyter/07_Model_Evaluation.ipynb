{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Analysis\n",
    "\n",
    "This notebook provides comprehensive model evaluation, analysis, and comparison capabilities.\n",
    "It includes performance metrics, error analysis, model interpretation, and comparison tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import all necessary libraries for model evaluation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# ML and evaluation libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, roc_curve, average_precision_score,\n",
    "    multilabel_confusion_matrix, hamming_loss, jaccard_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies\n",
    "\n",
    "Load configuration and model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and checkpoint managers\n",
    "%run 01_Configuration_Management.ipynb\n",
    "%run 02_Enhanced_Checkpoint_System.ipynb\n",
    "\n",
    "# Import project modules\n",
    "from src.models.model import EvidenceModel\n",
    "from src.data.dataset import DataModule\n",
    "from src.utils import evaluate, prepare_thresholds\n",
    "from src.utils.metrics import compute_metrics\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Configuration\n",
    "\n",
    "Load trained models and configure evaluation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_selector():\n",
    "    \"\"\"Create an interactive model selector for evaluation.\"\"\"\n",
    "    \n",
    "    # Get available checkpoints\n",
    "    checkpoints_df = checkpoint_manager.list_checkpoints()\n",
    "    \n",
    "    if checkpoints_df.empty:\n",
    "        print(\"No trained models found. Please train a model first.\")\n",
    "        return None\n",
    "    \n",
    "    # Group by experiment\n",
    "    experiments = checkpoints_df['experiment'].unique().tolist()\n",
    "    \n",
    "    experiment_dropdown = widgets.Dropdown(\n",
    "        options=experiments,\n",
    "        value=experiments[0] if experiments else None,\n",
    "        description='Experiment:'\n",
    "    )\n",
    "    \n",
    "    checkpoint_dropdown = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='Checkpoint:'\n",
    "    )\n",
    "    \n",
    "    use_best = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Use Best Checkpoint'\n",
    "    )\n",
    "    \n",
    "    device_dropdown = widgets.Dropdown(\n",
    "        options=['auto', 'cpu', 'cuda'],\n",
    "        value='auto',\n",
    "        description='Device:'\n",
    "    )\n",
    "    \n",
    "    load_button = widgets.Button(\n",
    "        description='Load Model',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def update_checkpoints(change):\n",
    "        \"\"\"Update checkpoint dropdown based on selected experiment.\"\"\"\n",
    "        experiment = change['new']\n",
    "        exp_checkpoints = checkpoints_df[checkpoints_df['experiment'] == experiment]\n",
    "        checkpoint_options = [(f\"{row['checkpoint_id']} (epoch {row['epoch']}, metric: {row['best_metric']:.4f})\", \n",
    "                              row['checkpoint_id']) for _, row in exp_checkpoints.iterrows()]\n",
    "        checkpoint_dropdown.options = checkpoint_options\n",
    "        if checkpoint_options:\n",
    "            checkpoint_dropdown.value = checkpoint_options[0][1]\n",
    "    \n",
    "    experiment_dropdown.observe(update_checkpoints, names='value')\n",
    "    \n",
    "    # Initialize checkpoints\n",
    "    if experiments:\n",
    "        update_checkpoints({'new': experiments[0]})\n",
    "    \n",
    "    def on_load_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                experiment = experiment_dropdown.value\n",
    "                \n",
    "                # Determine checkpoint to use\n",
    "                if use_best.value:\n",
    "                    checkpoint_id = checkpoint_manager.find_best_checkpoint(experiment)\n",
    "                    print(f\"üèÜ Using best checkpoint for {experiment}\")\n",
    "                else:\n",
    "                    checkpoint_id = checkpoint_dropdown.value\n",
    "                    print(f\"üìã Using selected checkpoint: {checkpoint_id}\")\n",
    "                \n",
    "                if not checkpoint_id:\n",
    "                    print(\"‚ùå No checkpoint found\")\n",
    "                    return\n",
    "                \n",
    "                # Determine device\n",
    "                if device_dropdown.value == 'auto':\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                else:\n",
    "                    device = torch.device(device_dropdown.value)\n",
    "                \n",
    "                print(f\"   Device: {device}\")\n",
    "                \n",
    "                # Load checkpoint info\n",
    "                checkpoint_info = checkpoint_manager.get_checkpoint_info(checkpoint_id)\n",
    "                print(f\"   Checkpoint: {checkpoint_id}\")\n",
    "                print(f\"   Epoch: {checkpoint_info['metadata']['epoch']}\")\n",
    "                print(f\"   Best metric: {checkpoint_info['metadata']['best_metric']:.4f}\")\n",
    "                \n",
    "                # Store in global variables\n",
    "                global current_checkpoint_id, current_device, current_experiment\n",
    "                current_checkpoint_id = checkpoint_id\n",
    "                current_device = device\n",
    "                current_experiment = experiment\n",
    "                \n",
    "                print(f\"\\n‚úÖ Model configuration ready!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading model: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    load_button.on_click(on_load_clicked)\n",
    "    \n",
    "    layout = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Model Selection for Evaluation</h3>\"),\n",
    "        experiment_dropdown,\n",
    "        widgets.HBox([use_best, device_dropdown]),\n",
    "        checkpoint_dropdown,\n",
    "        load_button,\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return layout\n",
    "\n",
    "# Display model selector\n",
    "model_selector = create_model_selector()\n",
    "if model_selector:\n",
    "    display(model_selector)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No trained models available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Pipeline\n",
    "\n",
    "Comprehensive model evaluation with detailed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_comprehensive(checkpoint_id: str, device: torch.device, \n",
    "                                split: str = 'test') -> Dict[str, Any]:\n",
    "    \"\"\"Perform comprehensive model evaluation.\"\"\"\n",
    "    \n",
    "    print(f\"üß™ Comprehensive Model Evaluation\")\n",
    "    print(f\"   Checkpoint: {checkpoint_id}\")\n",
    "    print(f\"   Split: {split}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Load checkpoint info and configuration\n",
    "        checkpoint_info = checkpoint_manager.get_checkpoint_info(checkpoint_id)\n",
    "        \n",
    "        # Load configuration from checkpoint\n",
    "        config_path = checkpoint_manager.checkpoint_dir / checkpoint_id / \"config.json\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        \n",
    "        # Reconstruct configuration object\n",
    "        config = ExperimentConfig(**config_dict)\n",
    "        \n",
    "        print(f\"üìä Setting up data...\")\n",
    "        \n",
    "        # Setup data module\n",
    "        data_module = DataModule(config.data, config.model)\n",
    "        \n",
    "        # Get data loaders\n",
    "        train_loader, val_loader, test_loader = data_module.dataloaders(\n",
    "            batch_size=config.training.val_batch_size,\n",
    "            val_batch_size=config.training.val_batch_size,\n",
    "            test_batch_size=config.training.test_batch_size,\n",
    "            num_workers=config.training.num_workers\n",
    "        )\n",
    "        \n",
    "        # Select appropriate data loader\n",
    "        if split == 'train':\n",
    "            data_loader = train_loader\n",
    "        elif split == 'val':\n",
    "            data_loader = val_loader\n",
    "        else:\n",
    "            data_loader = test_loader\n",
    "        \n",
    "        print(f\"   {split.capitalize()} samples: {len(data_loader.dataset)}\")\n",
    "        \n",
    "        print(f\"\\nü§ñ Loading model...\")\n",
    "        \n",
    "        # Create and load model\n",
    "        model = EvidenceModel(config.model)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Create dummy optimizer and scheduler for checkpoint loading\n",
    "        from src.utils import get_optimizer, get_scheduler\n",
    "        optimizer = get_optimizer(model, config.training.optimizer)\n",
    "        scheduler = get_scheduler(optimizer, config.training.scheduler, 1000)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        training_state, _ = checkpoint_manager.load_checkpoint(\n",
    "            checkpoint_id, model, optimizer, scheduler, device\n",
    "        )\n",
    "        \n",
    "        print(f\"   Model loaded from epoch {training_state.epoch}\")\n",
    "        print(f\"   Best metric: {training_state.best_metric:.4f}\")\n",
    "        \n",
    "        # Prepare thresholds\n",
    "        head_configs = model.head_configs\n",
    "        head_thresholds = {\n",
    "            head_name: prepare_thresholds(head_cfg)\n",
    "            for head_name, head_cfg in head_configs.items()\n",
    "            if head_cfg.get(\"type\") == \"multi_label\"\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüîç Running evaluation...\")\n",
    "        \n",
    "        # Run evaluation\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        all_logits = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                token_type_ids = batch.get(\"token_type_ids\")\n",
    "                if token_type_ids is not None:\n",
    "                    token_type_ids = token_type_ids.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "                \n",
    "                # Extract predictions and labels\n",
    "                head_outputs = outputs[\"head_outputs\"]\n",
    "                \n",
    "                for head_name, head_output in head_outputs.items():\n",
    "                    if head_name in batch:\n",
    "                        logits = head_output.cpu()\n",
    "                        labels = batch[head_name].cpu()\n",
    "                        \n",
    "                        all_logits.append(logits)\n",
    "                        all_labels.append(labels)\n",
    "                        \n",
    "                        # Apply thresholds for predictions\n",
    "                        if head_name in head_thresholds:\n",
    "                            thresholds = head_thresholds[head_name]\n",
    "                            probs = torch.sigmoid(logits)\n",
    "                            predictions = (probs > thresholds).float()\n",
    "                        else:\n",
    "                            predictions = (torch.sigmoid(logits) > 0.5).float()\n",
    "                        \n",
    "                        all_predictions.append(predictions)\n",
    "        \n",
    "        # Concatenate all results\n",
    "        all_predictions = torch.cat(all_predictions, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        all_probs = torch.sigmoid(all_logits)\n",
    "        \n",
    "        print(f\"   Predictions shape: {all_predictions.shape}\")\n",
    "        print(f\"   Labels shape: {all_labels.shape}\")\n",
    "        \n",
    "        # Compute comprehensive metrics\n",
    "        print(f\"\\nüìà Computing metrics...\")\n",
    "        \n",
    "        # Convert to numpy for sklearn metrics\n",
    "        y_true = all_labels.numpy()\n",
    "        y_pred = all_predictions.numpy()\n",
    "        y_prob = all_probs.numpy()\n",
    "        \n",
    "        # Overall metrics\n",
    "        hamming = hamming_loss(y_true, y_pred)\n",
    "        jaccard = jaccard_score(y_true, y_pred, average='samples')\n",
    "        \n",
    "        # Per-class metrics\n",
    "        n_classes = y_true.shape[1]\n",
    "        class_names = [f\"Class_{i}\" for i in range(n_classes)]  # Replace with actual class names if available\n",
    "        \n",
    "        per_class_metrics = {}\n",
    "        for i in range(n_classes):\n",
    "            class_true = y_true[:, i]\n",
    "            class_pred = y_pred[:, i]\n",
    "            class_prob = y_prob[:, i]\n",
    "            \n",
    "            # Skip if no positive examples\n",
    "            if class_true.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                auc = roc_auc_score(class_true, class_prob)\n",
    "                ap = average_precision_score(class_true, class_prob)\n",
    "            except ValueError:\n",
    "                auc = 0.0\n",
    "                ap = 0.0\n",
    "            \n",
    "            per_class_metrics[class_names[i]] = {\n",
    "                'auc': auc,\n",
    "                'average_precision': ap,\n",
    "                'support': int(class_true.sum())\n",
    "            }\n",
    "        \n",
    "        # Compute standard metrics using the project's metric function\n",
    "        standard_metrics = compute_metrics(y_pred, y_true)\n",
    "        \n",
    "        results = {\n",
    "            'checkpoint_id': checkpoint_id,\n",
    "            'split': split,\n",
    "            'n_samples': len(y_true),\n",
    "            'n_classes': n_classes,\n",
    "            'hamming_loss': hamming,\n",
    "            'jaccard_score': jaccard,\n",
    "            'standard_metrics': standard_metrics,\n",
    "            'per_class_metrics': per_class_metrics,\n",
    "            'predictions': y_pred,\n",
    "            'labels': y_true,\n",
    "            'probabilities': y_prob,\n",
    "            'class_names': class_names\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "        print(f\"   Hamming Loss: {hamming:.4f}\")\n",
    "        print(f\"   Jaccard Score: {jaccard:.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Evaluation pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Execution\n",
    "\n",
    "Execute model evaluation with interactive controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_interface():\n",
    "    \"\"\"Create interactive evaluation interface.\"\"\"\n",
    "    \n",
    "    split_dropdown = widgets.Dropdown(\n",
    "        options=['test', 'val', 'train'],\n",
    "        value='test',\n",
    "        description='Data Split:'\n",
    "    )\n",
    "    \n",
    "    evaluate_button = widgets.Button(\n",
    "        description='üß™ Evaluate Model',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_evaluate_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            # Check if model is loaded\n",
    "            if 'current_checkpoint_id' not in globals():\n",
    "                print(\"‚ùå No model loaded. Please load a model first.\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                # Run evaluation\n",
    "                results = evaluate_model_comprehensive(\n",
    "                    current_checkpoint_id, \n",
    "                    current_device, \n",
    "                    split_dropdown.value\n",
    "                )\n",
    "                \n",
    "                if results:\n",
    "                    # Store results globally for visualization\n",
    "                    global current_evaluation_results\n",
    "                    current_evaluation_results = results\n",
    "                    \n",
    "                    print(f\"\\nüìä Evaluation Results Summary:\")\n",
    "                    print(f\"   Dataset: {results['split']} ({results['n_samples']} samples)\")\n",
    "                    print(f\"   Classes: {results['n_classes']}\")\n",
    "                    print(f\"   Hamming Loss: {results['hamming_loss']:.4f}\")\n",
    "                    print(f\"   Jaccard Score: {results['jaccard_score']:.4f}\")\n",
    "                    \n",
    "                    # Display standard metrics\n",
    "                    if 'standard_metrics' in results:\n",
    "                        std_metrics = results['standard_metrics']\n",
    "                        print(f\"\\nüìà Standard Metrics:\")\n",
    "                        for metric, value in std_metrics.items():\n",
    "                            if isinstance(value, (int, float)):\n",
    "                                print(f\"   {metric}: {value:.4f}\")\n",
    "                    \n",
    "                    print(f\"\\n‚úÖ Use the visualization cells below to analyze results in detail.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Evaluation failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    evaluate_button.on_click(on_evaluate_clicked)\n",
    "    \n",
    "    layout = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Model Evaluation</h3>\"),\n",
    "        split_dropdown,\n",
    "        evaluate_button,\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return layout\n",
    "\n",
    "# Display evaluation interface\n",
    "evaluation_interface = create_evaluation_interface()\n",
    "display(evaluation_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization\n",
    "\n",
    "Visualize model performance with comprehensive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_metrics():\n",
    "    \"\"\"Plot comprehensive performance metrics.\"\"\"\n",
    "    \n",
    "    if 'current_evaluation_results' not in globals():\n",
    "        print(\"‚ùå No evaluation results available. Please run evaluation first.\")\n",
    "        return\n",
    "    \n",
    "    results = current_evaluation_results\n",
    "    \n",
    "    print(f\"üìä Performance Visualization\")\n",
    "    print(f\"   Model: {results['checkpoint_id']}\")\n",
    "    print(f\"   Split: {results['split']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Model Performance Analysis - {results[\"split\"].title()} Set', fontsize=16)\n",
    "    \n",
    "    # 1. Per-class AUC scores\n",
    "    if results['per_class_metrics']:\n",
    "        class_names = list(results['per_class_metrics'].keys())\n",
    "        auc_scores = [results['per_class_metrics'][cls]['auc'] for cls in class_names]\n",
    "        \n",
    "        axes[0, 0].bar(range(len(class_names)), auc_scores, color='skyblue')\n",
    "        axes[0, 0].set_xlabel('Classes')\n",
    "        axes[0, 0].set_ylabel('AUC Score')\n",
    "        axes[0, 0].set_title('Per-Class AUC Scores')\n",
    "        axes[0, 0].set_xticks(range(len(class_names)))\n",
    "        axes[0, 0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        axes[0, 0].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random')\n",
    "        axes[0, 0].legend()\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'No per-class metrics\\navailable', \n",
    "                        ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "        axes[0, 0].set_title('Per-Class AUC Scores')\n",
    "    \n",
    "    # 2. Per-class Average Precision\n",
    "    if results['per_class_metrics']:\n",
    "        ap_scores = [results['per_class_metrics'][cls]['average_precision'] for cls in class_names]\n",
    "        \n",
    "        axes[0, 1].bar(range(len(class_names)), ap_scores, color='lightcoral')\n",
    "        axes[0, 1].set_xlabel('Classes')\n",
    "        axes[0, 1].set_ylabel('Average Precision')\n",
    "        axes[0, 1].set_title('Per-Class Average Precision')\n",
    "        axes[0, 1].set_xticks(range(len(class_names)))\n",
    "        axes[0, 1].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No per-class metrics\\navailable', \n",
    "                        ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('Per-Class Average Precision')\n",
    "    \n",
    "    # 3. Class support (number of positive examples)\n",
    "    if results['per_class_metrics']:\n",
    "        support = [results['per_class_metrics'][cls]['support'] for cls in class_names]\n",
    "        \n",
    "        axes[1, 0].bar(range(len(class_names)), support, color='lightgreen')\n",
    "        axes[1, 0].set_xlabel('Classes')\n",
    "        axes[1, 0].set_ylabel('Number of Positive Examples')\n",
    "        axes[1, 0].set_title('Class Support Distribution')\n",
    "        axes[1, 0].set_xticks(range(len(class_names)))\n",
    "        axes[1, 0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No per-class metrics\\navailable', \n",
    "                        ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Class Support Distribution')\n",
    "    \n",
    "    # 4. Overall metrics summary\n",
    "    metrics_text = f\"\"\"Overall Performance Metrics:\n",
    "    \n",
    "Samples: {results['n_samples']:,}\n",
    "Classes: {results['n_classes']}\n",
    "    \n",
    "Hamming Loss: {results['hamming_loss']:.4f}\n",
    "Jaccard Score: {results['jaccard_score']:.4f}\n",
    "\"\"\"\n",
    "    \n",
    "    if 'standard_metrics' in results:\n",
    "        std_metrics = results['standard_metrics']\n",
    "        for metric, value in std_metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                metrics_text += f\"{metric}: {value:.4f}\\n\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, metrics_text, transform=axes[1, 1].transAxes, \n",
    "                    fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].axis('off')\n",
    "    axes[1, 1].set_title('Performance Summary')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional detailed metrics table\n",
    "    if results['per_class_metrics']:\n",
    "        print(f\"\\nüìã Detailed Per-Class Metrics:\")\n",
    "        \n",
    "        metrics_df = pd.DataFrame(results['per_class_metrics']).T\n",
    "        metrics_df = metrics_df.round(4)\n",
    "        metrics_df = metrics_df.sort_values('auc', ascending=False)\n",
    "        \n",
    "        display(metrics_df)\n",
    "\n",
    "def plot_confusion_matrices():\n",
    "    \"\"\"Plot confusion matrices for each class.\"\"\"\n",
    "    \n",
    "    if 'current_evaluation_results' not in globals():\n",
    "        print(\"‚ùå No evaluation results available. Please run evaluation first.\")\n",
    "        return\n",
    "    \n",
    "    results = current_evaluation_results\n",
    "    y_true = results['labels']\n",
    "    y_pred = results['predictions']\n",
    "    class_names = results['class_names']\n",
    "    \n",
    "    print(f\"üîç Confusion Matrix Analysis\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Compute multilabel confusion matrices\n",
    "    cm_multilabel = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plot confusion matrices for each class\n",
    "    n_classes = len(class_names)\n",
    "    n_cols = min(4, n_classes)\n",
    "    n_rows = (n_classes + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1 or n_cols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (class_name, cm) in enumerate(zip(class_names, cm_multilabel)):\n",
    "        if i < len(axes):\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=['Negative', 'Positive'],\n",
    "                       yticklabels=['Negative', 'Positive'],\n",
    "                       ax=axes[i])\n",
    "            axes[i].set_title(f'{class_name}')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_classes, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Per-Class Confusion Matrices', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization buttons\n",
    "plot_metrics_button = widgets.Button(\n",
    "    description='üìä Plot Performance Metrics',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "plot_confusion_button = widgets.Button(\n",
    "    description='üîç Plot Confusion Matrices',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "def on_plot_metrics_clicked(b):\n",
    "    plot_performance_metrics()\n",
    "\n",
    "def on_plot_confusion_clicked(b):\n",
    "    plot_confusion_matrices()\n",
    "\n",
    "plot_metrics_button.on_click(on_plot_metrics_clicked)\n",
    "plot_confusion_button.on_click(on_plot_confusion_clicked)\n",
    "\n",
    "print(\"üìä Visualization Tools:\")\n",
    "display(widgets.HBox([plot_metrics_button, plot_confusion_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare multiple models and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_comparison_tool():\n",
    "    \"\"\"Create tool for comparing multiple models.\"\"\"\n",
    "    \n",
    "    # Get available experiments\n",
    "    checkpoints_df = checkpoint_manager.list_checkpoints()\n",
    "    \n",
    "    if checkpoints_df.empty:\n",
    "        print(\"No models available for comparison.\")\n",
    "        return\n",
    "    \n",
    "    experiments = checkpoints_df['experiment'].unique().tolist()\n",
    "    \n",
    "    experiment_selector = widgets.SelectMultiple(\n",
    "        options=experiments,\n",
    "        value=[experiments[0]] if experiments else [],\n",
    "        description='Experiments:',\n",
    "        rows=min(10, len(experiments))\n",
    "    )\n",
    "    \n",
    "    split_dropdown = widgets.Dropdown(\n",
    "        options=['test', 'val'],\n",
    "        value='test',\n",
    "        description='Data Split:'\n",
    "    )\n",
    "    \n",
    "    compare_button = widgets.Button(\n",
    "        description='üîÑ Compare Models',\n",
    "        button_style='warning'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_compare_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            if not experiment_selector.value:\n",
    "                print(\"‚ùå Please select at least one experiment to compare.\")\n",
    "                return\n",
    "            \n",
    "            print(f\"üîÑ Comparing {len(experiment_selector.value)} experiments...\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            comparison_results = []\n",
    "            \n",
    "            for experiment in experiment_selector.value:\n",
    "                try:\n",
    "                    print(f\"\\nüìä Evaluating {experiment}...\")\n",
    "                    \n",
    "                    # Get best checkpoint for experiment\n",
    "                    checkpoint_id = checkpoint_manager.find_best_checkpoint(experiment)\n",
    "                    \n",
    "                    if not checkpoint_id:\n",
    "                        print(f\"   ‚ùå No checkpoint found for {experiment}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Determine device\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    \n",
    "                    # Run evaluation\n",
    "                    results = evaluate_model_comprehensive(\n",
    "                        checkpoint_id, device, split_dropdown.value\n",
    "                    )\n",
    "                    \n",
    "                    if results:\n",
    "                        comparison_results.append({\n",
    "                            'experiment': experiment,\n",
    "                            'checkpoint_id': checkpoint_id,\n",
    "                            'hamming_loss': results['hamming_loss'],\n",
    "                            'jaccard_score': results['jaccard_score'],\n",
    "                            'n_samples': results['n_samples'],\n",
    "                            'standard_metrics': results.get('standard_metrics', {})\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"   ‚úÖ {experiment}: Hamming={results['hamming_loss']:.4f}, Jaccard={results['jaccard_score']:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error evaluating {experiment}: {e}\")\n",
    "            \n",
    "            if comparison_results:\n",
    "                print(f\"\\nüìä Comparison Results:\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                # Create comparison DataFrame\n",
    "                comparison_data = []\n",
    "                for result in comparison_results:\n",
    "                    row = {\n",
    "                        'Experiment': result['experiment'],\n",
    "                        'Checkpoint': result['checkpoint_id'][:12] + '...',\n",
    "                        'Hamming Loss': result['hamming_loss'],\n",
    "                        'Jaccard Score': result['jaccard_score'],\n",
    "                        'Samples': result['n_samples']\n",
    "                    }\n",
    "                    \n",
    "                    # Add standard metrics\n",
    "                    for metric, value in result['standard_metrics'].items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            row[metric] = value\n",
    "                    \n",
    "                    comparison_data.append(row)\n",
    "                \n",
    "                comparison_df = pd.DataFrame(comparison_data)\n",
    "                comparison_df = comparison_df.round(4)\n",
    "                \n",
    "                # Sort by Jaccard score (higher is better)\n",
    "                comparison_df = comparison_df.sort_values('Jaccard Score', ascending=False)\n",
    "                \n",
    "                display(comparison_df)\n",
    "                \n",
    "                # Plot comparison\n",
    "                if len(comparison_results) > 1:\n",
    "                    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                    \n",
    "                    experiments = [r['experiment'] for r in comparison_results]\n",
    "                    hamming_losses = [r['hamming_loss'] for r in comparison_results]\n",
    "                    jaccard_scores = [r['jaccard_score'] for r in comparison_results]\n",
    "                    \n",
    "                    # Hamming Loss (lower is better)\n",
    "                    axes[0].bar(experiments, hamming_losses, color='lightcoral')\n",
    "                    axes[0].set_title('Hamming Loss (Lower is Better)')\n",
    "                    axes[0].set_ylabel('Hamming Loss')\n",
    "                    axes[0].tick_params(axis='x', rotation=45)\n",
    "                    \n",
    "                    # Jaccard Score (higher is better)\n",
    "                    axes[1].bar(experiments, jaccard_scores, color='lightgreen')\n",
    "                    axes[1].set_title('Jaccard Score (Higher is Better)')\n",
    "                    axes[1].set_ylabel('Jaccard Score')\n",
    "                    axes[1].tick_params(axis='x', rotation=45)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                \n",
    "                # Store comparison results globally\n",
    "                global current_comparison_results\n",
    "                current_comparison_results = comparison_results\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå No successful evaluations completed.\")\n",
    "    \n",
    "    compare_button.on_click(on_compare_clicked)\n",
    "    \n",
    "    layout = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Model Comparison</h3>\"),\n",
    "        experiment_selector,\n",
    "        split_dropdown,\n",
    "        compare_button,\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return layout\n",
    "\n",
    "# Display model comparison tool\n",
    "print(\"üîÑ Model Comparison Tool:\")\n",
    "comparison_tool = create_model_comparison_tool()\n",
    "if comparison_tool:\n",
    "    display(comparison_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Export\n",
    "\n",
    "Generate evaluation summary and export results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report():\n",
    "    \"\"\"Generate a comprehensive evaluation report.\"\"\"\n",
    "    \n",
    "    if 'current_evaluation_results' not in globals():\n",
    "        print(\"‚ùå No evaluation results available. Please run evaluation first.\")\n",
    "        return\n",
    "    \n",
    "    results = current_evaluation_results\n",
    "    \n",
    "    print(f\"üìã Evaluation Report\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nü§ñ Model Information:\")\n",
    "    print(f\"   Checkpoint ID: {results['checkpoint_id']}\")\n",
    "    print(f\"   Evaluation Split: {results['split']}\")\n",
    "    print(f\"   Number of Samples: {results['n_samples']:,}\")\n",
    "    print(f\"   Number of Classes: {results['n_classes']}\")\n",
    "    \n",
    "    print(f\"\\nüìä Overall Performance:\")\n",
    "    print(f\"   Hamming Loss: {results['hamming_loss']:.4f}\")\n",
    "    print(f\"   Jaccard Score: {results['jaccard_score']:.4f}\")\n",
    "    \n",
    "    if 'standard_metrics' in results:\n",
    "        print(f\"\\nüìà Standard Metrics:\")\n",
    "        for metric, value in results['standard_metrics'].items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"   {metric}: {value:.4f}\")\n",
    "    \n",
    "    if results['per_class_metrics']:\n",
    "        print(f\"\\nüè∑Ô∏è  Per-Class Performance:\")\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        auc_scores = [metrics['auc'] for metrics in results['per_class_metrics'].values()]\n",
    "        ap_scores = [metrics['average_precision'] for metrics in results['per_class_metrics'].values()]\n",
    "        \n",
    "        print(f\"   Mean AUC: {np.mean(auc_scores):.4f} (¬±{np.std(auc_scores):.4f})\")\n",
    "        print(f\"   Mean AP: {np.mean(ap_scores):.4f} (¬±{np.std(ap_scores):.4f})\")\n",
    "        \n",
    "        # Best and worst performing classes\n",
    "        class_performance = [(name, metrics['auc']) for name, metrics in results['per_class_metrics'].items()]\n",
    "        class_performance.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n   Best performing classes (by AUC):\")\n",
    "        for name, auc in class_performance[:3]:\n",
    "            print(f\"     {name}: {auc:.4f}\")\n",
    "        \n",
    "        print(f\"\\n   Worst performing classes (by AUC):\")\n",
    "        for name, auc in class_performance[-3:]:\n",
    "            print(f\"     {name}: {auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation report complete!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create summary button\n",
    "report_button = widgets.Button(\n",
    "    description='üìã Generate Report',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "def on_report_clicked(b):\n",
    "    generate_evaluation_report()\n",
    "\n",
    "report_button.on_click(on_report_clicked)\n",
    "\n",
    "print(\"\\nüìã Evaluation Summary:\")\n",
    "display(report_button)\n",
    "\n",
    "print(\"\\n‚úÖ Model Evaluation notebook complete!\")\n",
    "print(\"\\nThis notebook provides:\")\n",
    "print(\"‚Ä¢ Interactive model loading and selection\")\n",
    "print(\"‚Ä¢ Comprehensive model evaluation with detailed metrics\")\n",
    "print(\"‚Ä¢ Performance visualization and analysis\")\n",
    "print(\"‚Ä¢ Model comparison across experiments\")\n",
    "print(\"‚Ä¢ Evaluation report generation\")"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": "3"
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.8.0"
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 }
}