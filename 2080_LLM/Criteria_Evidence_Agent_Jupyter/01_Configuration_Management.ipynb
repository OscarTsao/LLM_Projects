{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Management\n",
    "\n",
    "This notebook provides a comprehensive configuration management system for the Criteria Evidence Agent project.\n",
    "It replaces the scattered Hydra configuration files with an interactive, notebook-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Classes\n",
    "\n",
    "Define structured configuration classes for better type safety and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Data configuration.\"\"\"\n",
    "    root_dir: str = \"./Data\"\n",
    "    groundtruth_path: str = \"./Data/groundtruth/redsm5_ground_truth.json\"\n",
    "    posts_path: str = \"./Data/redsm5/redsm5_posts.csv\"\n",
    "    id_field: str = \"post_id\"\n",
    "    text_field: str = \"text\"\n",
    "    multi_label_fields: List[str] = field(default_factory=lambda: [\n",
    "        \"ANHEDONIA\", \"APPETITE_CHANGE\", \"COGNITIVE_ISSUES\", \"DEPRESSED_MOOD\",\n",
    "        \"FATIGUE\", \"PSYCHOMOTOR\", \"SLEEP_ISSUES\", \"SPECIAL_CASE\",\n",
    "        \"SUICIDAL_THOUGHTS\", \"WORTHLESSNESS\"\n",
    "    ])\n",
    "    evidence_char_spans_field: Optional[str] = None\n",
    "    evidence_token_labels_field: Optional[str] = None\n",
    "    max_length: int = 256\n",
    "    val_size: float = 0.15\n",
    "    test_size: float = 0.15\n",
    "    seed: int = 42\n",
    "\n",
    "@dataclass\n",
    "class LoRAConfig:\n",
    "    \"\"\"LoRA configuration for parameter-efficient fine-tuning.\"\"\"\n",
    "    enabled: bool = False\n",
    "    r: int = 16\n",
    "    alpha: int = 32\n",
    "    dropout: float = 0.05\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\"query\", \"key\", \"value\"])\n",
    "\n",
    "@dataclass\n",
    "class EncoderConfig:\n",
    "    \"\"\"Encoder configuration.\"\"\"\n",
    "    type: str = \"roberta\"\n",
    "    pretrained_model_name_or_path: str = \"roberta-base\"\n",
    "    gradient_checkpointing: bool = True\n",
    "    freeze_encoder: bool = False\n",
    "    output_dropout: float = 0.2\n",
    "    pooling: str = \"cls\"\n",
    "    layer_norm_eps: float = 1e-12\n",
    "    lora: LoRAConfig = field(default_factory=LoRAConfig)\n",
    "\n",
    "@dataclass\n",
    "class HeadLayersConfig:\n",
    "    \"\"\"Classification head layers configuration.\"\"\"\n",
    "    hidden_dims: List[int] = field(default_factory=list)\n",
    "    activation: str = \"gelu\"\n",
    "    dropout: float = 0.1\n",
    "\n",
    "@dataclass\n",
    "class MultiLabelHeadConfig:\n",
    "    \"\"\"Multi-label classification head configuration.\"\"\"\n",
    "    type: str = \"multi_label\"\n",
    "    labels: List[str] = field(default_factory=list)\n",
    "    layers: HeadLayersConfig = field(default_factory=HeadLayersConfig)\n",
    "    classifier_dropout: float = 0.1\n",
    "    loss: str = \"bce\"\n",
    "    label_smoothing: float = 0.0\n",
    "    pos_weight: List[float] = field(default_factory=list)\n",
    "    thresholds: Dict[str, float] = field(default_factory=lambda: {\n",
    "        \"ANHEDONIA\": 0.5, \"APPETITE_CHANGE\": 0.5, \"COGNITIVE_ISSUES\": 0.5,\n",
    "        \"DEPRESSED_MOOD\": 0.5, \"FATIGUE\": 0.5, \"PSYCHOMOTOR\": 0.5,\n",
    "        \"SLEEP_ISSUES\": 0.5, \"SPECIAL_CASE\": 0.5, \"SUICIDAL_THOUGHTS\": 0.5,\n",
    "        \"WORTHLESSNESS\": 0.5\n",
    "    })\n",
    "\n",
    "@dataclass\n",
    "class TokenHeadConfig:\n",
    "    \"\"\"Token classification head configuration.\"\"\"\n",
    "    enabled: bool = False\n",
    "    type: str = \"token_classification\"\n",
    "    num_labels: int = 2\n",
    "    layers: HeadLayersConfig = field(default_factory=HeadLayersConfig)\n",
    "    classifier_dropout: float = 0.1\n",
    "    loss: str = \"cross_entropy\"\n",
    "    ignore_index: int = -100\n",
    "    class_weights: List[float] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class SpanHeadConfig:\n",
    "    \"\"\"Span classification head configuration.\"\"\"\n",
    "    enabled: bool = False\n",
    "    type: str = \"span_classification\"\n",
    "    layers: HeadLayersConfig = field(default_factory=HeadLayersConfig)\n",
    "    classifier_dropout: float = 0.1\n",
    "    loss: str = \"cross_entropy\"\n",
    "    ignore_index: int = -100\n",
    "\n",
    "@dataclass\n",
    "class ModelHeadsConfig:\n",
    "    \"\"\"Model heads configuration.\"\"\"\n",
    "    symptom_labels: MultiLabelHeadConfig = field(default_factory=MultiLabelHeadConfig)\n",
    "    evidence_token: TokenHeadConfig = field(default_factory=TokenHeadConfig)\n",
    "    evidence_span: SpanHeadConfig = field(default_factory=SpanHeadConfig)\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration.\"\"\"\n",
    "    encoder: EncoderConfig = field(default_factory=EncoderConfig)\n",
    "    heads: ModelHeadsConfig = field(default_factory=ModelHeadsConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    \"\"\"Optimizer configuration.\"\"\"\n",
    "    name: str = \"adamw\"\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    eps: float = 1e-8\n",
    "    layerwise_lr_decay: float = 1.0\n",
    "\n",
    "@dataclass\n",
    "class SchedulerConfig:\n",
    "    \"\"\"Scheduler configuration.\"\"\"\n",
    "    name: str = \"linear\"\n",
    "    warmup_ratio: float = 0.1\n",
    "    cosine_cycles: float = 0.5\n",
    "    onecycle_max_lr: float = 5e-5\n",
    "    onecycle_pct_start: float = 0.3\n",
    "    plateau_patience: int = 2\n",
    "    plateau_factor: float = 0.5\n",
    "    polynomial_power: float = 1.0\n",
    "\n",
    "@dataclass\n",
    "class FocalLossConfig:\n",
    "    \"\"\"Focal loss configuration.\"\"\"\n",
    "    initial_gamma: float = 2.0\n",
    "    target_positive_rate: float = 0.25\n",
    "    alpha: float = 0.25\n",
    "    min_gamma: float = 1.0\n",
    "    max_gamma: float = 5.0\n",
    "\n",
    "@dataclass\n",
    "class EarlyStoppingConfig:\n",
    "    \"\"\"Early stopping configuration.\"\"\"\n",
    "    patience: int = 3\n",
    "    monitor: str = \"val_symptom_labels_macro_f1\"\n",
    "    min_delta: float = 0.001\n",
    "\n",
    "@dataclass\n",
    "class LossWeightsConfig:\n",
    "    \"\"\"Loss weights configuration.\"\"\"\n",
    "    symptom_labels: float = 1.0\n",
    "    evidence_token: float = 1.0\n",
    "    evidence_span_start: float = 1.0\n",
    "    evidence_span_end: float = 1.0\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration.\"\"\"\n",
    "    batch_size: int = 8\n",
    "    val_batch_size: int = 16\n",
    "    test_batch_size: int = 16\n",
    "    num_workers: int = 8\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    max_epochs: int = 100\n",
    "    optimizer: OptimizerConfig = field(default_factory=OptimizerConfig)\n",
    "    scheduler: SchedulerConfig = field(default_factory=SchedulerConfig)\n",
    "    max_grad_norm: float = 1.0\n",
    "    amp: bool = True\n",
    "    bf16: bool = True\n",
    "    logging_interval: int = 50\n",
    "    ema_decay: float = 0.0\n",
    "    focal: FocalLossConfig = field(default_factory=FocalLossConfig)\n",
    "    early_stopping: EarlyStoppingConfig = field(default_factory=EarlyStoppingConfig)\n",
    "    loss_weights: LossWeightsConfig = field(default_factory=LossWeightsConfig)\n",
    "    # Checkpoint configuration\n",
    "    checkpoint_dir: str = \"./checkpoints\"\n",
    "    save_every_n_epochs: int = 5\n",
    "    keep_last_n_checkpoints: int = 3\n",
    "    auto_resume: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HPOSamplerConfig:\n",
    "    \"\"\"HPO sampler configuration.\"\"\"\n",
    "    _target_: str = \"optuna.samplers.TPESampler\"\n",
    "    seed: int = 42\n",
    "    multivariate: bool = True\n",
    "\n",
    "@dataclass\n",
    "class HPOPrunerConfig:\n",
    "    \"\"\"HPO pruner configuration.\"\"\"\n",
    "    _target_: str = \"optuna.pruners.MedianPruner\"\n",
    "    n_startup_trials: int = 5\n",
    "    n_warmup_steps: int = 100\n",
    "\n",
    "@dataclass\n",
    "class HPOConfig:\n",
    "    \"\"\"Hyperparameter optimization configuration.\"\"\"\n",
    "    study_name: str = \"redsm5_hpo\"\n",
    "    storage: str = \"sqlite:///optuna.db\"\n",
    "    direction: str = \"maximize\"\n",
    "    n_trials: int = 500\n",
    "    n_jobs: int = 1\n",
    "    timeout: Optional[int] = None\n",
    "    sampler: HPOSamplerConfig = field(default_factory=HPOSamplerConfig)\n",
    "    pruner: HPOPrunerConfig = field(default_factory=HPOPrunerConfig)\n",
    "    # Auto-resume configuration\n",
    "    auto_resume: bool = True\n",
    "    checkpoint_interval: int = 10  # Save study state every N trials\n",
    "\n",
    "@dataclass\n",
    "class MLflowConfig:\n",
    "    \"\"\"MLflow configuration.\"\"\"\n",
    "    tracking_uri: str = \"http://127.0.0.1:5000\"\n",
    "    experiment_name: str = \"redsm5_classification\"\n",
    "    nested: bool = False\n",
    "    autolog: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Complete experiment configuration.\"\"\"\n",
    "    seed: int = 1337\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    hpo: HPOConfig = field(default_factory=HPOConfig)\n",
    "    mlflow: MLflowConfig = field(default_factory=MLflowConfig)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Post-initialization to set up dependent fields.\"\"\"\n",
    "        # Set multi-label fields in model head\n",
    "        self.model.heads.symptom_labels.labels = self.data.multi_label_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Manager\n",
    "\n",
    "A comprehensive configuration manager with save/load capabilities and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"Manages experiment configurations with save/load and validation capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, config_dir: str = \"./configs_notebook\"):\n",
    "        self.config_dir = Path(config_dir)\n",
    "        self.config_dir.mkdir(exist_ok=True)\n",
    "        self.current_config: Optional[ExperimentConfig] = None\n",
    "        \n",
    "    def create_default_config(self) -> ExperimentConfig:\n",
    "        \"\"\"Create a default configuration.\"\"\"\n",
    "        return ExperimentConfig()\n",
    "    \n",
    "    def load_config(self, config_name: str) -> ExperimentConfig:\n",
    "        \"\"\"Load configuration from file.\"\"\"\n",
    "        config_path = self.config_dir / f\"{config_name}.yaml\"\n",
    "        if not config_path.exists():\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "        \n",
    "        with open(config_path, 'r') as f:\n",
    "            config_dict = yaml.safe_load(f)\n",
    "        \n",
    "        # Convert dict to ExperimentConfig\n",
    "        self.current_config = self._dict_to_config(config_dict)\n",
    "        return self.current_config\n",
    "    \n",
    "    def save_config(self, config: ExperimentConfig, config_name: str) -> None:\n",
    "        \"\"\"Save configuration to file.\"\"\"\n",
    "        config_path = self.config_dir / f\"{config_name}.yaml\"\n",
    "        config_dict = asdict(config)\n",
    "        \n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(config_dict, f, default_flow_style=False, indent=2)\n",
    "        \n",
    "        print(f\"Configuration saved to: {config_path}\")\n",
    "    \n",
    "    def _dict_to_config(self, config_dict: Dict[str, Any]) -> ExperimentConfig:\n",
    "        \"\"\"Convert dictionary to ExperimentConfig.\"\"\"\n",
    "        # This is a simplified conversion - in practice, you might want more robust handling\n",
    "        return ExperimentConfig(**config_dict)\n",
    "    \n",
    "    def list_configs(self) -> List[str]:\n",
    "        \"\"\"List available configuration files.\"\"\"\n",
    "        return [f.stem for f in self.config_dir.glob(\"*.yaml\")]\n",
    "    \n",
    "    def validate_config(self, config: ExperimentConfig) -> List[str]:\n",
    "        \"\"\"Validate configuration and return list of issues.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Validate data paths\n",
    "        if not Path(config.data.groundtruth_path).exists():\n",
    "            issues.append(f\"Groundtruth file not found: {config.data.groundtruth_path}\")\n",
    "        if not Path(config.data.posts_path).exists():\n",
    "            issues.append(f\"Posts file not found: {config.data.posts_path}\")\n",
    "        \n",
    "        # Validate model configuration\n",
    "        if config.model.encoder.type not in [\"bert\", \"roberta\", \"deberta\"]:\n",
    "            issues.append(f\"Unsupported encoder type: {config.model.encoder.type}\")\n",
    "        \n",
    "        # Validate training parameters\n",
    "        if config.training.batch_size <= 0:\n",
    "            issues.append(\"Batch size must be positive\")\n",
    "        if config.training.optimizer.learning_rate <= 0:\n",
    "            issues.append(\"Learning rate must be positive\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def get_config_summary(self, config: ExperimentConfig) -> pd.DataFrame:\n",
    "        \"\"\"Get a summary of the configuration as a DataFrame.\"\"\"\n",
    "        def flatten_dict(d, parent_key='', sep='.'):\n",
    "            items = []\n",
    "            for k, v in d.items():\n",
    "                new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "                if isinstance(v, dict):\n",
    "                    items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "                else:\n",
    "                    items.append((new_key, v))\n",
    "            return dict(items)\n",
    "        \n",
    "        config_dict = asdict(config)\n",
    "        flat_dict = flatten_dict(config_dict)\n",
    "        \n",
    "        df = pd.DataFrame(list(flat_dict.items()), columns=['Parameter', 'Value'])\n",
    "        return df\n",
    "\n",
    "# Initialize configuration manager\n",
    "config_manager = ConfigurationManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Configuration Builder\n",
    "\n",
    "Create and modify configurations interactively using widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_config_builder():\n",
    "    \"\"\"Create an interactive configuration builder using ipywidgets.\"\"\"\n",
    "    \n",
    "    # Model configuration widgets\n",
    "    encoder_type = widgets.Dropdown(\n",
    "        options=['roberta', 'bert', 'deberta'],\n",
    "        value='roberta',\n",
    "        description='Encoder:'\n",
    "    )\n",
    "    \n",
    "    model_name = widgets.Dropdown(\n",
    "        options={\n",
    "            'RoBERTa Base': 'roberta-base',\n",
    "            'BERT Base': 'bert-base-uncased',\n",
    "            'DeBERTa Base': 'microsoft/deberta-base'\n",
    "        },\n",
    "        value='roberta-base',\n",
    "        description='Model:'\n",
    "    )\n",
    "    \n",
    "    # Training configuration widgets\n",
    "    batch_size = widgets.IntSlider(\n",
    "        value=8, min=1, max=128, step=1,\n",
    "        description='Batch Size:'\n",
    "    )\n",
    "    \n",
    "    learning_rate = widgets.FloatLogSlider(\n",
    "        value=2e-5, base=10, min=-6, max=-3,\n",
    "        description='Learning Rate:'\n",
    "    )\n",
    "    \n",
    "    max_epochs = widgets.IntSlider(\n",
    "        value=100, min=1, max=500, step=1,\n",
    "        description='Max Epochs:'\n",
    "    )\n",
    "    \n",
    "    # Data configuration widgets\n",
    "    max_length = widgets.Dropdown(\n",
    "        options=[128, 256, 384, 512],\n",
    "        value=256,\n",
    "        description='Max Length:'\n",
    "    )\n",
    "    \n",
    "    # Advanced options\n",
    "    use_lora = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Use LoRA'\n",
    "    )\n",
    "    \n",
    "    gradient_checkpointing = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Gradient Checkpointing'\n",
    "    )\n",
    "    \n",
    "    auto_resume = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Auto Resume'\n",
    "    )\n",
    "    \n",
    "    # Configuration name\n",
    "    config_name = widgets.Text(\n",
    "        value='default_config',\n",
    "        description='Config Name:'\n",
    "    )\n",
    "    \n",
    "    # Build configuration button\n",
    "    build_button = widgets.Button(\n",
    "        description='Build Configuration',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    # Output area\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_build_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            # Create configuration\n",
    "            config = ExperimentConfig()\n",
    "            \n",
    "            # Update model configuration\n",
    "            config.model.encoder.type = encoder_type.value\n",
    "            config.model.encoder.pretrained_model_name_or_path = model_name.value\n",
    "            config.model.encoder.gradient_checkpointing = gradient_checkpointing.value\n",
    "            config.model.encoder.lora.enabled = use_lora.value\n",
    "            \n",
    "            # Update training configuration\n",
    "            config.training.batch_size = batch_size.value\n",
    "            config.training.optimizer.learning_rate = learning_rate.value\n",
    "            config.training.max_epochs = max_epochs.value\n",
    "            config.training.auto_resume = auto_resume.value\n",
    "            \n",
    "            # Update data configuration\n",
    "            config.data.max_length = max_length.value\n",
    "            \n",
    "            # Validate configuration\n",
    "            issues = config_manager.validate_config(config)\n",
    "            if issues:\n",
    "                print(\"Configuration Issues:\")\n",
    "                for issue in issues:\n",
    "                    print(f\"  - {issue}\")\n",
    "                print()\n",
    "            \n",
    "            # Save configuration\n",
    "            config_manager.save_config(config, config_name.value)\n",
    "            config_manager.current_config = config\n",
    "            \n",
    "            # Display summary\n",
    "            print(\"Configuration Summary:\")\n",
    "            summary_df = config_manager.get_config_summary(config)\n",
    "            display(summary_df.head(20))  # Show first 20 parameters\n",
    "    \n",
    "    build_button.on_click(on_build_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    model_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Model Configuration</h3>\"),\n",
    "        encoder_type, model_name, use_lora, gradient_checkpointing\n",
    "    ])\n",
    "    \n",
    "    training_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Training Configuration</h3>\"),\n",
    "        batch_size, learning_rate, max_epochs, auto_resume\n",
    "    ])\n",
    "    \n",
    "    data_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Data Configuration</h3>\"),\n",
    "        max_length\n",
    "    ])\n",
    "    \n",
    "    config_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Save Configuration</h3>\"),\n",
    "        config_name, build_button\n",
    "    ])\n",
    "    \n",
    "    main_layout = widgets.HBox([\n",
    "        widgets.VBox([model_box, data_box]),\n",
    "        widgets.VBox([training_box, config_box])\n",
    "    ])\n",
    "    \n",
    "    return widgets.VBox([main_layout, output])\n",
    "\n",
    "# Display the interactive configuration builder\n",
    "print(\"Interactive Configuration Builder\")\n",
    "print(\"=\" * 40)\n",
    "config_builder = create_interactive_config_builder()\n",
    "display(config_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Presets\n",
    "\n",
    "Pre-defined configurations for common use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preset_configs():\n",
    "    \"\"\"Create and save preset configurations.\"\"\"\n",
    "    \n",
    "    # Quick training preset\n",
    "    quick_config = ExperimentConfig()\n",
    "    quick_config.training.max_epochs = 10\n",
    "    quick_config.training.batch_size = 16\n",
    "    quick_config.training.early_stopping.patience = 2\n",
    "    quick_config.data.max_length = 128\n",
    "    config_manager.save_config(quick_config, \"quick_training\")\n",
    "    \n",
    "    # High-performance preset\n",
    "    hp_config = ExperimentConfig()\n",
    "    hp_config.model.encoder.type = \"deberta\"\n",
    "    hp_config.model.encoder.pretrained_model_name_or_path = \"microsoft/deberta-base\"\n",
    "    hp_config.training.batch_size = 32\n",
    "    hp_config.training.optimizer.learning_rate = 1e-5\n",
    "    hp_config.training.max_epochs = 50\n",
    "    hp_config.data.max_length = 384\n",
    "    config_manager.save_config(hp_config, \"high_performance\")\n",
    "    \n",
    "    # LoRA fine-tuning preset\n",
    "    lora_config = ExperimentConfig()\n",
    "    lora_config.model.encoder.lora.enabled = True\n",
    "    lora_config.model.encoder.lora.r = 16\n",
    "    lora_config.model.encoder.lora.alpha = 32\n",
    "    lora_config.training.batch_size = 64\n",
    "    lora_config.training.optimizer.learning_rate = 5e-4\n",
    "    config_manager.save_config(lora_config, \"lora_finetuning\")\n",
    "    \n",
    "    # HPO preset\n",
    "    hpo_config = ExperimentConfig()\n",
    "    hpo_config.hpo.n_trials = 100\n",
    "    hpo_config.hpo.n_jobs = 2\n",
    "    hpo_config.training.max_epochs = 20\n",
    "    hpo_config.training.early_stopping.patience = 2\n",
    "    config_manager.save_config(hpo_config, \"hpo_search\")\n",
    "    \n",
    "    print(\"Created preset configurations:\")\n",
    "    for config_name in [\"quick_training\", \"high_performance\", \"lora_finetuning\", \"hpo_search\"]:\n",
    "        print(f\"  - {config_name}\")\n",
    "\n",
    "# Create preset configurations\n",
    "create_preset_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Utilities\n",
    "\n",
    "Utility functions for working with configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_config(config_name: str):\n",
    "    \"\"\"Load and display a configuration.\"\"\"\n",
    "    try:\n",
    "        config = config_manager.load_config(config_name)\n",
    "        print(f\"Loaded configuration: {config_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Display summary\n",
    "        summary_df = config_manager.get_config_summary(config)\n",
    "        display(summary_df)\n",
    "        \n",
    "        # Validate\n",
    "        issues = config_manager.validate_config(config)\n",
    "        if issues:\n",
    "            print(\"\\nValidation Issues:\")\n",
    "            for issue in issues:\n",
    "                print(f\"  âš ï¸  {issue}\")\n",
    "        else:\n",
    "            print(\"\\nâœ… Configuration is valid\")\n",
    "            \n",
    "        return config\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_configs(config1_name: str, config2_name: str):\n",
    "    \"\"\"Compare two configurations.\"\"\"\n",
    "    try:\n",
    "        config1 = config_manager.load_config(config1_name)\n",
    "        config2 = config_manager.load_config(config2_name)\n",
    "        \n",
    "        df1 = config_manager.get_config_summary(config1)\n",
    "        df2 = config_manager.get_config_summary(config2)\n",
    "        \n",
    "        # Merge dataframes for comparison\n",
    "        comparison = df1.merge(df2, on='Parameter', suffixes=(f'_{config1_name}', f'_{config2_name}'))\n",
    "        \n",
    "        # Find differences\n",
    "        differences = comparison[comparison[f'Value_{config1_name}'] != comparison[f'Value_{config2_name}']]\n",
    "        \n",
    "        if len(differences) > 0:\n",
    "            print(f\"Differences between {config1_name} and {config2_name}:\")\n",
    "            display(differences)\n",
    "        else:\n",
    "            print(f\"No differences found between {config1_name} and {config2_name}\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ {e}\")\n",
    "\n",
    "def list_available_configs():\n",
    "    \"\"\"List all available configurations.\"\"\"\n",
    "    configs = config_manager.list_configs()\n",
    "    if configs:\n",
    "        print(\"Available configurations:\")\n",
    "        for config in configs:\n",
    "            print(f\"  ðŸ“„ {config}\")\n",
    "    else:\n",
    "        print(\"No configurations found\")\n",
    "    return configs\n",
    "\n",
    "# Display available configurations\n",
    "list_available_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Configuration\n",
    "\n",
    "Export configuration to different formats for use in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_config_for_training(config_name: str) -> ExperimentConfig:\n",
    "    \"\"\"Export configuration for use in training notebook.\"\"\"\n",
    "    config = config_manager.load_config(config_name)\n",
    "    print(f\"âœ… Configuration '{config_name}' ready for training\")\n",
    "    print(f\"   Model: {config.model.encoder.type} ({config.model.encoder.pretrained_model_name_or_path})\")\n",
    "    print(f\"   Batch size: {config.training.batch_size}\")\n",
    "    print(f\"   Learning rate: {config.training.optimizer.learning_rate}\")\n",
    "    print(f\"   Max epochs: {config.training.max_epochs}\")\n",
    "    print(f\"   Auto-resume: {config.training.auto_resume}\")\n",
    "    return config\n",
    "\n",
    "def export_config_for_hpo(config_name: str) -> ExperimentConfig:\n",
    "    \"\"\"Export configuration for use in HPO notebook.\"\"\"\n",
    "    config = config_manager.load_config(config_name)\n",
    "    print(f\"âœ… Configuration '{config_name}' ready for HPO\")\n",
    "    print(f\"   Study: {config.hpo.study_name}\")\n",
    "    print(f\"   Trials: {config.hpo.n_trials}\")\n",
    "    print(f\"   Jobs: {config.hpo.n_jobs}\")\n",
    "    print(f\"   Storage: {config.hpo.storage}\")\n",
    "    print(f\"   Auto-resume: {config.hpo.auto_resume}\")\n",
    "    return config\n",
    "\n",
    "# Example usage\n",
    "print(\"Configuration Management Setup Complete!\")\n",
    "print(\"\\nTo use configurations in other notebooks:\")\n",
    "print(\"1. Run this notebook to set up configurations\")\n",
    "print(\"2. Use export_config_for_training('config_name') in training notebook\")\n",
    "print(\"3. Use export_config_for_hpo('config_name') in HPO notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}