{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Exploration\n",
    "\n",
    "This notebook provides comprehensive data loading, preprocessing, and exploration capabilities for the Criteria Evidence Agent project.\n",
    "It includes interactive data analysis, preprocessing pipelines, and data validation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import all necessary libraries for data processing and exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# NLP and ML libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration and Dependencies\n",
    "\n",
    "Load configuration management and data modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration management\n",
    "%run 01_Configuration_Management.ipynb\n",
    "\n",
    "# Import project modules\n",
    "from src.data.dataset import DataModule, EvidenceDataset\n",
    "from src.data.preprocessing import preprocess_text, extract_evidence_spans\n",
    "\n",
    "print(\"‚úÖ Configuration and data modules loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Configuration and Loading\n",
    "\n",
    "Configure data paths and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_config_selector():\n",
    "    \"\"\"Create an interactive data configuration selector.\"\"\"\n",
    "    \n",
    "    # Data path widgets\n",
    "    groundtruth_path = widgets.Text(\n",
    "        value=\"./Data/groundtruth/redsm5_ground_truth.json\",\n",
    "        description='Groundtruth:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    posts_path = widgets.Text(\n",
    "        value=\"./Data/redsm5/redsm5_posts.csv\",\n",
    "        description='Posts CSV:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Data parameters\n",
    "    max_length = widgets.Dropdown(\n",
    "        options=[128, 256, 384, 512],\n",
    "        value=256,\n",
    "        description='Max Length:'\n",
    "    )\n",
    "    \n",
    "    val_size = widgets.FloatSlider(\n",
    "        value=0.15,\n",
    "        min=0.1,\n",
    "        max=0.3,\n",
    "        step=0.05,\n",
    "        description='Val Size:'\n",
    "    )\n",
    "    \n",
    "    test_size = widgets.FloatSlider(\n",
    "        value=0.15,\n",
    "        min=0.1,\n",
    "        max=0.3,\n",
    "        step=0.05,\n",
    "        description='Test Size:'\n",
    "    )\n",
    "    \n",
    "    seed = widgets.IntText(\n",
    "        value=42,\n",
    "        description='Random Seed:'\n",
    "    )\n",
    "    \n",
    "    load_button = widgets.Button(\n",
    "        description='Load Data',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_load_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                # Create data configuration\n",
    "                data_config = DataConfig(\n",
    "                    groundtruth_path=groundtruth_path.value,\n",
    "                    posts_path=posts_path.value,\n",
    "                    max_length=max_length.value,\n",
    "                    val_size=val_size.value,\n",
    "                    test_size=test_size.value,\n",
    "                    seed=seed.value\n",
    "                )\n",
    "                \n",
    "                # Store in global variable\n",
    "                global current_data_config\n",
    "                current_data_config = data_config\n",
    "                \n",
    "                print(f\"‚úÖ Data configuration created!\")\n",
    "                print(f\"   Groundtruth: {data_config.groundtruth_path}\")\n",
    "                print(f\"   Posts: {data_config.posts_path}\")\n",
    "                print(f\"   Max length: {data_config.max_length}\")\n",
    "                print(f\"   Val/Test split: {data_config.val_size}/{data_config.test_size}\")\n",
    "                \n",
    "                # Check if files exist\n",
    "                if not Path(data_config.groundtruth_path).exists():\n",
    "                    print(f\"‚ö†Ô∏è  Groundtruth file not found: {data_config.groundtruth_path}\")\n",
    "                if not Path(data_config.posts_path).exists():\n",
    "                    print(f\"‚ö†Ô∏è  Posts file not found: {data_config.posts_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error creating data configuration: {e}\")\n",
    "    \n",
    "    load_button.on_click(on_load_clicked)\n",
    "    \n",
    "    layout = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Data Configuration</h3>\"),\n",
    "        groundtruth_path,\n",
    "        posts_path,\n",
    "        widgets.HBox([max_length, val_size]),\n",
    "        widgets.HBox([test_size, seed]),\n",
    "        load_button,\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return layout\n",
    "\n",
    "# Display data configuration selector\n",
    "data_config_selector = create_data_config_selector()\n",
    "display(data_config_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration\n",
    "\n",
    "Load the dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data():\n",
    "    \"\"\"Load and perform initial exploration of the dataset.\"\"\"\n",
    "    \n",
    "    if 'current_data_config' not in globals():\n",
    "        print(\"‚ùå Please configure and load data first using the selector above.\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(\"üìä Loading dataset...\")\n",
    "        \n",
    "        # Load groundtruth data\n",
    "        with open(current_data_config.groundtruth_path, 'r') as f:\n",
    "            groundtruth_data = json.load(f)\n",
    "        \n",
    "        # Load posts data\n",
    "        posts_df = pd.read_csv(current_data_config.posts_path)\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded successfully!\")\n",
    "        print(f\"   Groundtruth entries: {len(groundtruth_data)}\")\n",
    "        print(f\"   Posts entries: {len(posts_df)}\")\n",
    "        \n",
    "        # Basic data exploration\n",
    "        print(f\"\\nüìã Posts DataFrame Info:\")\n",
    "        print(f\"   Shape: {posts_df.shape}\")\n",
    "        print(f\"   Columns: {list(posts_df.columns)}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_values = posts_df.isnull().sum()\n",
    "        if missing_values.sum() > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  Missing values found:\")\n",
    "            for col, count in missing_values[missing_values > 0].items():\n",
    "                print(f\"   {col}: {count} ({count/len(posts_df)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No missing values found\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nüìÑ Sample posts:\")\n",
    "        display(posts_df.head())\n",
    "        \n",
    "        # Analyze groundtruth structure\n",
    "        print(f\"\\nüéØ Groundtruth Analysis:\")\n",
    "        if groundtruth_data:\n",
    "            sample_entry = list(groundtruth_data.values())[0]\n",
    "            print(f\"   Sample entry keys: {list(sample_entry.keys())}\")\n",
    "            \n",
    "            # Count labels\n",
    "            all_labels = []\n",
    "            for entry in groundtruth_data.values():\n",
    "                if 'labels' in entry:\n",
    "                    all_labels.extend(entry['labels'])\n",
    "            \n",
    "            label_counts = Counter(all_labels)\n",
    "            print(f\"   Total label instances: {len(all_labels)}\")\n",
    "            print(f\"   Unique labels: {len(label_counts)}\")\n",
    "            print(f\"   Label distribution:\")\n",
    "            for label, count in label_counts.most_common():\n",
    "                print(f\"     {label}: {count}\")\n",
    "        \n",
    "        return posts_df, groundtruth_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Load and explore data\n",
    "posts_df, groundtruth_data = load_and_explore_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis and Statistics\n",
    "\n",
    "Analyze text characteristics and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_statistics(posts_df: pd.DataFrame):\n",
    "    \"\"\"Analyze text statistics and characteristics.\"\"\"\n",
    "    \n",
    "    if posts_df is None:\n",
    "        print(\"‚ùå No data loaded. Please load data first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Text Statistics Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Assume text column is 'text' or find it\n",
    "    text_column = None\n",
    "    for col in ['text', 'content', 'post_text', 'message']:\n",
    "        if col in posts_df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        print(\"‚ùå No text column found in the dataset\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Using text column: '{text_column}'\")\n",
    "    \n",
    "    # Calculate text statistics\n",
    "    texts = posts_df[text_column].dropna()\n",
    "    \n",
    "    # Character counts\n",
    "    char_counts = texts.str.len()\n",
    "    \n",
    "    # Word counts\n",
    "    word_counts = texts.str.split().str.len()\n",
    "    \n",
    "    # Sentence counts (approximate)\n",
    "    sentence_counts = texts.str.count(r'[.!?]+') + 1\n",
    "    \n",
    "    print(f\"\\nüìà Text Length Statistics:\")\n",
    "    print(f\"   Total texts: {len(texts)}\")\n",
    "    print(f\"   Character count - Mean: {char_counts.mean():.1f}, Median: {char_counts.median():.1f}\")\n",
    "    print(f\"   Character count - Min: {char_counts.min()}, Max: {char_counts.max()}\")\n",
    "    print(f\"   Word count - Mean: {word_counts.mean():.1f}, Median: {word_counts.median():.1f}\")\n",
    "    print(f\"   Word count - Min: {word_counts.min()}, Max: {word_counts.max()}\")\n",
    "    print(f\"   Sentence count - Mean: {sentence_counts.mean():.1f}, Median: {sentence_counts.median():.1f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Text Statistics Analysis', fontsize=16)\n",
    "    \n",
    "    # Character count distribution\n",
    "    axes[0, 0].hist(char_counts, bins=50, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].axvline(char_counts.mean(), color='red', linestyle='--', label=f'Mean: {char_counts.mean():.1f}')\n",
    "    axes[0, 0].axvline(char_counts.median(), color='green', linestyle='--', label=f'Median: {char_counts.median():.1f}')\n",
    "    axes[0, 0].set_xlabel('Character Count')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Character Count Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Word count distribution\n",
    "    axes[0, 1].hist(word_counts, bins=50, alpha=0.7, color='lightcoral')\n",
    "    axes[0, 1].axvline(word_counts.mean(), color='red', linestyle='--', label=f'Mean: {word_counts.mean():.1f}')\n",
    "    axes[0, 1].axvline(word_counts.median(), color='green', linestyle='--', label=f'Median: {word_counts.median():.1f}')\n",
    "    axes[0, 1].set_xlabel('Word Count')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Word Count Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Box plots\n",
    "    axes[1, 0].boxplot([char_counts], labels=['Characters'])\n",
    "    axes[1, 0].set_title('Character Count Box Plot')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    \n",
    "    axes[1, 1].boxplot([word_counts], labels=['Words'])\n",
    "    axes[1, 1].set_title('Word Count Box Plot')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tokenization analysis with different max_lengths\n",
    "    print(f\"\\nüî§ Tokenization Analysis:\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Sample a subset for tokenization analysis\n",
    "        sample_texts = texts.sample(min(1000, len(texts)), random_state=42)\n",
    "        \n",
    "        token_counts = []\n",
    "        for text in tqdm(sample_texts, desc=\"Tokenizing\"):\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "            token_counts.append(len(tokens))\n",
    "        \n",
    "        token_counts = np.array(token_counts)\n",
    "        \n",
    "        print(f\"   Token count - Mean: {token_counts.mean():.1f}, Median: {np.median(token_counts):.1f}\")\n",
    "        print(f\"   Token count - Min: {token_counts.min()}, Max: {token_counts.max()}\")\n",
    "        \n",
    "        # Check truncation rates for different max_lengths\n",
    "        for max_len in [128, 256, 384, 512]:\n",
    "            truncated = (token_counts > max_len).sum()\n",
    "            truncation_rate = truncated / len(token_counts) * 100\n",
    "            print(f\"   Max length {max_len}: {truncation_rate:.1f}% would be truncated\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Tokenization analysis failed: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'char_counts': char_counts,\n",
    "        'word_counts': word_counts,\n",
    "        'sentence_counts': sentence_counts\n",
    "    }\n",
    "\n",
    "# Analyze text statistics\n",
    "if posts_df is not None:\n",
    "    text_stats = analyze_text_statistics(posts_df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Load data first to perform text analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Analysis and Distribution\n",
    "\n",
    "Analyze label distribution and multi-label characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_label_distribution(groundtruth_data: Dict):\n",
    "    \"\"\"Analyze label distribution and multi-label characteristics.\"\"\"\n",
    "    \n",
    "    if groundtruth_data is None:\n",
    "        print(\"‚ùå No groundtruth data loaded. Please load data first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üè∑Ô∏è  Label Distribution Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Extract all labels and create label matrix\n",
    "    all_labels = []\n",
    "    label_combinations = []\n",
    "    post_ids = []\n",
    "    \n",
    "    for post_id, entry in groundtruth_data.items():\n",
    "        if 'labels' in entry:\n",
    "            labels = entry['labels']\n",
    "            all_labels.extend(labels)\n",
    "            label_combinations.append(labels)\n",
    "            post_ids.append(post_id)\n",
    "    \n",
    "    # Label frequency analysis\n",
    "    label_counts = Counter(all_labels)\n",
    "    unique_labels = list(label_counts.keys())\n",
    "    \n",
    "    print(f\"\\nüìä Label Statistics:\")\n",
    "    print(f\"   Total posts with labels: {len(label_combinations)}\")\n",
    "    print(f\"   Unique labels: {len(unique_labels)}\")\n",
    "    print(f\"   Total label instances: {len(all_labels)}\")\n",
    "    print(f\"   Average labels per post: {len(all_labels) / len(label_combinations):.2f}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    print(f\"\\nüè∑Ô∏è  Label Frequency:\")\n",
    "    for label, count in label_counts.most_common():\n",
    "        percentage = count / len(label_combinations) * 100\n",
    "        print(f\"   {label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Multi-label analysis\n",
    "    label_counts_per_post = [len(labels) for labels in label_combinations]\n",
    "    label_count_distribution = Counter(label_counts_per_post)\n",
    "    \n",
    "    print(f\"\\nüìà Labels per Post Distribution:\")\n",
    "    for num_labels, count in sorted(label_count_distribution.items()):\n",
    "        percentage = count / len(label_combinations) * 100\n",
    "        print(f\"   {num_labels} labels: {count} posts ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Label Distribution Analysis', fontsize=16)\n",
    "    \n",
    "    # Label frequency bar plot\n",
    "    labels, counts = zip(*label_counts.most_common())\n",
    "    axes[0, 0].bar(range(len(labels)), counts, color='skyblue')\n",
    "    axes[0, 0].set_xlabel('Labels')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Label Frequency Distribution')\n",
    "    axes[0, 0].set_xticks(range(len(labels)))\n",
    "    axes[0, 0].set_xticklabels(labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Label percentage pie chart\n",
    "    axes[0, 1].pie(counts, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Label Distribution (Percentage)')\n",
    "    \n",
    "    # Labels per post distribution\n",
    "    num_labels_list, post_counts = zip(*sorted(label_count_distribution.items()))\n",
    "    axes[1, 0].bar(num_labels_list, post_counts, color='lightcoral')\n",
    "    axes[1, 0].set_xlabel('Number of Labels per Post')\n",
    "    axes[1, 0].set_ylabel('Number of Posts')\n",
    "    axes[1, 0].set_title('Distribution of Labels per Post')\n",
    "    \n",
    "    # Label co-occurrence heatmap\n",
    "    if len(unique_labels) <= 15:  # Only for manageable number of labels\n",
    "        # Create co-occurrence matrix\n",
    "        cooccurrence_matrix = np.zeros((len(unique_labels), len(unique_labels)))\n",
    "        \n",
    "        for labels in label_combinations:\n",
    "            for i, label1 in enumerate(unique_labels):\n",
    "                for j, label2 in enumerate(unique_labels):\n",
    "                    if label1 in labels and label2 in labels:\n",
    "                        cooccurrence_matrix[i, j] += 1\n",
    "        \n",
    "        # Normalize by diagonal (individual label counts)\n",
    "        for i in range(len(unique_labels)):\n",
    "            for j in range(len(unique_labels)):\n",
    "                if i != j and cooccurrence_matrix[i, i] > 0:\n",
    "                    cooccurrence_matrix[i, j] /= cooccurrence_matrix[i, i]\n",
    "        \n",
    "        # Set diagonal to 1\n",
    "        np.fill_diagonal(cooccurrence_matrix, 1.0)\n",
    "        \n",
    "        im = axes[1, 1].imshow(cooccurrence_matrix, cmap='Blues', aspect='auto')\n",
    "        axes[1, 1].set_xticks(range(len(unique_labels)))\n",
    "        axes[1, 1].set_yticks(range(len(unique_labels)))\n",
    "        axes[1, 1].set_xticklabels(unique_labels, rotation=45, ha='right')\n",
    "        axes[1, 1].set_yticklabels(unique_labels)\n",
    "        axes[1, 1].set_title('Label Co-occurrence (Normalized)')\n",
    "        plt.colorbar(im, ax=axes[1, 1])\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Too many labels\\nfor co-occurrence\\nvisualization', \n",
    "                        ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Label Co-occurrence (Skipped)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Class imbalance analysis\n",
    "    print(f\"\\n‚öñÔ∏è  Class Imbalance Analysis:\")\n",
    "    total_posts = len(label_combinations)\n",
    "    for label, count in label_counts.most_common():\n",
    "        positive_ratio = count / total_posts\n",
    "        negative_ratio = 1 - positive_ratio\n",
    "        imbalance_ratio = negative_ratio / positive_ratio if positive_ratio > 0 else float('inf')\n",
    "        print(f\"   {label}: {positive_ratio:.3f} positive, {negative_ratio:.3f} negative (ratio: {imbalance_ratio:.1f}:1)\")\n",
    "    \n",
    "    return {\n",
    "        'label_counts': label_counts,\n",
    "        'unique_labels': unique_labels,\n",
    "        'label_combinations': label_combinations,\n",
    "        'label_count_distribution': label_count_distribution\n",
    "    }\n",
    "\n",
    "# Analyze label distribution\n",
    "if groundtruth_data is not None:\n",
    "    label_analysis = analyze_label_distribution(groundtruth_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Load data first to perform label analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n",
    "\n",
    "Create and test the data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"Create and test the data preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    if 'current_data_config' not in globals():\n",
    "        print(\"‚ùå Please configure data first.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üîß Creating Data Preprocessing Pipeline\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create model config for data module\n",
    "        model_config = ModelConfig()\n",
    "        \n",
    "        # Create data module\n",
    "        data_module = DataModule(current_data_config, model_config)\n",
    "        \n",
    "        print(f\"‚úÖ Data module created successfully!\")\n",
    "        print(f\"   Tokenizer: {data_module.tokenizer.__class__.__name__}\")\n",
    "        print(f\"   Max length: {current_data_config.max_length}\")\n",
    "        \n",
    "        # Load and split data\n",
    "        print(f\"\\nüìä Loading and splitting data...\")\n",
    "        \n",
    "        # Get data splits\n",
    "        train_data, val_data, test_data = data_module.get_data_splits()\n",
    "        \n",
    "        print(f\"   Train samples: {len(train_data)}\")\n",
    "        print(f\"   Validation samples: {len(val_data)}\")\n",
    "        print(f\"   Test samples: {len(test_data)}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        print(f\"\\nüîÑ Creating datasets...\")\n",
    "        \n",
    "        train_dataset = EvidenceDataset(\n",
    "            train_data, \n",
    "            data_module.tokenizer, \n",
    "            current_data_config.max_length,\n",
    "            current_data_config.multi_label_fields\n",
    "        )\n",
    "        \n",
    "        val_dataset = EvidenceDataset(\n",
    "            val_data, \n",
    "            data_module.tokenizer, \n",
    "            current_data_config.max_length,\n",
    "            current_data_config.multi_label_fields\n",
    "        )\n",
    "        \n",
    "        print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "        print(f\"   Val dataset: {len(val_dataset)} samples\")\n",
    "        \n",
    "        # Test preprocessing with sample\n",
    "        print(f\"\\nüß™ Testing preprocessing with sample...\")\n",
    "        \n",
    "        sample = train_dataset[0]\n",
    "        print(f\"   Sample keys: {list(sample.keys())}\")\n",
    "        print(f\"   Input IDs shape: {sample['input_ids'].shape}\")\n",
    "        print(f\"   Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "        \n",
    "        if 'labels' in sample:\n",
    "            print(f\"   Labels shape: {sample['labels'].shape}\")\n",
    "            print(f\"   Labels: {sample['labels']}\")\n",
    "        \n",
    "        # Decode sample to verify tokenization\n",
    "        decoded_text = data_module.tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "        print(f\"\\nüìù Sample decoded text (first 200 chars):\")\n",
    "        print(f\"   {decoded_text[:200]}...\")\n",
    "        \n",
    "        return {\n",
    "            'data_module': data_module,\n",
    "            'train_dataset': train_dataset,\n",
    "            'val_dataset': val_dataset,\n",
    "            'train_data': train_data,\n",
    "            'val_data': val_data,\n",
    "            'test_data': test_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating preprocessing pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessing_result = create_preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "Assess data quality and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(posts_df: pd.DataFrame, groundtruth_data: Dict):\n",
    "    \"\"\"Assess data quality and identify potential issues.\"\"\"\n",
    "    \n",
    "    if posts_df is None or groundtruth_data is None:\n",
    "        print(\"‚ùå No data loaded. Please load data first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîç Data Quality Assessment\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Find text column\n",
    "    text_column = None\n",
    "    id_column = None\n",
    "    \n",
    "    for col in ['text', 'content', 'post_text', 'message']:\n",
    "        if col in posts_df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    for col in ['post_id', 'id', 'ID', 'Post_ID']:\n",
    "        if col in posts_df.columns:\n",
    "            id_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        print(\"‚ùå No text column found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Using text column: '{text_column}'\")\n",
    "    print(f\"Using ID column: '{id_column}'\" if id_column else \"No ID column found\")\n",
    "    \n",
    "    # Basic quality checks\n",
    "    print(f\"\\nüìä Basic Quality Checks:\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_text = posts_df[text_column].isnull().sum()\n",
    "    print(f\"   Missing text values: {missing_text} ({missing_text/len(posts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Empty strings\n",
    "    empty_text = (posts_df[text_column].str.strip() == '').sum()\n",
    "    print(f\"   Empty text values: {empty_text} ({empty_text/len(posts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Very short texts (< 10 characters)\n",
    "    short_text = (posts_df[text_column].str.len() < 10).sum()\n",
    "    print(f\"   Very short texts (<10 chars): {short_text} ({short_text/len(posts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Very long texts (> 1000 characters)\n",
    "    long_text = (posts_df[text_column].str.len() > 1000).sum()\n",
    "    print(f\"   Very long texts (>1000 chars): {long_text} ({long_text/len(posts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Duplicate texts\n",
    "    duplicate_text = posts_df[text_column].duplicated().sum()\n",
    "    print(f\"   Duplicate texts: {duplicate_text} ({duplicate_text/len(posts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Coverage analysis\n",
    "    print(f\"\\nüéØ Coverage Analysis:\")\n",
    "    \n",
    "    if id_column:\n",
    "        posts_ids = set(posts_df[id_column].astype(str))\n",
    "        groundtruth_ids = set(groundtruth_data.keys())\n",
    "        \n",
    "        intersection = posts_ids.intersection(groundtruth_ids)\n",
    "        posts_only = posts_ids - groundtruth_ids\n",
    "        groundtruth_only = groundtruth_ids - posts_ids\n",
    "        \n",
    "        print(f\"   Posts with groundtruth: {len(intersection)} ({len(intersection)/len(posts_ids)*100:.1f}%)\")\n",
    "        print(f\"   Posts without groundtruth: {len(posts_only)} ({len(posts_only)/len(posts_ids)*100:.1f}%)\")\n",
    "        print(f\"   Groundtruth without posts: {len(groundtruth_only)} ({len(groundtruth_only)/len(groundtruth_ids)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   Cannot perform coverage analysis without ID column\")\n",
    "    \n",
    "    # Text quality patterns\n",
    "    print(f\"\\nüìù Text Quality Patterns:\")\n",
    "    \n",
    "    texts = posts_df[text_column].dropna()\n",
    "    \n",
    "    # URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    texts_with_urls = texts.str.contains(url_pattern, regex=True).sum()\n",
    "    print(f\"   Texts with URLs: {texts_with_urls} ({texts_with_urls/len(texts)*100:.1f}%)\")\n",
    "    \n",
    "    # Mentions (@username)\n",
    "    mention_pattern = r'@\\w+'\n",
    "    texts_with_mentions = texts.str.contains(mention_pattern, regex=True).sum()\n",
    "    print(f\"   Texts with mentions: {texts_with_mentions} ({texts_with_mentions/len(texts)*100:.1f}%)\")\n",
    "    \n",
    "    # Hashtags\n",
    "    hashtag_pattern = r'#\\w+'\n",
    "    texts_with_hashtags = texts.str.contains(hashtag_pattern, regex=True).sum()\n",
    "    print(f\"   Texts with hashtags: {texts_with_hashtags} ({texts_with_hashtags/len(texts)*100:.1f}%)\")\n",
    "    \n",
    "    # Special characters ratio\n",
    "    special_char_ratios = texts.str.count(r'[^\\w\\s]') / texts.str.len()\n",
    "    high_special_chars = (special_char_ratios > 0.3).sum()\n",
    "    print(f\"   Texts with high special char ratio (>30%): {high_special_chars} ({high_special_chars/len(texts)*100:.1f}%)\")\n",
    "    \n",
    "    # Language detection (simple heuristic)\n",
    "    non_ascii_chars = texts.str.count(r'[^\\x00-\\x7F]') / texts.str.len()\n",
    "    likely_non_english = (non_ascii_chars > 0.1).sum()\n",
    "    print(f\"   Likely non-English texts (>10% non-ASCII): {likely_non_english} ({likely_non_english/len(texts)*100:.1f}%)\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    \n",
    "    if missing_text > 0:\n",
    "        print(f\"   ‚Ä¢ Consider removing {missing_text} posts with missing text\")\n",
    "    \n",
    "    if empty_text > 0:\n",
    "        print(f\"   ‚Ä¢ Consider removing {empty_text} posts with empty text\")\n",
    "    \n",
    "    if short_text > len(texts) * 0.05:  # More than 5%\n",
    "        print(f\"   ‚Ä¢ High number of very short texts - consider minimum length filtering\")\n",
    "    \n",
    "    if duplicate_text > 0:\n",
    "        print(f\"   ‚Ä¢ Consider deduplication to remove {duplicate_text} duplicate texts\")\n",
    "    \n",
    "    if texts_with_urls > len(texts) * 0.1:  # More than 10%\n",
    "        print(f\"   ‚Ä¢ Consider URL preprocessing/removal for {texts_with_urls} texts\")\n",
    "    \n",
    "    if likely_non_english > len(texts) * 0.05:  # More than 5%\n",
    "        print(f\"   ‚Ä¢ Consider language filtering for {likely_non_english} likely non-English texts\")\n",
    "    \n",
    "    return {\n",
    "        'missing_text': missing_text,\n",
    "        'empty_text': empty_text,\n",
    "        'short_text': short_text,\n",
    "        'long_text': long_text,\n",
    "        'duplicate_text': duplicate_text,\n",
    "        'texts_with_urls': texts_with_urls,\n",
    "        'texts_with_mentions': texts_with_mentions,\n",
    "        'texts_with_hashtags': texts_with_hashtags\n",
    "    }\n",
    "\n",
    "# Assess data quality\n",
    "if posts_df is not None and groundtruth_data is not None:\n",
    "    quality_assessment = assess_data_quality(posts_df, groundtruth_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Load data first to perform quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Data Explorer\n",
    "\n",
    "Interactive widget-based data exploration tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_data_explorer():\n",
    "    \"\"\"Create an interactive data exploration dashboard.\"\"\"\n",
    "    \n",
    "    if posts_df is None or groundtruth_data is None:\n",
    "        print(\"‚ùå No data loaded. Please load data first.\")\n",
    "        return\n",
    "    \n",
    "    # Find text and ID columns\n",
    "    text_column = None\n",
    "    id_column = None\n",
    "    \n",
    "    for col in ['text', 'content', 'post_text', 'message']:\n",
    "        if col in posts_df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    for col in ['post_id', 'id', 'ID', 'Post_ID']:\n",
    "        if col in posts_df.columns:\n",
    "            id_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        print(\"‚ùå No text column found\")\n",
    "        return\n",
    "    \n",
    "    # Create widgets\n",
    "    sample_size = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=100,\n",
    "        description='Sample Size:'\n",
    "    )\n",
    "    \n",
    "    filter_by_labels = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Filter by Labels'\n",
    "    )\n",
    "    \n",
    "    # Get unique labels for filtering\n",
    "    all_labels = []\n",
    "    for entry in groundtruth_data.values():\n",
    "        if 'labels' in entry:\n",
    "            all_labels.extend(entry['labels'])\n",
    "    unique_labels = list(set(all_labels))\n",
    "    \n",
    "    label_filter = widgets.SelectMultiple(\n",
    "        options=unique_labels,\n",
    "        value=[],\n",
    "        description='Labels:',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    search_text = widgets.Text(\n",
    "        value='',\n",
    "        description='Search:',\n",
    "        placeholder='Enter search terms...'\n",
    "    )\n",
    "    \n",
    "    explore_button = widgets.Button(\n",
    "        description='Explore Data',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_filter_change(change):\n",
    "        label_filter.disabled = not change['new']\n",
    "    \n",
    "    filter_by_labels.observe(on_filter_change, names='value')\n",
    "    \n",
    "    def on_explore_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                # Start with all posts\n",
    "                filtered_df = posts_df.copy()\n",
    "                \n",
    "                # Apply search filter\n",
    "                if search_text.value.strip():\n",
    "                    search_terms = search_text.value.strip().lower()\n",
    "                    filtered_df = filtered_df[filtered_df[text_column].str.lower().str.contains(search_terms, na=False)]\n",
    "                \n",
    "                # Apply label filter\n",
    "                if filter_by_labels.value and label_filter.value:\n",
    "                    if id_column:\n",
    "                        # Filter by posts that have the selected labels\n",
    "                        matching_ids = []\n",
    "                        for post_id, entry in groundtruth_data.items():\n",
    "                            if 'labels' in entry:\n",
    "                                post_labels = set(entry['labels'])\n",
    "                                selected_labels = set(label_filter.value)\n",
    "                                if selected_labels.intersection(post_labels):\n",
    "                                    matching_ids.append(post_id)\n",
    "                        \n",
    "                        filtered_df = filtered_df[filtered_df[id_column].astype(str).isin(matching_ids)]\n",
    "                \n",
    "                print(f\"üìä Filtered Results: {len(filtered_df)} posts\")\n",
    "                \n",
    "                if len(filtered_df) == 0:\n",
    "                    print(\"No posts match the current filters.\")\n",
    "                    return\n",
    "                \n",
    "                # Sample data\n",
    "                sample_df = filtered_df.sample(min(sample_size.value, len(filtered_df)), random_state=42)\n",
    "                \n",
    "                print(f\"\\nüìÑ Sample of {len(sample_df)} posts:\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                for idx, (_, row) in enumerate(sample_df.iterrows(), 1):\n",
    "                    post_id = row[id_column] if id_column else f\"Post_{idx}\"\n",
    "                    text = row[text_column]\n",
    "                    \n",
    "                    print(f\"\\n{idx}. Post ID: {post_id}\")\n",
    "                    print(f\"   Length: {len(text)} characters\")\n",
    "                    \n",
    "                    # Show labels if available\n",
    "                    if str(post_id) in groundtruth_data:\n",
    "                        entry = groundtruth_data[str(post_id)]\n",
    "                        if 'labels' in entry:\n",
    "                            print(f\"   Labels: {', '.join(entry['labels'])}\")\n",
    "                    \n",
    "                    # Show text (truncated)\n",
    "                    display_text = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                    print(f\"   Text: {display_text}\")\n",
    "                    print(\"-\" * 50)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error exploring data: {e}\")\n",
    "    \n",
    "    explore_button.on_click(on_explore_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Interactive Data Explorer</h3>\"),\n",
    "        widgets.HBox([sample_size, filter_by_labels]),\n",
    "        label_filter,\n",
    "        search_text,\n",
    "        explore_button\n",
    "    ])\n",
    "    \n",
    "    return widgets.VBox([controls, output])\n",
    "\n",
    "# Create and display interactive explorer\n",
    "if posts_df is not None and groundtruth_data is not None:\n",
    "    print(\"\\nüîç Interactive Data Explorer:\")\n",
    "    data_explorer = create_interactive_data_explorer()\n",
    "    if data_explorer:\n",
    "        display(data_explorer)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Load data first to use the interactive explorer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary and Export\n",
    "\n",
    "Generate summary reports and export data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_summary_report():\n",
    "    \"\"\"Generate a comprehensive data summary report.\"\"\"\n",
    "    \n",
    "    if posts_df is None or groundtruth_data is None:\n",
    "        print(\"‚ùå No data loaded. Please load data first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìã Data Summary Report\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find text column\n",
    "    text_column = None\n",
    "    for col in ['text', 'content', 'post_text', 'message']:\n",
    "        if col in posts_df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        print(\"‚ùå No text column found\")\n",
    "        return\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"   Total posts: {len(posts_df):,}\")\n",
    "    print(f\"   Posts with groundtruth: {len(groundtruth_data):,}\")\n",
    "    print(f\"   Coverage: {len(groundtruth_data)/len(posts_df)*100:.1f}%\")\n",
    "    \n",
    "    # Text statistics\n",
    "    texts = posts_df[text_column].dropna()\n",
    "    char_counts = texts.str.len()\n",
    "    word_counts = texts.str.split().str.len()\n",
    "    \n",
    "    print(f\"\\nüìù Text Statistics:\")\n",
    "    print(f\"   Character count - Mean: {char_counts.mean():.1f}, Median: {char_counts.median():.1f}\")\n",
    "    print(f\"   Word count - Mean: {word_counts.mean():.1f}, Median: {word_counts.median():.1f}\")\n",
    "    \n",
    "    # Label statistics\n",
    "    all_labels = []\n",
    "    for entry in groundtruth_data.values():\n",
    "        if 'labels' in entry:\n",
    "            all_labels.extend(entry['labels'])\n",
    "    \n",
    "    label_counts = Counter(all_labels)\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  Label Statistics:\")\n",
    "    print(f\"   Unique labels: {len(label_counts)}\")\n",
    "    print(f\"   Total label instances: {len(all_labels):,}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data summary report complete!\")\n",
    "    \n",
    "    return {\n",
    "        'total_posts': len(posts_df),\n",
    "        'posts_with_groundtruth': len(groundtruth_data),\n",
    "        'unique_labels': len(label_counts),\n",
    "        'label_counts': label_counts\n",
    "    }\n",
    "\n",
    "# Generate summary report\n",
    "if posts_df is not None and groundtruth_data is not None:\n",
    "    summary_report = generate_data_summary_report()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Load data first to generate summary report\")\n",
    "\n",
    "print(\"\\n‚úÖ Data Processing and Exploration notebook complete!\")\n",
    "print(\"\\nThis notebook provides:\")\n",
    "print(\"‚Ä¢ Interactive data configuration and loading\")\n",
    "print(\"‚Ä¢ Comprehensive text and label analysis\")\n",
    "print(\"‚Ä¢ Data quality assessment and recommendations\")\n",
    "print(\"‚Ä¢ Interactive data exploration tools\")\n",
    "print(\"‚Ä¢ Data preprocessing pipeline testing\")"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": "3"
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.8.0"
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 }
}