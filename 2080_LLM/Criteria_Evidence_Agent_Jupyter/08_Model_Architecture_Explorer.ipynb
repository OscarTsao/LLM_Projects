{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture Explorer\n",
    "\n",
    "This notebook provides an interactive exploration of model architectures, encoders, and classification heads.\n",
    "It allows you to visualize model structures, analyze parameters, and experiment with different configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import all necessary libraries for model exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies\n",
    "\n",
    "Load configuration and model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration management\n",
    "%run 01_Configuration_Management.ipynb\n",
    "\n",
    "# Import project modules\n",
    "from src.models.model import EvidenceModel\n",
    "from src.models.encoders import (\n",
    "    RobertaEncoder, BertEncoder, DebertaEncoder,\n",
    "    get_encoder_class\n",
    ")\n",
    "from src.models.heads import (\n",
    "    MultiLabelClassificationHead, RegressionHead,\n",
    "    get_head_class\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Explorer\n",
    "\n",
    "Interactive exploration of different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_architecture_explorer():\n",
    "    \"\"\"Create an interactive model architecture explorer.\"\"\"\n",
    "    \n",
    "    # Model type selection\n",
    "    encoder_type = widgets.Dropdown(\n",
    "        options=['roberta', 'bert', 'deberta'],\n",
    "        value='roberta',\n",
    "        description='Encoder Type:'\n",
    "    )\n",
    "    \n",
    "    # Model size selection\n",
    "    model_size = widgets.Dropdown(\n",
    "        options=['base', 'large'],\n",
    "        value='base',\n",
    "        description='Model Size:'\n",
    "    )\n",
    "    \n",
    "    # LoRA configuration\n",
    "    use_lora = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Use LoRA'\n",
    "    )\n",
    "    \n",
    "    lora_r = widgets.IntSlider(\n",
    "        value=16,\n",
    "        min=4,\n",
    "        max=64,\n",
    "        step=4,\n",
    "        description='LoRA r:',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    lora_alpha = widgets.IntSlider(\n",
    "        value=32,\n",
    "        min=8,\n",
    "        max=128,\n",
    "        step=8,\n",
    "        description='LoRA Œ±:',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Head configuration\n",
    "    num_labels = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=2,\n",
    "        max=50,\n",
    "        description='Num Labels:'\n",
    "    )\n",
    "    \n",
    "    hidden_size = widgets.Dropdown(\n",
    "        options=[256, 512, 768, 1024],\n",
    "        value=512,\n",
    "        description='Hidden Size:'\n",
    "    )\n",
    "    \n",
    "    dropout = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.0,\n",
    "        max=0.5,\n",
    "        step=0.05,\n",
    "        description='Dropout:'\n",
    "    )\n",
    "    \n",
    "    # Buttons\n",
    "    create_button = widgets.Button(\n",
    "        description='üèóÔ∏è Create Model',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    \n",
    "    analyze_button = widgets.Button(\n",
    "        description='üìä Analyze Architecture',\n",
    "        button_style='info',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_lora_change(change):\n",
    "        lora_r.disabled = not change['new']\n",
    "        lora_alpha.disabled = not change['new']\n",
    "    \n",
    "    use_lora.observe(on_lora_change, names='value')\n",
    "    \n",
    "    def on_create_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                print(f\"üèóÔ∏è Creating Model Architecture\")\n",
    "                print(\"=\" * 40)\n",
    "                \n",
    "                # Create configuration\n",
    "                config = ExperimentConfig()\n",
    "                \n",
    "                # Update encoder configuration\n",
    "                config.model.encoder.type = encoder_type.value\n",
    "                \n",
    "                # Set model name based on type and size\n",
    "                model_mapping = {\n",
    "                    ('roberta', 'base'): 'roberta-base',\n",
    "                    ('roberta', 'large'): 'roberta-large',\n",
    "                    ('bert', 'base'): 'bert-base-uncased',\n",
    "                    ('bert', 'large'): 'bert-large-uncased',\n",
    "                    ('deberta', 'base'): 'microsoft/deberta-base',\n",
    "                    ('deberta', 'large'): 'microsoft/deberta-large'\n",
    "                }\n",
    "                \n",
    "                config.model.encoder.pretrained_model_name_or_path = model_mapping[\n",
    "                    (encoder_type.value, model_size.value)\n",
    "                ]\n",
    "                \n",
    "                # LoRA configuration\n",
    "                config.model.encoder.lora.enabled = use_lora.value\n",
    "                if use_lora.value:\n",
    "                    config.model.encoder.lora.r = lora_r.value\n",
    "                    config.model.encoder.lora.alpha = lora_alpha.value\n",
    "                \n",
    "                # Head configuration\n",
    "                config.model.heads.symptom_labels.layers.hidden_size = hidden_size.value\n",
    "                config.model.heads.symptom_labels.layers.dropout = dropout.value\n",
    "                \n",
    "                # Update number of labels\n",
    "                config.data.multi_label_fields = [f\"label_{i}\" for i in range(num_labels.value)]\n",
    "                \n",
    "                print(f\"   Encoder: {encoder_type.value}-{model_size.value}\")\n",
    "                print(f\"   Model: {config.model.encoder.pretrained_model_name_or_path}\")\n",
    "                print(f\"   LoRA: {'Enabled' if use_lora.value else 'Disabled'}\")\n",
    "                if use_lora.value:\n",
    "                    print(f\"     r={lora_r.value}, Œ±={lora_alpha.value}\")\n",
    "                print(f\"   Labels: {num_labels.value}\")\n",
    "                print(f\"   Hidden size: {hidden_size.value}\")\n",
    "                print(f\"   Dropout: {dropout.value}\")\n",
    "                \n",
    "                # Create model\n",
    "                print(f\"\\nü§ñ Instantiating model...\")\n",
    "                model = EvidenceModel(config.model)\n",
    "                \n",
    "                # Count parameters\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                \n",
    "                print(f\"   ‚úÖ Model created successfully!\")\n",
    "                print(f\"   Total parameters: {total_params:,}\")\n",
    "                print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "                print(f\"   Frozen parameters: {total_params - trainable_params:,}\")\n",
    "                \n",
    "                # Store model globally for analysis\n",
    "                global current_model, current_config\n",
    "                current_model = model\n",
    "                current_config = config\n",
    "                \n",
    "                # Enable analysis button\n",
    "                analyze_button.disabled = False\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error creating model: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def on_analyze_clicked(b):\n",
    "        if 'current_model' in globals():\n",
    "            analyze_model_architecture(current_model, current_config)\n",
    "        else:\n",
    "            print(\"‚ùå No model created. Please create a model first.\")\n",
    "    \n",
    "    create_button.on_click(on_create_clicked)\n",
    "    analyze_button.on_click(on_analyze_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Model Architecture Explorer</h3>\"),\n",
    "        widgets.HBox([encoder_type, model_size]),\n",
    "        widgets.HBox([use_lora, lora_r, lora_alpha]),\n",
    "        widgets.HBox([num_labels, hidden_size, dropout]),\n",
    "        widgets.HBox([create_button, analyze_button])\n",
    "    ])\n",
    "    \n",
    "    return widgets.VBox([controls, output])\n",
    "\n",
    "# Display architecture explorer\n",
    "architecture_explorer = create_architecture_explorer()\n",
    "display(architecture_explorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis Functions\n",
    "\n",
    "Detailed analysis of model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_architecture(model: nn.Module, config: ExperimentConfig):\n",
    "    \"\"\"Analyze model architecture in detail.\"\"\"\n",
    "    \n",
    "    print(f\"üìä Model Architecture Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Model summary\n",
    "    print(f\"\\nüèóÔ∏è Model Structure:\")\n",
    "    print(f\"   Model class: {model.__class__.__name__}\")\n",
    "    print(f\"   Encoder type: {config.model.encoder.type}\")\n",
    "    print(f\"   Pretrained model: {config.model.encoder.pretrained_model_name_or_path}\")\n",
    "    \n",
    "    # Parameter analysis\n",
    "    print(f\"\\nüìà Parameter Analysis:\")\n",
    "    \n",
    "    param_info = []\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Leaf modules only\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            module_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "            \n",
    "            if module_params > 0:\n",
    "                param_info.append({\n",
    "                    'Module': name,\n",
    "                    'Type': module.__class__.__name__,\n",
    "                    'Total Params': module_params,\n",
    "                    'Trainable Params': module_trainable,\n",
    "                    'Frozen Params': module_params - module_trainable,\n",
    "                    'Trainable %': (module_trainable / module_params * 100) if module_params > 0 else 0\n",
    "                })\n",
    "                \n",
    "                total_params += module_params\n",
    "                trainable_params += module_trainable\n",
    "    \n",
    "    # Create parameter DataFrame\n",
    "    param_df = pd.DataFrame(param_info)\n",
    "    param_df = param_df.sort_values('Total Params', ascending=False)\n",
    "    \n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
    "    print(f\"   Frozen parameters: {total_params - trainable_params:,} ({(total_params - trainable_params)/total_params*100:.1f}%)\")\n",
    "    \n",
    "    # Display top parameter-heavy modules\n",
    "    print(f\"\\nüîù Top Parameter-Heavy Modules:\")\n",
    "    top_modules = param_df.head(10)\n",
    "    for _, row in top_modules.iterrows():\n",
    "        print(f\"   {row['Module']}: {row['Total Params']:,} params ({row['Trainable %']:.1f}% trainable)\")\n",
    "    \n",
    "    # Visualize parameter distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Parameter Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Parameter distribution by module type\n",
    "    module_type_params = param_df.groupby('Type')['Total Params'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    axes[0, 0].pie(module_type_params.values, labels=module_type_params.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Parameters by Module Type')\n",
    "    \n",
    "    # 2. Trainable vs Frozen parameters\n",
    "    trainable_frozen = [trainable_params, total_params - trainable_params]\n",
    "    labels = ['Trainable', 'Frozen']\n",
    "    colors = ['lightgreen', 'lightcoral']\n",
    "    \n",
    "    axes[0, 1].pie(trainable_frozen, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Trainable vs Frozen Parameters')\n",
    "    \n",
    "    # 3. Top modules by parameter count\n",
    "    top_10 = param_df.head(10)\n",
    "    y_pos = np.arange(len(top_10))\n",
    "    \n",
    "    axes[1, 0].barh(y_pos, top_10['Total Params'], color='skyblue')\n",
    "    axes[1, 0].set_yticks(y_pos)\n",
    "    axes[1, 0].set_yticklabels([name.split('.')[-1] for name in top_10['Module']])\n",
    "    axes[1, 0].set_xlabel('Parameters')\n",
    "    axes[1, 0].set_title('Top 10 Modules by Parameter Count')\n",
    "    \n",
    "    # 4. Trainable percentage by module\n",
    "    trainable_pct = param_df[param_df['Total Params'] > 1000]['Trainable %'].head(15)\n",
    "    module_names = [name.split('.')[-1] for name in param_df[param_df['Total Params'] > 1000]['Module'].head(15)]\n",
    "    \n",
    "    axes[1, 1].bar(range(len(trainable_pct)), trainable_pct, color='lightgreen')\n",
    "    axes[1, 1].set_xticks(range(len(trainable_pct)))\n",
    "    axes[1, 1].set_xticklabels(module_names, rotation=45, ha='right')\n",
    "    axes[1, 1].set_ylabel('Trainable %')\n",
    "    axes[1, 1].set_title('Trainable Percentage by Module')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Memory estimation\n",
    "    print(f\"\\nüíæ Memory Estimation:\")\n",
    "    \n",
    "    # Estimate memory usage (rough approximation)\n",
    "    param_memory_mb = total_params * 4 / (1024 * 1024)  # 4 bytes per float32 parameter\n",
    "    gradient_memory_mb = trainable_params * 4 / (1024 * 1024)  # Gradients for trainable params\n",
    "    optimizer_memory_mb = trainable_params * 8 / (1024 * 1024)  # Adam optimizer states (rough estimate)\n",
    "    \n",
    "    total_memory_mb = param_memory_mb + gradient_memory_mb + optimizer_memory_mb\n",
    "    \n",
    "    print(f\"   Model parameters: {param_memory_mb:.1f} MB\")\n",
    "    print(f\"   Gradients: {gradient_memory_mb:.1f} MB\")\n",
    "    print(f\"   Optimizer states: {optimizer_memory_mb:.1f} MB\")\n",
    "    print(f\"   Total (approx): {total_memory_mb:.1f} MB ({total_memory_mb/1024:.2f} GB)\")\n",
    "    \n",
    "    # LoRA analysis if enabled\n",
    "    if config.model.encoder.lora.enabled:\n",
    "        print(f\"\\nüîß LoRA Analysis:\")\n",
    "        print(f\"   LoRA rank (r): {config.model.encoder.lora.r}\")\n",
    "        print(f\"   LoRA alpha: {config.model.encoder.lora.alpha}\")\n",
    "        print(f\"   LoRA dropout: {config.model.encoder.lora.dropout}\")\n",
    "        \n",
    "        # Count LoRA parameters\n",
    "        lora_params = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name.lower())\n",
    "        if lora_params > 0:\n",
    "            print(f\"   LoRA parameters: {lora_params:,} ({lora_params/total_params*100:.2f}% of total)\")\n",
    "            reduction_ratio = total_params / lora_params if lora_params > 0 else 0\n",
    "            print(f\"   Parameter reduction: {reduction_ratio:.1f}x\")\n",
    "    \n",
    "    return param_df\n",
    "\n",
    "def compare_model_architectures():\n",
    "    \"\"\"Compare different model architectures.\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Model Architecture Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define architectures to compare\n",
    "    architectures = [\n",
    "        {'name': 'RoBERTa-base', 'type': 'roberta', 'model': 'roberta-base'},\n",
    "        {'name': 'BERT-base', 'type': 'bert', 'model': 'bert-base-uncased'},\n",
    "        {'name': 'DeBERTa-base', 'type': 'deberta', 'model': 'microsoft/deberta-base'},\n",
    "    ]\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for arch in architectures:\n",
    "        try:\n",
    "            print(f\"\\nüìä Analyzing {arch['name']}...\")\n",
    "            \n",
    "            # Create configuration\n",
    "            config = ExperimentConfig()\n",
    "            config.model.encoder.type = arch['type']\n",
    "            config.model.encoder.pretrained_model_name_or_path = arch['model']\n",
    "            \n",
    "            # Create model\n",
    "            model = EvidenceModel(config.model)\n",
    "            \n",
    "            # Count parameters\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            \n",
    "            # Get model config for additional info\n",
    "            try:\n",
    "                model_config = AutoConfig.from_pretrained(arch['model'])\n",
    "                hidden_size = getattr(model_config, 'hidden_size', 'N/A')\n",
    "                num_layers = getattr(model_config, 'num_hidden_layers', 'N/A')\n",
    "                num_heads = getattr(model_config, 'num_attention_heads', 'N/A')\n",
    "            except:\n",
    "                hidden_size = num_layers = num_heads = 'N/A'\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Architecture': arch['name'],\n",
    "                'Total Parameters': total_params,\n",
    "                'Trainable Parameters': trainable_params,\n",
    "                'Hidden Size': hidden_size,\n",
    "                'Layers': num_layers,\n",
    "                'Attention Heads': num_heads,\n",
    "                'Memory (MB)': total_params * 4 / (1024 * 1024)\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚úÖ {arch['name']}: {total_params:,} parameters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error with {arch['name']}: {e}\")\n",
    "    \n",
    "    if comparison_data:\n",
    "        # Create comparison DataFrame\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        print(f\"\\nüìã Architecture Comparison:\")\n",
    "        display(comparison_df)\n",
    "        \n",
    "        # Visualize comparison\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Parameter comparison\n",
    "        architectures = comparison_df['Architecture']\n",
    "        total_params = comparison_df['Total Parameters']\n",
    "        \n",
    "        axes[0].bar(architectures, total_params, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "        axes[0].set_ylabel('Total Parameters')\n",
    "        axes[0].set_title('Parameter Count Comparison')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Memory comparison\n",
    "        memory_usage = comparison_df['Memory (MB)']\n",
    "        \n",
    "        axes[1].bar(architectures, memory_usage, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "        axes[1].set_ylabel('Memory Usage (MB)')\n",
    "        axes[1].set_title('Memory Usage Comparison')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Create comparison button\n",
    "compare_button = widgets.Button(\n",
    "    description='üîÑ Compare Architectures',\n",
    "    button_style='warning'\n",
    ")\n",
    "\n",
    "def on_compare_clicked(b):\n",
    "    compare_model_architectures()\n",
    "\n",
    "compare_button.on_click(on_compare_clicked)\n",
    "\n",
    "print(\"\\nüîÑ Architecture Comparison:\")\n",
    "display(compare_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Deep Dive\n",
    "\n",
    "Detailed exploration of different encoder types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_encoder_details():\n",
    "    \"\"\"Explore encoder architectures in detail.\"\"\"\n",
    "    \n",
    "    print(f\"üîç Encoder Architecture Deep Dive\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    encoder_configs = {\n",
    "        'RoBERTa': {\n",
    "            'class': RobertaEncoder,\n",
    "            'model': 'roberta-base',\n",
    "            'description': 'Robustly Optimized BERT Pretraining Approach'\n",
    "        },\n",
    "        'BERT': {\n",
    "            'class': BertEncoder,\n",
    "            'model': 'bert-base-uncased',\n",
    "            'description': 'Bidirectional Encoder Representations from Transformers'\n",
    "        },\n",
    "        'DeBERTa': {\n",
    "            'class': DebertaEncoder,\n",
    "            'model': 'microsoft/deberta-base',\n",
    "            'description': 'Decoding-enhanced BERT with Disentangled Attention'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, info in encoder_configs.items():\n",
    "        print(f\"\\nü§ñ {name} Encoder:\")\n",
    "        print(f\"   Description: {info['description']}\")\n",
    "        print(f\"   Model: {info['model']}\")\n",
    "        \n",
    "        try:\n",
    "            # Load model config\n",
    "            config = AutoConfig.from_pretrained(info['model'])\n",
    "            \n",
    "            print(f\"   Architecture Details:\")\n",
    "            print(f\"     Hidden size: {getattr(config, 'hidden_size', 'N/A')}\")\n",
    "            print(f\"     Layers: {getattr(config, 'num_hidden_layers', 'N/A')}\")\n",
    "            print(f\"     Attention heads: {getattr(config, 'num_attention_heads', 'N/A')}\")\n",
    "            print(f\"     Intermediate size: {getattr(config, 'intermediate_size', 'N/A')}\")\n",
    "            print(f\"     Max position embeddings: {getattr(config, 'max_position_embeddings', 'N/A')}\")\n",
    "            print(f\"     Vocab size: {getattr(config, 'vocab_size', 'N/A')}\")\n",
    "            \n",
    "            # Special features\n",
    "            if name == 'DeBERTa':\n",
    "                print(f\"     Relative attention: {getattr(config, 'relative_attention', 'N/A')}\")\n",
    "                print(f\"     Position bucket size: {getattr(config, 'position_bucket_size', 'N/A')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Could not load config: {e}\")\n",
    "\n",
    "def explore_head_architectures():\n",
    "    \"\"\"Explore classification head architectures.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüéØ Classification Head Architectures\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Multi-label classification head\n",
    "    print(f\"\\nüè∑Ô∏è  Multi-Label Classification Head:\")\n",
    "    print(f\"   Purpose: Multi-label classification with sigmoid activation\")\n",
    "    print(f\"   Architecture:\")\n",
    "    print(f\"     Input: [batch_size, hidden_size]\")\n",
    "    print(f\"     Hidden layer: Linear(hidden_size, head_hidden_size)\")\n",
    "    print(f\"     Activation: ReLU or GELU\")\n",
    "    print(f\"     Dropout: Configurable dropout rate\")\n",
    "    print(f\"     Output: Linear(head_hidden_size, num_labels)\")\n",
    "    print(f\"     Final activation: Sigmoid (for multi-label)\")\n",
    "    \n",
    "    # Regression head\n",
    "    print(f\"\\nüìä Regression Head:\")\n",
    "    print(f\"   Purpose: Continuous value prediction\")\n",
    "    print(f\"   Architecture:\")\n",
    "    print(f\"     Input: [batch_size, hidden_size]\")\n",
    "    print(f\"     Hidden layer: Linear(hidden_size, head_hidden_size)\")\n",
    "    print(f\"     Activation: ReLU or GELU\")\n",
    "    print(f\"     Dropout: Configurable dropout rate\")\n",
    "    print(f\"     Output: Linear(head_hidden_size, 1)\")\n",
    "    print(f\"     Final activation: None (linear output)\")\n",
    "    \n",
    "    # Pooling strategies\n",
    "    print(f\"\\nüèä Pooling Strategies:\")\n",
    "    pooling_strategies = {\n",
    "        'cls': 'Use [CLS] token representation',\n",
    "        'mean': 'Average pooling over all tokens',\n",
    "        'max': 'Max pooling over all tokens',\n",
    "        'attention': 'Attention-weighted pooling'\n",
    "    }\n",
    "    \n",
    "    for strategy, description in pooling_strategies.items():\n",
    "        print(f\"   {strategy.upper()}: {description}\")\n",
    "\n",
    "# Create exploration buttons\n",
    "encoder_button = widgets.Button(\n",
    "    description='üîç Explore Encoders',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "head_button = widgets.Button(\n",
    "    description='üéØ Explore Heads',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "def on_encoder_clicked(b):\n",
    "    explore_encoder_details()\n",
    "\n",
    "def on_head_clicked(b):\n",
    "    explore_head_architectures()\n",
    "\n",
    "encoder_button.on_click(on_encoder_clicked)\n",
    "head_button.on_click(on_head_clicked)\n",
    "\n",
    "print(\"\\nüîç Architecture Deep Dive:\")\n",
    "display(widgets.HBox([encoder_button, head_button]))\n",
    "\n",
    "print(\"\\n‚úÖ Model Architecture Explorer complete!\")\n",
    "print(\"\\nThis notebook provides:\")\n",
    "print(\"‚Ä¢ Interactive model architecture exploration\")\n",
    "print(\"‚Ä¢ Detailed parameter analysis and visualization\")\n",
    "print(\"‚Ä¢ Model architecture comparison\")\n",
    "print(\"‚Ä¢ Encoder and head architecture deep dive\")\n",
    "print(\"‚Ä¢ Memory usage estimation\")\n",
    "print(\"‚Ä¢ LoRA configuration analysis\")"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": "3"
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.8.0"
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 }
}