{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive training system with auto-resume capabilities, enhanced checkpointing,\n",
    "and real-time monitoring. It replaces the original train.py script with an interactive notebook interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import all necessary libraries and initialize the training environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from dataclasses import asdict\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for imports\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "# Import project modules\n",
    "from src.data.dataset import DataModule\n",
    "from src.models.model import EvidenceModel\n",
    "from src.utils import (\n",
    "    evaluate, flatten_dict, get_optimizer, get_scheduler, \n",
    "    prepare_thresholds, set_seed\n",
    ")\n",
    "from src.utils.ema import EMA\n",
    "from src.utils.training import compute_loss\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration and Checkpoint Managers\n",
    "\n",
    "Load configuration from the configuration notebook and initialize checkpoint managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the configuration and checkpoint notebooks to get managers\n",
    "%run 01_Configuration_Management.ipynb\n",
    "%run 02_Enhanced_Checkpoint_System.ipynb\n",
    "\n",
    "print(\"‚úÖ Configuration and checkpoint managers loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration Selection\n",
    "\n",
    "Select or create a configuration for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive configuration selection\n",
    "def create_config_selector():\n",
    "    \"\"\"Create an interactive configuration selector.\"\"\"\n",
    "    \n",
    "    available_configs = config_manager.list_configs()\n",
    "    \n",
    "    if not available_configs:\n",
    "        print(\"No configurations found. Please run the Configuration Management notebook first.\")\n",
    "        return None\n",
    "    \n",
    "    config_dropdown = widgets.Dropdown(\n",
    "        options=available_configs,\n",
    "        value=available_configs[0] if available_configs else None,\n",
    "        description='Configuration:'\n",
    "    )\n",
    "    \n",
    "    experiment_name = widgets.Text(\n",
    "        value='training_experiment',\n",
    "        description='Experiment:'\n",
    "    )\n",
    "    \n",
    "    auto_resume = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Auto Resume'\n",
    "    )\n",
    "    \n",
    "    load_button = widgets.Button(\n",
    "        description='Load Configuration',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_load_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                # Load configuration\n",
    "                config = config_manager.load_config(config_dropdown.value)\n",
    "                \n",
    "                # Update experiment name\n",
    "                config.mlflow.experiment_name = experiment_name.value\n",
    "                config.training.auto_resume = auto_resume.value\n",
    "                \n",
    "                # Store in global variable for use in training\n",
    "                global current_config, current_experiment_name\n",
    "                current_config = config\n",
    "                current_experiment_name = experiment_name.value\n",
    "                \n",
    "                print(f\"‚úÖ Configuration loaded: {config_dropdown.value}\")\n",
    "                print(f\"   Experiment: {experiment_name.value}\")\n",
    "                print(f\"   Model: {config.model.encoder.type}\")\n",
    "                print(f\"   Auto-resume: {auto_resume.value}\")\n",
    "                \n",
    "                # Display configuration summary\n",
    "                summary_df = config_manager.get_config_summary(config)\n",
    "                display(summary_df.head(15))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading configuration: {e}\")\n",
    "    \n",
    "    load_button.on_click(on_load_clicked)\n",
    "    \n",
    "    layout = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Training Configuration</h3>\"),\n",
    "        config_dropdown,\n",
    "        experiment_name,\n",
    "        auto_resume,\n",
    "        load_button,\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return layout\n",
    "\n",
    "# Display configuration selector\n",
    "config_selector = create_config_selector()\n",
    "if config_selector:\n",
    "    display(config_selector)\n",
    "else:\n",
    "    # Fallback: create default configuration\n",
    "    current_config = ExperimentConfig()\n",
    "    current_experiment_name = \"default_training\"\n",
    "    print(\"Using default configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Training Loop\n",
    "\n",
    "The main training loop with auto-resume, enhanced checkpointing, and real-time monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_train_loop(config: ExperimentConfig, experiment_name: str) -> Dict[str, float]:\n",
    "    \"\"\"Enhanced training loop with auto-resume and comprehensive checkpointing.\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Starting training: {experiment_name}\")\n",
    "    print(f\"   Model: {config.model.encoder.type}\")\n",
    "    print(f\"   Max epochs: {config.training.max_epochs}\")\n",
    "    print(f\"   Batch size: {config.training.batch_size}\")\n",
    "    print(f\"   Learning rate: {config.training.optimizer.learning_rate}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(config.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    \n",
    "    # Setup MLflow\n",
    "    mlflow.set_tracking_uri(config.mlflow.tracking_uri)\n",
    "    mlflow.set_experiment(config.mlflow.experiment_name)\n",
    "    \n",
    "    # Setup data\n",
    "    print(\"\\nüìä Setting up data...\")\n",
    "    data_module = DataModule(config.data, config.model)\n",
    "    train_loader, val_loader, test_loader = data_module.dataloaders(\n",
    "        batch_size=config.training.batch_size,\n",
    "        val_batch_size=config.training.val_batch_size,\n",
    "        test_batch_size=config.training.test_batch_size,\n",
    "        num_workers=config.training.num_workers,\n",
    "    )\n",
    "    print(f\"   Train samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"   Val samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"   Test samples: {len(test_loader.dataset)}\")\n",
    "    \n",
    "    # Setup model\n",
    "    print(\"\\nü§ñ Setting up model...\")\n",
    "    model = EvidenceModel(config.model)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    head_cfgs = model.head_configs\n",
    "    head_thresholds = {\n",
    "        head_name: prepare_thresholds(head_cfg)\n",
    "        for head_name, head_cfg in head_cfgs.items()\n",
    "        if head_cfg.get(\"type\") == \"multi_label\"\n",
    "    }\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    print(\"\\n‚öôÔ∏è Setting up optimizer and scheduler...\")\n",
    "    optimizer = get_optimizer(model, config.training.optimizer)\n",
    "    \n",
    "    updates_per_epoch = math.ceil(\n",
    "        len(train_loader) / config.training.gradient_accumulation_steps\n",
    "    )\n",
    "    total_steps = updates_per_epoch * config.training.max_epochs\n",
    "    scheduler = get_scheduler(optimizer, config.training.scheduler, total_steps)\n",
    "    \n",
    "    print(f\"   Optimizer: {config.training.optimizer.name}\")\n",
    "    print(f\"   Scheduler: {config.training.scheduler.name}\")\n",
    "    print(f\"   Total steps: {total_steps:,}\")\n",
    "    \n",
    "    # Setup EMA if enabled\n",
    "    ema = None\n",
    "    ema_decay = config.training.get(\"ema_decay\", 0.0)\n",
    "    if ema_decay > 0:\n",
    "        ema = EMA(model, decay=ema_decay)\n",
    "        print(f\"   EMA enabled with decay: {ema_decay}\")\n",
    "    \n",
    "    # Setup mixed precision training\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    amp_enabled = config.training.amp and device.type == \"cuda\"\n",
    "    is_bf16_supported = bool(\n",
    "        getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
    "    )\n",
    "    use_bf16 = amp_enabled and config.training.bf16 and is_bf16_supported\n",
    "    amp_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "    scaler = GradScaler(enabled=amp_enabled and not use_bf16)\n",
    "    \n",
    "    if amp_enabled:\n",
    "        print(f\"   Mixed precision: {amp_dtype}\")\n",
    "    \n",
    "    # Check for auto-resume\n",
    "    training_state = None\n",
    "    start_epoch = 1\n",
    "    \n",
    "    if config.training.auto_resume:\n",
    "        print(\"\\nüîÑ Checking for resumable checkpoint...\")\n",
    "        config_dict = asdict(config)\n",
    "        should_resume, checkpoint_id = auto_resume_manager.should_resume_training(\n",
    "            experiment_name, config_dict\n",
    "        )\n",
    "        \n",
    "        if should_resume and checkpoint_id:\n",
    "            training_state, _ = auto_resume_manager.resume_training(\n",
    "                checkpoint_id, model, optimizer, scheduler, device\n",
    "            )\n",
    "            start_epoch = training_state.epoch + 1\n",
    "    \n",
    "    # Create new training state if not resuming\n",
    "    if training_state is None:\n",
    "        print(\"\\nüÜï Starting fresh training...\")\n",
    "        training_state = auto_resume_manager.create_training_state()\n",
    "    \n",
    "    # Training configuration\n",
    "    loss_weights = config.training.loss_weights\n",
    "    focal_cfg = config.training.focal\n",
    "    best_metric = training_state.best_metric\n",
    "    best_epoch = training_state.best_epoch\n",
    "    epochs_without_improve = training_state.epochs_without_improve\n",
    "    \n",
    "    # Start MLflow run\n",
    "    config_dict = asdict(config)\n",
    "    flattened = flatten_dict(config_dict)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{experiment_name}_{config.model.encoder.type}\"):\n",
    "        mlflow.log_params(flattened)\n",
    "        if config.mlflow.autolog:\n",
    "            mlflow.pytorch.autolog(log_models=False)\n",
    "        \n",
    "        print(f\"\\nüéØ Training from epoch {start_epoch} to {config.training.max_epochs}\")\n",
    "        \n",
    "        # Training loop with progress tracking\n",
    "        epoch_progress = tqdm(\n",
    "            range(start_epoch, config.training.max_epochs + 1),\n",
    "            desc=\"Training\",\n",
    "            position=0,\n",
    "            leave=True\n",
    "        )\n",
    "        \n",
    "        for epoch in epoch_progress:\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            progress = tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False, position=1)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            for step, batch in enumerate(progress, start=1):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                token_type_ids = batch.get(\"token_type_ids\")\n",
    "                if token_type_ids is not None:\n",
    "                    token_type_ids = token_type_ids.to(device)\n",
    "                \n",
    "                with autocast(\n",
    "                    enabled=amp_enabled,\n",
    "                    dtype=amp_dtype,\n",
    "                ):\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                    )\n",
    "                    head_outputs = outputs[\"head_outputs\"]\n",
    "                    loss = compute_loss(\n",
    "                        head_outputs, batch, head_cfgs, loss_weights, focal_cfg, device\n",
    "                    )\n",
    "                    loss = loss / config.training.gradient_accumulation_steps\n",
    "                \n",
    "                if scaler.is_enabled():\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                \n",
    "                if step % config.training.gradient_accumulation_steps == 0:\n",
    "                    if scaler.is_enabled():\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            model.parameters(), config.training.max_grad_norm\n",
    "                        )\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            model.parameters(), config.training.max_grad_norm\n",
    "                        )\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # Step scheduler (except ReduceLROnPlateau)\n",
    "                    if not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        scheduler.step()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    # Update EMA\n",
    "                    if ema is not None:\n",
    "                        ema.update()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                avg_loss = running_loss / step\n",
    "                \n",
    "                # Get learning rate\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                else:\n",
    "                    current_lr = scheduler.get_last_lr()[0]\n",
    "                \n",
    "                progress.set_postfix({\n",
    "                    \"loss\": f\"{avg_loss:.4f}\", \n",
    "                    \"lr\": f\"{current_lr:.2e}\"\n",
    "                })\n",
    "                \n",
    "                if step % config.training.logging_interval == 0:\n",
    "                    mlflow.log_metric(\n",
    "                        \"train_loss\",\n",
    "                        avg_loss,\n",
    "                        step=(epoch - 1) * len(train_loader) + step,\n",
    "                    )\n",
    "            \n",
    "            # Validation\n",
    "            if ema is not None:\n",
    "                ema.apply_shadow()\n",
    "            \n",
    "            val_loss, val_metrics = evaluate(\n",
    "                model, val_loader, device, config, head_thresholds\n",
    "            )\n",
    "            \n",
    "            if ema is not None:\n",
    "                ema.restore()\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            for name, value in val_metrics.items():\n",
    "                if name != \"val_loss\":\n",
    "                    mlflow.log_metric(name, value, step=epoch)\n",
    "            \n",
    "            # Update training state\n",
    "            training_state.epoch = epoch\n",
    "            training_state.step = (epoch - 1) * len(train_loader) + len(train_loader)\n",
    "            \n",
    "            # Add to training history\n",
    "            from src.utils.training import TrainingMetrics\n",
    "            epoch_metrics = TrainingMetrics(\n",
    "                epoch=epoch,\n",
    "                train_loss=avg_loss,\n",
    "                val_loss=val_loss,\n",
    "                val_metrics=val_metrics,\n",
    "                learning_rate=current_lr,\n",
    "                timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "            training_state.training_history.append(epoch_metrics)\n",
    "            \n",
    "            # Early stopping and checkpointing\n",
    "            monitor_metric = val_metrics.get(\n",
    "                config.training.early_stopping.monitor, float(\"-inf\")\n",
    "            )\n",
    "            \n",
    "            # Step ReduceLROnPlateau scheduler if used\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(monitor_metric)\n",
    "            \n",
    "            improved = monitor_metric > best_metric + config.training.early_stopping.min_delta\n",
    "            \n",
    "            if improved:\n",
    "                best_metric = monitor_metric\n",
    "                best_epoch = epoch\n",
    "                epochs_without_improve = 0\n",
    "                training_state.best_metric = best_metric\n",
    "                training_state.best_epoch = best_epoch\n",
    "                training_state.epochs_without_improve = epochs_without_improve\n",
    "                \n",
    "                # Save best checkpoint\n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    training_state=training_state,\n",
    "                    config=config_dict,\n",
    "                    experiment_name=experiment_name,\n",
    "                    notes=f\"Best model at epoch {epoch}\",\n",
    "                    is_best=True\n",
    "                )\n",
    "                status = \"‚úì Improved\"\n",
    "            else:\n",
    "                epochs_without_improve += 1\n",
    "                training_state.epochs_without_improve = epochs_without_improve\n",
    "                status = f\"No improvement ({epochs_without_improve}/{config.training.early_stopping.patience})\"\n",
    "            \n",
    "            # Save periodic checkpoint\n",
    "            if epoch % config.training.save_every_n_epochs == 0:\n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    training_state=training_state,\n",
    "                    config=config_dict,\n",
    "                    experiment_name=experiment_name,\n",
    "                    notes=f\"Periodic checkpoint at epoch {epoch}\",\n",
    "                    is_best=False\n",
    "                )\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_progress.set_postfix({\n",
    "                \"val_loss\": f\"{val_loss:.4f}\",\n",
    "                config.training.early_stopping.monitor: f\"{monitor_metric:.4f}\",\n",
    "                \"best\": f\"{best_metric:.4f}\",\n",
    "                \"status\": status\n",
    "            })\n",
    "            \n",
    "            # Early stopping check\n",
    "            if epochs_without_improve >= config.training.early_stopping.patience:\n",
    "                tqdm.write(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
    "                tqdm.write(f\"Best {config.training.early_stopping.monitor}: {best_metric:.4f} at epoch {best_epoch}\")\n",
    "                break\n",
    "        \n",
    "        epoch_progress.close()\n",
    "        \n",
    "        # Final evaluation on test set\n",
    "        print(\"\\nüß™ Final evaluation on test set...\")\n",
    "        \n",
    "        # Load best model\n",
    "        best_checkpoint_id = checkpoint_manager.find_best_checkpoint(experiment_name)\n",
    "        if best_checkpoint_id:\n",
    "            checkpoint_manager.load_checkpoint(\n",
    "                best_checkpoint_id, model, optimizer, scheduler, device\n",
    "            )\n",
    "            print(f\"Loaded best model from epoch {best_epoch}\")\n",
    "        \n",
    "        # Use EMA for final evaluation if enabled\n",
    "        if ema is not None:\n",
    "            ema.apply_shadow()\n",
    "        \n",
    "        test_loss, test_metrics = evaluate(model, test_loader, device, config, head_thresholds)\n",
    "        \n",
    "        if ema is not None:\n",
    "            ema.restore()\n",
    "        \n",
    "        # Log final metrics\n",
    "        mlflow.log_metric(\"test_loss\", test_loss)\n",
    "        for name, value in test_metrics.items():\n",
    "            if name != \"val_loss\":\n",
    "                mlflow.log_metric(name.replace(\"val_\", \"test_\"), value)\n",
    "        \n",
    "        mlflow.log_metric(\"best_metric\", best_metric)\n",
    "        mlflow.log_metric(\"best_epoch\", best_epoch)\n",
    "        \n",
    "        results = {\n",
    "            \"best_metric\": best_metric,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"test_loss\": test_loss,\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training completed!\")\n",
    "        print(f\"   Best {config.training.early_stopping.monitor}: {best_metric:.4f} at epoch {best_epoch}\")\n",
    "        print(f\"   Test loss: {test_loss:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ Enhanced training loop defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "Execute the training with the selected configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training execution cell\n",
    "def start_training():\n",
    "    \"\"\"Start the training process.\"\"\"\n",
    "    \n",
    "    # Check if configuration is loaded\n",
    "    if 'current_config' not in globals() or current_config is None:\n",
    "        print(\"‚ùå No configuration loaded. Please run the configuration selection cell first.\")\n",
    "        return\n",
    "    \n",
    "    if 'current_experiment_name' not in globals() or current_experiment_name is None:\n",
    "        print(\"‚ùå No experiment name set. Please run the configuration selection cell first.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Validate configuration\n",
    "        issues = config_manager.validate_config(current_config)\n",
    "        if issues:\n",
    "            print(\"‚ö†Ô∏è  Configuration issues found:\")\n",
    "            for issue in issues:\n",
    "                print(f\"   - {issue}\")\n",
    "            \n",
    "            response = input(\"Continue anyway? (y/n): \")\n",
    "            if response.lower() != 'y':\n",
    "                print(\"Training cancelled.\")\n",
    "                return\n",
    "        \n",
    "        # Start training\n",
    "        print(f\"\\nüöÄ Starting training: {current_experiment_name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = enhanced_train_loop(current_config, current_experiment_name)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üéâ Training completed successfully!\")\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìä Final Results:\")\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"   {key}:\")\n",
    "                for k, v in value.items():\n",
    "                    print(f\"     {k}: {v:.4f}\")\n",
    "            else:\n",
    "                print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # Display checkpoint summary\n",
    "        print(\"\\nüìÅ Checkpoint Summary:\")\n",
    "        checkpoints_df = checkpoint_manager.list_checkpoints(current_experiment_name)\n",
    "        if not checkpoints_df.empty:\n",
    "            display(checkpoints_df)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è  Training interrupted by user\")\n",
    "        print(\"   Checkpoints have been saved and training can be resumed\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Create training button\n",
    "train_button = widgets.Button(\n",
    "    description='üöÄ Start Training',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "def on_train_clicked(b):\n",
    "    start_training()\n",
    "\n",
    "train_button.on_click(on_train_clicked)\n",
    "\n",
    "print(\"Click the button below to start training:\")\n",
    "display(train_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Monitoring and Visualization\n",
    "\n",
    "Real-time monitoring and visualization of training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(experiment_name: str):\n",
    "    \"\"\"Plot training history from checkpoints.\"\"\"\n",
    "    \n",
    "    # Get latest checkpoint for the experiment\n",
    "    latest_checkpoint = checkpoint_manager.find_latest_checkpoint(experiment_name)\n",
    "    \n",
    "    if not latest_checkpoint:\n",
    "        print(f\"No checkpoints found for experiment: {experiment_name}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load training history\n",
    "        checkpoint_info = checkpoint_manager.get_checkpoint_info(latest_checkpoint)\n",
    "        training_history = checkpoint_info['training_state']['training_history']\n",
    "        \n",
    "        if not training_history:\n",
    "            print(\"No training history found\")\n",
    "            return\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(training_history)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'Training History: {experiment_name}', fontsize=16)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0, 0].plot(df['epoch'], df['train_loss'], label='Train Loss', color='blue')\n",
    "        axes[0, 0].plot(df['epoch'], df['val_loss'], label='Val Loss', color='red')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Learning rate plot\n",
    "        axes[0, 1].plot(df['epoch'], df['learning_rate'], color='green')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].set_title('Learning Rate Schedule')\n",
    "        axes[0, 1].set_yscale('log')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Extract validation metrics\n",
    "        val_metrics_keys = []\n",
    "        if training_history:\n",
    "            val_metrics_keys = list(training_history[0]['val_metrics'].keys())\n",
    "        \n",
    "        # Plot main validation metric\n",
    "        if val_metrics_keys:\n",
    "            main_metric = val_metrics_keys[0]  # Use first metric\n",
    "            metric_values = [epoch['val_metrics'][main_metric] for epoch in training_history]\n",
    "            axes[1, 0].plot(df['epoch'], metric_values, color='purple')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel(main_metric)\n",
    "            axes[1, 0].set_title(f'Validation {main_metric}')\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot multiple metrics if available\n",
    "        if len(val_metrics_keys) > 1:\n",
    "            for i, metric in enumerate(val_metrics_keys[:4]):  # Plot up to 4 metrics\n",
    "                metric_values = [epoch['val_metrics'][metric] for epoch in training_history]\n",
    "                axes[1, 1].plot(df['epoch'], metric_values, label=metric)\n",
    "            \n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Metric Value')\n",
    "            axes[1, 1].set_title('Validation Metrics')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(f\"\\nüìä Training Summary for {experiment_name}:\")\n",
    "        print(f\"   Total epochs: {len(training_history)}\")\n",
    "        print(f\"   Final train loss: {training_history[-1]['train_loss']:.4f}\")\n",
    "        print(f\"   Final val loss: {training_history[-1]['val_loss']:.4f}\")\n",
    "        \n",
    "        if val_metrics_keys:\n",
    "            print(f\"   Final validation metrics:\")\n",
    "            for metric, value in training_history[-1]['val_metrics'].items():\n",
    "                print(f\"     {metric}: {value:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting training history: {e}\")\n",
    "\n",
    "def create_monitoring_dashboard():\n",
    "    \"\"\"Create an interactive monitoring dashboard.\"\"\"\n",
    "    \n",
    "    # Get available experiments\n",
    "    checkpoints_df = checkpoint_manager.list_checkpoints()\n",
    "    experiments = checkpoints_df['experiment'].unique().tolist() if not checkpoints_df.empty else []\n",
    "    \n",
    "    if not experiments:\n",
    "        print(\"No experiments found. Start training first.\")\n",
    "        return\n",
    "    \n",
    "    experiment_dropdown = widgets.Dropdown(\n",
    "        options=experiments,\n",
    "        value=experiments[0],\n",
    "        description='Experiment:'\n",
    "    )\n",
    "    \n",
    "    plot_button = widgets.Button(\n",
    "        description='üìä Plot History',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    checkpoints_button = widgets.Button(\n",
    "        description='üìÅ Show Checkpoints',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_plot_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            plot_training_history(experiment_dropdown.value)\n",
    "    \n",
    "    def on_checkpoints_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            exp_checkpoints = checkpoint_manager.list_checkpoints(experiment_dropdown.value)\n",
    "            if not exp_checkpoints.empty:\n",
    "                print(f\"Checkpoints for {experiment_dropdown.value}:\")\n",
    "                display(exp_checkpoints)\n",
    "            else:\n",
    "                print(f\"No checkpoints found for {experiment_dropdown.value}\")\n",
    "    \n",
    "    plot_button.on_click(on_plot_clicked)\n",
    "    checkpoints_button.on_click(on_checkpoints_clicked)\n",
    "    \n",
    "    dashboard = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Training Monitoring Dashboard</h3>\"),\n",
    "        experiment_dropdown,\n",
    "        widgets.HBox([plot_button, checkpoints_button]),\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return dashboard\n",
    "\n",
    "# Display monitoring dashboard\n",
    "print(\"Training Monitoring Dashboard:\")\n",
    "monitoring_dashboard = create_monitoring_dashboard()\n",
    "if monitoring_dashboard:\n",
    "    display(monitoring_dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities\n",
    "\n",
    "Additional utilities for training management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_training_from_checkpoint(checkpoint_id: str):\n",
    "    \"\"\"Resume training from a specific checkpoint.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get checkpoint info\n",
    "        checkpoint_info = checkpoint_manager.get_checkpoint_info(checkpoint_id)\n",
    "        experiment_name = checkpoint_info['metadata']['experiment_name']\n",
    "        \n",
    "        print(f\"üîÑ Resuming training from checkpoint: {checkpoint_id}\")\n",
    "        print(f\"   Experiment: {experiment_name}\")\n",
    "        print(f\"   Epoch: {checkpoint_info['metadata']['epoch']}\")\n",
    "        \n",
    "        # Load configuration from checkpoint\n",
    "        config_dict = checkpoint_info['training_state']  # This should contain the config\n",
    "        \n",
    "        # Convert to ExperimentConfig (simplified)\n",
    "        # In practice, you might want to reconstruct this more carefully\n",
    "        global current_config, current_experiment_name\n",
    "        current_experiment_name = experiment_name\n",
    "        \n",
    "        # Start training\n",
    "        results = enhanced_train_loop(current_config, current_experiment_name)\n",
    "        \n",
    "        print(\"‚úÖ Training resumed and completed successfully!\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error resuming training: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_experiments(experiment_names: List[str]):\n",
    "    \"\"\"Compare multiple experiments.\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for exp_name in experiment_names:\n",
    "        latest_checkpoint = checkpoint_manager.find_latest_checkpoint(exp_name)\n",
    "        if latest_checkpoint:\n",
    "            try:\n",
    "                checkpoint_info = checkpoint_manager.get_checkpoint_info(latest_checkpoint)\n",
    "                metadata = checkpoint_info['metadata']\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'experiment': exp_name,\n",
    "                    'model_type': metadata['model_type'],\n",
    "                    'epochs': metadata['epoch'],\n",
    "                    'best_metric': metadata['best_metric'],\n",
    "                    'best_epoch': metadata['best_epoch'],\n",
    "                    'created_at': metadata['created_at']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading experiment {exp_name}: {e}\")\n",
    "    \n",
    "    if comparison_data:\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        df = df.sort_values('best_metric', ascending=False)\n",
    "        \n",
    "        print(\"üèÜ Experiment Comparison:\")\n",
    "        display(df)\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"No valid experiments found for comparison\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def cleanup_experiments(keep_best_n: int = 3):\n",
    "    \"\"\"Clean up old experiments, keeping only the best N.\"\"\"\n",
    "    \n",
    "    checkpoints_df = checkpoint_manager.list_checkpoints()\n",
    "    \n",
    "    if checkpoints_df.empty:\n",
    "        print(\"No checkpoints to clean up\")\n",
    "        return\n",
    "    \n",
    "    experiments = checkpoints_df['experiment'].unique()\n",
    "    \n",
    "    for experiment in experiments:\n",
    "        exp_checkpoints = checkpoints_df[checkpoints_df['experiment'] == experiment]\n",
    "        exp_checkpoints = exp_checkpoints.sort_values('best_metric', ascending=False)\n",
    "        \n",
    "        if len(exp_checkpoints) > keep_best_n:\n",
    "            to_delete = exp_checkpoints.iloc[keep_best_n:]\n",
    "            \n",
    "            print(f\"\\nüßπ Cleaning up experiment: {experiment}\")\n",
    "            for _, checkpoint in to_delete.iterrows():\n",
    "                checkpoint_manager.delete_checkpoint(checkpoint['checkpoint_id'])\n",
    "                print(f\"   Deleted: {checkpoint['checkpoint_id']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training notebook setup complete!\")\n",
    "print(\"\\nTo start training:\")\n",
    "print(\"1. Select a configuration using the configuration selector above\")\n",
    "print(\"2. Click the 'Start Training' button\")\n",
    "print(\"3. Monitor progress using the monitoring dashboard\")\n",
    "print(\"\\nTraining will automatically resume from checkpoints if interrupted.\")"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": "3"
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.8.0"
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 }
}