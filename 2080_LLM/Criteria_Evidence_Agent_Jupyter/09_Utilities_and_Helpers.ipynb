{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities and Helper Functions\n",
    "\n",
    "This notebook provides interactive exploration and testing of utility functions, metrics, losses, and training helpers.\n",
    "It allows you to understand and experiment with the core components of the training system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import all necessary libraries and utility modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Project Utilities\n",
    "\n",
    "Import all utility modules from the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration management\n",
    "%run 01_Configuration_Management.ipynb\n",
    "\n",
    "# Import utility modules\n",
    "from src.utils import (\n",
    "    set_seed, get_optimizer, get_scheduler, evaluate,\n",
    "    prepare_thresholds, compute_loss\n",
    ")\n",
    "from src.utils.metrics import compute_metrics\n",
    "from src.utils.training import (\n",
    "    train_epoch, validate_epoch, compute_loss as training_compute_loss\n",
    ")\n",
    "from src.utils.ema import EMA\n",
    "from src.losses import FocalLoss, LabelSmoothingCrossEntropy\n",
    "\n",
    "print(\"‚úÖ Project utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions Explorer\n",
    "\n",
    "Interactive exploration of different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_function_explorer():\n",
    "    \"\"\"Create an interactive loss function explorer.\"\"\"\n",
    "    \n",
    "    # Loss function selection\n",
    "    loss_type = widgets.Dropdown(\n",
    "        options=['Binary Cross Entropy', 'Focal Loss', 'Label Smoothing CE'],\n",
    "        value='Binary Cross Entropy',\n",
    "        description='Loss Type:'\n",
    "    )\n",
    "    \n",
    "    # Focal Loss parameters\n",
    "    focal_alpha = widgets.FloatSlider(\n",
    "        value=1.0,\n",
    "        min=0.1,\n",
    "        max=2.0,\n",
    "        step=0.1,\n",
    "        description='Focal Œ±:',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    focal_gamma = widgets.FloatSlider(\n",
    "        value=2.0,\n",
    "        min=0.0,\n",
    "        max=5.0,\n",
    "        step=0.5,\n",
    "        description='Focal Œ≥:',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Label Smoothing parameter\n",
    "    smoothing = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.0,\n",
    "        max=0.5,\n",
    "        step=0.05,\n",
    "        description='Smoothing:',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Test parameters\n",
    "    num_samples = widgets.IntSlider(\n",
    "        value=1000,\n",
    "        min=100,\n",
    "        max=5000,\n",
    "        step=100,\n",
    "        description='Samples:'\n",
    "    )\n",
    "    \n",
    "    num_classes = widgets.IntSlider(\n",
    "        value=5,\n",
    "        min=2,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description='Classes:'\n",
    "    )\n",
    "    \n",
    "    test_button = widgets.Button(\n",
    "        description='üß™ Test Loss Function',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_loss_type_change(change):\n",
    "        loss_name = change['new']\n",
    "        \n",
    "        # Enable/disable parameters based on loss type\n",
    "        focal_alpha.disabled = loss_name != 'Focal Loss'\n",
    "        focal_gamma.disabled = loss_name != 'Focal Loss'\n",
    "        smoothing.disabled = loss_name != 'Label Smoothing CE'\n",
    "    \n",
    "    loss_type.observe(on_loss_type_change, names='value')\n",
    "    \n",
    "    def on_test_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                print(f\"üß™ Testing {loss_type.value}\")\n",
    "                print(\"=\" * 40)\n",
    "                \n",
    "                # Generate synthetic data\n",
    "                torch.manual_seed(42)\n",
    "                \n",
    "                # Create logits and targets\n",
    "                logits = torch.randn(num_samples.value, num_classes.value)\n",
    "                targets = torch.randint(0, 2, (num_samples.value, num_classes.value)).float()\n",
    "                \n",
    "                print(f\"   Data shape: {logits.shape}\")\n",
    "                print(f\"   Positive rate: {targets.mean():.3f}\")\n",
    "                \n",
    "                # Create loss function\n",
    "                if loss_type.value == 'Binary Cross Entropy':\n",
    "                    loss_fn = nn.BCEWithLogitsLoss()\n",
    "                    loss_name = 'BCE'\n",
    "                elif loss_type.value == 'Focal Loss':\n",
    "                    loss_fn = FocalLoss(alpha=focal_alpha.value, gamma=focal_gamma.value)\n",
    "                    loss_name = f'Focal(Œ±={focal_alpha.value}, Œ≥={focal_gamma.value})'\n",
    "                else:  # Label Smoothing CE\n",
    "                    loss_fn = LabelSmoothingCrossEntropy(smoothing=smoothing.value)\n",
    "                    loss_name = f'LabelSmooth(Œµ={smoothing.value})'\n",
    "                \n",
    "                # Compute loss\n",
    "                loss_value = loss_fn(logits, targets)\n",
    "                \n",
    "                print(f\"\\nüìä Loss Results:\")\n",
    "                print(f\"   Loss function: {loss_name}\")\n",
    "                print(f\"   Loss value: {loss_value.item():.4f}\")\n",
    "                \n",
    "                # Analyze loss behavior\n",
    "                probs = torch.sigmoid(logits)\n",
    "                predictions = (probs > 0.5).float()\n",
    "                \n",
    "                accuracy = (predictions == targets).float().mean()\n",
    "                print(f\"   Accuracy: {accuracy.item():.4f}\")\n",
    "                \n",
    "                # Visualize loss landscape\n",
    "                print(f\"\\nüìà Loss Landscape Analysis:\")\n",
    "                \n",
    "                # Test different confidence levels\n",
    "                confidence_levels = torch.linspace(0.01, 0.99, 50)\n",
    "                loss_values = []\n",
    "                \n",
    "                for conf in confidence_levels:\n",
    "                    # Create confident predictions\n",
    "                    confident_logits = torch.log(conf / (1 - conf)) * torch.ones_like(targets)\n",
    "                    loss_val = loss_fn(confident_logits, targets)\n",
    "                    loss_values.append(loss_val.item())\n",
    "                \n",
    "                # Plot loss landscape\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                # Loss vs confidence\n",
    "                plt.subplot(2, 2, 1)\n",
    "                plt.plot(confidence_levels.numpy(), loss_values, 'b-', linewidth=2)\n",
    "                plt.xlabel('Prediction Confidence')\n",
    "                plt.ylabel('Loss Value')\n",
    "                plt.title(f'{loss_name} vs Confidence')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Loss distribution\n",
    "                plt.subplot(2, 2, 2)\n",
    "                individual_losses = []\n",
    "                for i in range(min(100, num_samples.value)):\n",
    "                    single_loss = loss_fn(logits[i:i+1], targets[i:i+1])\n",
    "                    individual_losses.append(single_loss.item())\n",
    "                \n",
    "                plt.hist(individual_losses, bins=20, alpha=0.7, color='skyblue')\n",
    "                plt.xlabel('Loss Value')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.title('Loss Distribution (Sample)')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Prediction distribution\n",
    "                plt.subplot(2, 2, 3)\n",
    "                plt.hist(probs.flatten().numpy(), bins=30, alpha=0.7, color='lightcoral')\n",
    "                plt.xlabel('Predicted Probability')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.title('Prediction Distribution')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Target distribution\n",
    "                plt.subplot(2, 2, 4)\n",
    "                class_counts = targets.sum(dim=0).numpy()\n",
    "                plt.bar(range(len(class_counts)), class_counts, color='lightgreen')\n",
    "                plt.xlabel('Class Index')\n",
    "                plt.ylabel('Positive Count')\n",
    "                plt.title('Target Distribution')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Compare with other loss functions\n",
    "                print(f\"\\nüîÑ Comparison with Other Loss Functions:\")\n",
    "                \n",
    "                comparison_losses = {\n",
    "                    'BCE': nn.BCEWithLogitsLoss(),\n",
    "                    'Focal(Œ±=1,Œ≥=2)': FocalLoss(alpha=1.0, gamma=2.0),\n",
    "                    'LabelSmooth(Œµ=0.1)': LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "                }\n",
    "                \n",
    "                for name, loss_func in comparison_losses.items():\n",
    "                    try:\n",
    "                        comp_loss = loss_func(logits, targets)\n",
    "                        print(f\"   {name}: {comp_loss.item():.4f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   {name}: Error - {e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error testing loss function: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    test_button.on_click(on_test_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Loss Function Explorer</h3>\"),\n",
    "        loss_type,\n",
    "        widgets.HBox([focal_alpha, focal_gamma]),\n",
    "        smoothing,\n",
    "        widgets.HBox([num_samples, num_classes]),\n",
    "        test_button\n",
    "    ])\n",
    "    \n",
    "    return widgets.VBox([controls, output])\n",
    "\n",
    "# Display loss function explorer\n",
    "loss_explorer = create_loss_function_explorer()\n",
    "display(loss_explorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Scheduler Explorer\n",
    "\n",
    "Interactive exploration of optimizers and learning rate schedulers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer_scheduler_explorer():\n",
    "    \"\"\"Create an interactive optimizer and scheduler explorer.\"\"\"\n",
    "    \n",
    "    # Optimizer selection\n",
    "    optimizer_type = widgets.Dropdown(\n",
    "        options=['AdamW', 'Adam', 'SGD'],\n",
    "        value='AdamW',\n",
    "        description='Optimizer:'\n",
    "    )\n",
    "    \n",
    "    # Optimizer parameters\n",
    "    learning_rate = widgets.FloatLogSlider(\n",
    "        value=1e-4,\n",
    "        base=10,\n",
    "        min=-6,\n",
    "        max=-2,\n",
    "        step=0.1,\n",
    "        description='Learning Rate:'\n",
    "    )\n",
    "    \n",
    "    weight_decay = widgets.FloatLogSlider(\n",
    "        value=1e-2,\n",
    "        base=10,\n",
    "        min=-5,\n",
    "        max=-1,\n",
    "        step=0.1,\n",
    "        description='Weight Decay:'\n",
    "    )\n",
    "    \n",
    "    # Scheduler selection\n",
    "    scheduler_type = widgets.Dropdown(\n",
    "        options=['linear', 'cosine', 'polynomial', 'constant'],\n",
    "        value='linear',\n",
    "        description='Scheduler:'\n",
    "    )\n",
    "    \n",
    "    # Scheduler parameters\n",
    "    num_epochs = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=100,\n",
    "        description='Epochs:'\n",
    "    )\n",
    "    \n",
    "    warmup_ratio = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.0,\n",
    "        max=0.5,\n",
    "        step=0.05,\n",
    "        description='Warmup Ratio:'\n",
    "    )\n",
    "    \n",
    "    test_button = widgets.Button(\n",
    "        description='üìä Test Configuration',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_test_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                print(f\"üìä Testing Optimizer and Scheduler Configuration\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                # Create a simple model for testing\n",
    "                model = nn.Linear(10, 1)\n",
    "                \n",
    "                # Create optimizer configuration\n",
    "                optimizer_config = OptimizerConfig(\n",
    "                    type=optimizer_type.value.lower(),\n",
    "                    learning_rate=learning_rate.value,\n",
    "                    weight_decay=weight_decay.value\n",
    "                )\n",
    "                \n",
    "                # Create scheduler configuration\n",
    "                scheduler_config = SchedulerConfig(\n",
    "                    type=scheduler_type.value,\n",
    "                    warmup_ratio=warmup_ratio.value\n",
    "                )\n",
    "                \n",
    "                print(f\"   Optimizer: {optimizer_type.value}\")\n",
    "                print(f\"   Learning Rate: {learning_rate.value:.2e}\")\n",
    "                print(f\"   Weight Decay: {weight_decay.value:.2e}\")\n",
    "                print(f\"   Scheduler: {scheduler_type.value}\")\n",
    "                print(f\"   Warmup Ratio: {warmup_ratio.value}\")\n",
    "                \n",
    "                # Create optimizer and scheduler\n",
    "                optimizer = get_optimizer(model, optimizer_config)\n",
    "                \n",
    "                # Calculate total steps\n",
    "                steps_per_epoch = 100  # Simulated\n",
    "                total_steps = num_epochs.value * steps_per_epoch\n",
    "                \n",
    "                scheduler = get_scheduler(optimizer, scheduler_config, total_steps)\n",
    "                \n",
    "                print(f\"\\nüìà Learning Rate Schedule:\")\n",
    "                print(f\"   Total steps: {total_steps}\")\n",
    "                print(f\"   Warmup steps: {int(total_steps * warmup_ratio.value)}\")\n",
    "                \n",
    "                # Simulate training and collect learning rates\n",
    "                learning_rates = []\n",
    "                steps = []\n",
    "                \n",
    "                for step in range(total_steps):\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    learning_rates.append(current_lr)\n",
    "                    steps.append(step)\n",
    "                    \n",
    "                    # Simulate optimizer step\n",
    "                    if scheduler is not None:\n",
    "                        scheduler.step()\n",
    "                \n",
    "                # Plot learning rate schedule\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                # Learning rate over steps\n",
    "                plt.subplot(2, 2, 1)\n",
    "                plt.plot(steps, learning_rates, 'b-', linewidth=2)\n",
    "                plt.xlabel('Training Step')\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.title(f'{scheduler_type.value.title()} LR Schedule')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Learning rate over epochs\n",
    "                plt.subplot(2, 2, 2)\n",
    "                epoch_lrs = [learning_rates[i * steps_per_epoch] for i in range(num_epochs.value)]\n",
    "                plt.plot(range(num_epochs.value), epoch_lrs, 'r-o', linewidth=2, markersize=6)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.title('LR at Epoch Start')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Log scale learning rate\n",
    "                plt.subplot(2, 2, 3)\n",
    "                plt.semilogy(steps, learning_rates, 'g-', linewidth=2)\n",
    "                plt.xlabel('Training Step')\n",
    "                plt.ylabel('Learning Rate (log scale)')\n",
    "                plt.title('LR Schedule (Log Scale)')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Learning rate statistics\n",
    "                plt.subplot(2, 2, 4)\n",
    "                lr_stats = {\n",
    "                    'Initial LR': learning_rates[0],\n",
    "                    'Max LR': max(learning_rates),\n",
    "                    'Final LR': learning_rates[-1],\n",
    "                    'Min LR': min(learning_rates)\n",
    "                }\n",
    "                \n",
    "                bars = plt.bar(lr_stats.keys(), lr_stats.values(), color=['blue', 'green', 'red', 'orange'])\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.title('LR Statistics')\n",
    "                plt.yscale('log')\n",
    "                plt.xticks(rotation=45)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, value in zip(bars, lr_stats.values()):\n",
    "                    plt.text(bar.get_x() + bar.get_width()/2, value, f'{value:.2e}', \n",
    "                            ha='center', va='bottom', fontsize=8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Print detailed statistics\n",
    "                print(f\"\\nüìä Detailed Statistics:\")\n",
    "                print(f\"   Initial LR: {learning_rates[0]:.2e}\")\n",
    "                print(f\"   Maximum LR: {max(learning_rates):.2e}\")\n",
    "                print(f\"   Final LR: {learning_rates[-1]:.2e}\")\n",
    "                print(f\"   Minimum LR: {min(learning_rates):.2e}\")\n",
    "                print(f\"   LR Decay Ratio: {learning_rates[-1] / learning_rates[0]:.4f}\")\n",
    "                \n",
    "                # Warmup analysis\n",
    "                warmup_steps = int(total_steps * warmup_ratio.value)\n",
    "                if warmup_steps > 0:\n",
    "                    warmup_lrs = learning_rates[:warmup_steps]\n",
    "                    print(f\"\\nüî• Warmup Analysis:\")\n",
    "                    print(f\"   Warmup steps: {warmup_steps}\")\n",
    "                    print(f\"   LR at warmup end: {warmup_lrs[-1]:.2e}\")\n",
    "                    print(f\"   Warmup slope: {(warmup_lrs[-1] - warmup_lrs[0]) / warmup_steps:.2e} per step\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error testing configuration: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    test_button.on_click(on_test_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Optimizer & Scheduler Explorer</h3>\"),\n",
    "        widgets.HBox([optimizer_type, scheduler_type]),\n",
    "        widgets.HBox([learning_rate, weight_decay]),\n",
    "        widgets.HBox([num_epochs, warmup_ratio]),\n",
    "        test_button\n",
    "    ])\n",
    "    \n",
    "    return widgets.VBox([controls, output])\n",
    "\n",
    "# Display optimizer and scheduler explorer\n",
    "print(\"\\nüìä Optimizer & Scheduler Explorer:\")\n",
    "optimizer_explorer = create_optimizer_scheduler_explorer()\n",
    "display(optimizer_explorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation Explorer\n",
    "\n",
    "Interactive exploration of evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_explorer():\n",
    "    \"\"\"Create an interactive metrics explorer.\"\"\"\n",
    "    \n",
    "    # Data generation parameters\n",
    "    num_samples = widgets.IntSlider(\n",
    "        value=1000,\n",
    "        min=100,\n",
    "        max=5000,\n",
    "        step=100,\n",
    "        description='Samples:'\n",
    "    )\n",
    "    \n",
    "    num_classes = widgets.IntSlider(\n",
    "        value=5,\n",
    "        min=2,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description='Classes:'\n",
    "    )\n",
    "    \n",
    "    positive_rate = widgets.FloatSlider(\n",
    "        value=0.3,\n",
    "        min=0.1,\n",
    "        max=0.9,\n",
    "        step=0.1,\n",
    "        description='Positive Rate:'\n",
    "    )\n",
    "    \n",
    "    noise_level = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.0,\n",
    "        max=0.5,\n",
    "        step=0.05,\n",
    "        description='Noise Level:'\n",
    "    )\n",
    "    \n",
    "    compute_button = widgets.Button(\n",
    "        description='üìä Compute Metrics',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_compute_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                print(f\"üìä Computing Evaluation Metrics\")\n",
    "                print(\"=\" * 40)\n",
    "                \n",
    "                # Generate synthetic data\n",
    "                torch.manual_seed(42)\n",
    "                np.random.seed(42)\n",
    "                \n",
    "                # Generate targets\n",
    "                targets = torch.bernoulli(torch.full((num_samples.value, num_classes.value), positive_rate.value))\n",
    "                \n",
    "                # Generate predictions with some correlation to targets\n",
    "                base_probs = targets.float() * (0.8 - noise_level.value) + (1 - targets.float()) * noise_level.value\n",
    "                noise = torch.randn_like(base_probs) * 0.1\n",
    "                probs = torch.clamp(base_probs + noise, 0.01, 0.99)\n",
    "                \n",
    "                # Convert to predictions\n",
    "                predictions = (probs > 0.5).float()\n",
    "                \n",
    "                print(f\"   Data shape: {targets.shape}\")\n",
    "                print(f\"   Actual positive rate: {targets.mean():.3f}\")\n",
    "                print(f\"   Predicted positive rate: {predictions.mean():.3f}\")\n",
    "                \n",
    "                # Compute metrics using project function\n",
    "                metrics = compute_metrics(predictions.numpy(), targets.numpy())\n",
    "                \n",
    "                print(f\"\\nüìà Computed Metrics:\")\n",
    "                for metric_name, value in metrics.items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        print(f\"   {metric_name}: {value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"   {metric_name}: {value}\")\n",
    "                \n",
    "                # Compute additional sklearn metrics for comparison\n",
    "                print(f\"\\nüîç Additional Metrics:\")\n",
    "                \n",
    "                # Per-class metrics\n",
    "                for i in range(num_classes.value):\n",
    "                    class_targets = targets[:, i].numpy()\n",
    "                    class_preds = predictions[:, i].numpy()\n",
    "                    class_probs = probs[:, i].numpy()\n",
    "                    \n",
    "                    if class_targets.sum() > 0:  # Only if there are positive examples\n",
    "                        try:\n",
    "                            auc = roc_auc_score(class_targets, class_probs)\n",
    "                            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                                class_targets, class_preds, average='binary', zero_division=0\n",
    "                            )\n",
    "                            print(f\"   Class {i}: AUC={auc:.3f}, P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
    "                        except ValueError:\n",
    "                            print(f\"   Class {i}: Cannot compute AUC (single class)\")\n",
    "                \n",
    "                # Visualize metrics\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "                fig.suptitle('Evaluation Metrics Analysis', fontsize=16)\n",
    "                \n",
    "                # 1. Confusion matrix heatmap (for first class)\n",
    "                from sklearn.metrics import confusion_matrix\n",
    "                cm = confusion_matrix(targets[:, 0].numpy(), predictions[:, 0].numpy())\n",
    "                \n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "                axes[0, 0].set_title('Confusion Matrix (Class 0)')\n",
    "                axes[0, 0].set_xlabel('Predicted')\n",
    "                axes[0, 0].set_ylabel('Actual')\n",
    "                \n",
    "                # 2. Prediction probability distribution\n",
    "                axes[0, 1].hist(probs.flatten().numpy(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "                axes[0, 1].axvline(0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "                axes[0, 1].set_xlabel('Predicted Probability')\n",
    "                axes[0, 1].set_ylabel('Frequency')\n",
    "                axes[0, 1].set_title('Prediction Probability Distribution')\n",
    "                axes[0, 1].legend()\n",
    "                axes[0, 1].grid(True, alpha=0.3)\n",
    "                \n",
    "                # 3. Per-class performance\n",
    "                class_f1_scores = []\n",
    "                for i in range(num_classes.value):\n",
    "                    class_targets = targets[:, i].numpy()\n",
    "                    class_preds = predictions[:, i].numpy()\n",
    "                    \n",
    "                    _, _, f1, _ = precision_recall_fscore_support(\n",
    "                        class_targets, class_preds, average='binary', zero_division=0\n",
    "                    )\n",
    "                    class_f1_scores.append(f1)\n",
    "                \n",
    "                axes[1, 0].bar(range(num_classes.value), class_f1_scores, color='lightgreen')\n",
    "                axes[1, 0].set_xlabel('Class Index')\n",
    "                axes[1, 0].set_ylabel('F1 Score')\n",
    "                axes[1, 0].set_title('Per-Class F1 Scores')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # 4. Precision-Recall curve (for first class)\n",
    "                from sklearn.metrics import precision_recall_curve\n",
    "                \n",
    "                if targets[:, 0].sum() > 0:\n",
    "                    precision_curve, recall_curve, _ = precision_recall_curve(\n",
    "                        targets[:, 0].numpy(), probs[:, 0].numpy()\n",
    "                    )\n",
    "                    \n",
    "                    axes[1, 1].plot(recall_curve, precision_curve, 'b-', linewidth=2)\n",
    "                    axes[1, 1].set_xlabel('Recall')\n",
    "                    axes[1, 1].set_ylabel('Precision')\n",
    "                    axes[1, 1].set_title('Precision-Recall Curve (Class 0)')\n",
    "                    axes[1, 1].grid(True, alpha=0.3)\n",
    "                else:\n",
    "                    axes[1, 1].text(0.5, 0.5, 'No positive examples\\nfor Class 0', \n",
    "                                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "                    axes[1, 1].set_title('Precision-Recall Curve (Class 0)')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Threshold analysis\n",
    "                print(f\"\\nüéØ Threshold Analysis:\")\n",
    "                \n",
    "                thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "                threshold_results = []\n",
    "                \n",
    "                for threshold in thresholds:\n",
    "                    thresh_preds = (probs > threshold).float()\n",
    "                    thresh_metrics = compute_metrics(thresh_preds.numpy(), targets.numpy())\n",
    "                    \n",
    "                    threshold_results.append({\n",
    "                        'Threshold': threshold,\n",
    "                        'Accuracy': thresh_metrics.get('accuracy', 0),\n",
    "                        'F1': thresh_metrics.get('f1_macro', 0),\n",
    "                        'Precision': thresh_metrics.get('precision_macro', 0),\n",
    "                        'Recall': thresh_metrics.get('recall_macro', 0)\n",
    "                    })\n",
    "                \n",
    "                threshold_df = pd.DataFrame(threshold_results)\n",
    "                print(\"\\n   Threshold Analysis Results:\")\n",
    "                display(threshold_df.round(4))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error computing metrics: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    compute_button.on_click(on_compute_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Metrics Explorer</h3>\"),\n",
    "        widgets.HBox([num_samples, num_classes]),\n",
    "        widgets.HBox([positive_rate, noise_level]),\n",
    "        compute_button\n",
    "    ])\n",
    "    \n",
    "    return widgets.VBox([controls, output])\n",
    "\n",
    "# Display metrics explorer\n",
    "print(\"\\nüìä Metrics Explorer:\")\n",
    "metrics_explorer = create_metrics_explorer()\n",
    "display(metrics_explorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMA (Exponential Moving Average) Explorer\n",
    "\n",
    "Interactive exploration of EMA for model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ema_explorer():\n",
    "    \"\"\"Create an interactive EMA explorer.\"\"\"\n",
    "    \n",
    "    # EMA parameters\n",
    "    decay_rate = widgets.FloatSlider(\n",
    "        value=0.999,\n",
    "        min=0.9,\n",
    "        max=0.9999,\n",
    "        step=0.0001,\n",
    "        description='Decay Rate:',\n",
    "        readout_format='.4f'\n",
    "    )\n",
    "    \n",
    "    num_updates = widgets.IntSlider(\n",
    "        value=1000,\n",
    "        min=100,\n",
    "        max=10000,\n",
    "        step=100,\n",
    "        description='Updates:'\n",
    "    )\n",
    "    \n",
    "    test_button = widgets.Button(\n",
    "        description='üß™ Test EMA',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_test_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            try:\n",
    "                print(f\"üß™ Testing EMA with decay={decay_rate.value}\")\n",
    "                print(\"=\" * 40)\n",
    "                \n",
    "                # Create a simple model\n",
    "                model = nn.Linear(10, 1)\n",
    "                \n",
    "                # Initialize EMA\n",
    "                ema = EMA(model, decay=decay_rate.value)\n",
    "                \n",
    "                print(f\"   Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "                print(f\"   EMA decay rate: {decay_rate.value}\")\n",
    "                print(f\"   Effective window: ~{1/(1-decay_rate.value):.1f} updates\")\n",
    "                \n",
    "                # Simulate training updates\n",
    "                original_weights = []\n",
    "                ema_weights = []\n",
    "                weight_differences = []\n",
    "                \n",
    "                # Get initial weight\n",
    "                initial_weight = model.weight.data[0, 0].item()\n",
    "                \n",
    "                for step in range(num_updates.value):\n",
    "                    # Simulate weight update (add some noise)\n",
    "                    with torch.no_grad():\n",
    "                        model.weight.data += torch.randn_like(model.weight.data) * 0.01\n",
    "                    \n",
    "                    # Update EMA\n",
    "                    ema.update()\n",
    "                    \n",
    "                    # Record weights (just first weight for visualization)\n",
    "                    if step % 10 == 0:  # Sample every 10 steps\n",
    "                        current_weight = model.weight.data[0, 0].item()\n",
    "                        ema_weight = ema.ema_model.weight.data[0, 0].item()\n",
    "                        \n",
    "                        original_weights.append(current_weight)\n",
    "                        ema_weights.append(ema_weight)\n",
    "                        weight_differences.append(abs(current_weight - ema_weight))\n",
    "                \n",
    "                # Visualize EMA behavior\n",
    "                steps_sampled = list(range(0, num_updates.value, 10))\n",
    "                \n",
    "                plt.figure(figsize=(15, 10))\n",
    "                \n",
    "                # Weight evolution\n",
    "                plt.subplot(2, 2, 1)\n",
    "                plt.plot(steps_sampled, original_weights, 'b-', alpha=0.7, label='Original Model', linewidth=1)\n",
    "                plt.plot(steps_sampled, ema_weights, 'r-', label='EMA Model', linewidth=2)\n",
    "                plt.xlabel('Training Step')\n",
    "                plt.ylabel('Weight Value')\n",
    "                plt.title('Weight Evolution: Original vs EMA')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Weight difference\n",
    "                plt.subplot(2, 2, 2)\n",
    "                plt.plot(steps_sampled, weight_differences, 'g-', linewidth=2)\n",
    "                plt.xlabel('Training Step')\n",
    "                plt.ylabel('|Original - EMA|')\n",
    "                plt.title('Absolute Difference Between Models')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # EMA decay analysis\n",
    "                plt.subplot(2, 2, 3)\n",
    "                decay_powers = np.arange(1, 101)\n",
    "                effective_weights = [(1 - decay_rate.value) * (decay_rate.value ** (p-1)) for p in decay_powers]\n",
    "                \n",
    "                plt.plot(decay_powers, effective_weights, 'purple', linewidth=2)\n",
    "                plt.xlabel('Steps Ago')\n",
    "                plt.ylabel('Effective Weight')\n",
    "                plt.title('EMA Weight Distribution')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Cumulative weight\n",
    "                plt.subplot(2, 2, 4)\n",
    "                cumulative_weights = np.cumsum(effective_weights)\n",
    "                plt.plot(decay_powers, cumulative_weights, 'orange', linewidth=2)\n",
    "                plt.axhline(y=0.5, color='red', linestyle='--', label='50% Weight')\n",
    "                plt.axhline(y=0.9, color='blue', linestyle='--', label='90% Weight')\n",
    "                plt.xlabel('Steps Ago')\n",
    "                plt.ylabel('Cumulative Weight')\n",
    "                plt.title('Cumulative EMA Weight')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # EMA statistics\n",
    "                print(f\"\\nüìä EMA Statistics:\")\n",
    "                print(f\"   Final original weight: {original_weights[-1]:.6f}\")\n",
    "                print(f\"   Final EMA weight: {ema_weights[-1]:.6f}\")\n",
    "                print(f\"   Final difference: {weight_differences[-1]:.6f}\")\n",
    "                print(f\"   Mean difference: {np.mean(weight_differences):.6f}\")\n",
    "                print(f\"   Max difference: {np.max(weight_differences):.6f}\")\n",
    "                \n",
    "                # Effective window analysis\n",
    "                steps_for_50_percent = None\n",
    "                steps_for_90_percent = None\n",
    "                \n",
    "                for i, cum_weight in enumerate(cumulative_weights):\n",
    "                    if steps_for_50_percent is None and cum_weight >= 0.5:\n",
    "                        steps_for_50_percent = i + 1\n",
    "                    if steps_for_90_percent is None and cum_weight >= 0.9:\n",
    "                        steps_for_90_percent = i + 1\n",
    "                        break\n",
    "                \n",
    "                print(f\"\\nüéØ Effective Window Analysis:\")\n",
    "                print(f\"   50% of weight from last {steps_for_50_percent} steps\")\n",
    "                print(f\"   90% of weight from last {steps_for_90_percent} steps\")\n",
    "                print(f\"   Theoretical window: {1/(1-decay_rate.value):.1f} steps\")\n",
    "                \n",
    "                # Compare different decay rates\n",
    "                print(f\"\\nüîÑ Decay Rate Comparison:\")\n",
    "                decay_rates = [0.99, 0.995, 0.999, 0.9995, 0.9999]\n",
    "                \n",
    "                for dr in decay_rates:\n",
    "                    effective_window = 1 / (1 - dr)\n",
    "                    print(f\"   Decay {dr}: ~{effective_window:.1f} step window\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error testing EMA: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    test_button.on_click(on_test_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>EMA Explorer</h3>\"),\n",
    "        widgets.HBox([decay_rate, num_updates]),\n",
    "        test_button\n",
    "    ])\n",
    "    \n",
    "    return widgets.VBox([controls, output])\n",
    "\n",
    "# Display EMA explorer\n",
    "print(\"\\nüß™ EMA Explorer:\")\n",
    "ema_explorer = create_ema_explorer()\n",
    "display(ema_explorer)\n",
    "\n",
    "print(\"\\n‚úÖ Utilities and Helpers notebook complete!\")\n",
    "print(\"\\nThis notebook provides:\")\n",
    "print(\"‚Ä¢ Interactive loss function exploration and testing\")\n",
    "print(\"‚Ä¢ Optimizer and scheduler configuration testing\")\n",
    "print(\"‚Ä¢ Comprehensive metrics computation and analysis\")\n",
    "print(\"‚Ä¢ EMA behavior visualization and analysis\")\n",
    "print(\"‚Ä¢ Utility function testing and validation\")"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": "3"
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.8.0"
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 }
}