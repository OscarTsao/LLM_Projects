# Optimized configuration for improved GPU memory usage and performance
model:
  name: "SpanBERT/spanbert-base-cased"
  use_label_head: true
  num_labels: 2
  max_length: 384
  doc_stride: 128
  max_answer_len: 64
  lambda_span: 0.5
  dropout: 0.1

train:
  # Increased batch size with gradient accumulation for effective batch size of 32
  batch_size: 16  # Up from 8
  grad_accum: 2   # Effective batch size: 16 * 2 = 32
  epochs: 500
  lr: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.06

  # Mixed precision training
  fp16: true
  bf16: false  # Alternative to fp16 for newer GPUs

  # Memory optimization
  gradient_checkpointing: true
  max_grad_norm: 1.0

  # DataLoader optimizations
  dataloader_pin_memory: true
  dataloader_num_workers: 4

  # Training schedule
  eval_every_steps: 500
  save_k_best: 2
  early_stop_patience: 5

data:
  train_path: "data/redsm5/train.jsonl"
  dev_path: "data/redsm5/dev.jsonl"
  test_path: "data/redsm5/test.jsonl"

  # Optimized data loading
  num_workers: 4
  prefetch_factor: 2
  persistent_workers: true

decode:
  top_k: 5
  nms_iou_thresh: 0.5
  allow_overlap: false

logging:
  output_dir: "outputs/run_optimized"
  wandb: false
  project: "criteria_binder"
  run_name: "spanbert_base_optimized"
  seed: 42

metrics:
  span_iou_thresh: 0.5
  combined_weight_span: 0.5
  combined_weight_label: 0.5

# Additional optimization notes:
# - Effective batch size increased from 8 to 32 (4x improvement)
# - Gradient checkpointing saves ~30-50% memory
# - Tensor core alignment (padding to multiples of 8) improves throughput
# - Memory-efficient collation reduces peak memory usage
# - Persistent workers reduce dataloading overhead