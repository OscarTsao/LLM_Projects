# Custom HPO configuration with optimized hyperparameters

# Dataset paths
posts_path: Data/redsm5/redsm5_posts.csv
annotations_path: Data/redsm5/redsm5_annotations.csv
criteria_path: Data/DSM-5/DSM_Criteria_Array_Fixed_Major_Depressive.json

# Model
model:
  _target_: model.get_pairwise_model
  model_name: bert-base-uncased
  device: null # auto-detect
  dropout: 0.18087897198748515

# Data loaders
# Evaluation batch size (kept consistent between val and test)
eval_batch_size: 32

train_loader:
  batch_size: 16
  shuffle: true
  num_workers: null # auto-detect based on CPU cores
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4
  drop_last: false

val_loader:
  batch_size: 32
  shuffle: false
  num_workers: null # auto-detect based on CPU cores
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  drop_last: false

test_loader:
  batch_size: 32
  shuffle: false
  num_workers: null # auto-detect based on CPU cores
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  drop_last: false

# Training settings
training:
  num_epochs: 100
  gradient_accumulation_steps: 1
  clip_grad_norm: 2.5743406195889795
  use_compile: false
  use_amp: true
  amp_dtype: float16
  use_grad_checkpointing: false
  threshold: 0.6590689742204876
  max_steps_per_epoch: 2000
  max_checkpoints: 5
  early_stopping_patience: 25
  save_checkpoints: true
  save_best_only: false
  save_optimizer_state: true
  save_history: true
  include_history_in_checkpoint: true
  save_config_in_checkpoint: true

# Optimization
optimizer:
  _target_: torch.optim.AdamW
  lr: 2.612582919692025e-05
  weight_decay: 0.0008574287554967094
  betas: [0.9222391435579143, 0.993747588451296]
  eps: 5.346946512135645e-09

# Loss - Hybrid BCE + Adaptive Focal Loss
loss:
  _target_: model.HybridBCEAdaptiveFocalLoss
  alpha: 0.6809458970943675
  gamma: 1.789301521916303
  delta: 1.8535704334044592
  bce_weight: 0.6963416097600597
  reduction: mean

# Scheduler - Cosine Annealing with Warmup
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  T_0: 15
  T_mult: 2
  eta_min: 1e-7

# Evaluation
save_top_k: 1
monitor_metric: f1
monitor_mode: max

# Paths
output_dir: outputs/training

# Hardware
hardware:
  dataloader_pin_memory: true
  enable_tf32: true
  cudnn_benchmark: true
  use_bfloat16_if_available: true

# Hydra overrides for sweeps
hydra:
  run:
    dir: outputs/training/${now:%Y%m%d_%H%M%S}
  sweep:
    dir: outputs/training/multirun/${now:%Y%m%d_%H%M%S}